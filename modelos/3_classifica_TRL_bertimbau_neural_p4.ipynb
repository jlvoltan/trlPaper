{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jKuooZfNFU"
      },
      "source": [
        "# Classificação faixa TRL - Bertimbau + Rede Neural [kfold][P4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9-gf3IMD5C"
      },
      "source": [
        "- BERTimbau Base (aka \"bert-base-portuguese-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i41YvsyfSUR"
      },
      "source": [
        "## Código Versão 1.0 - 21 OUT 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfR22FfgbJ"
      },
      "source": [
        "- Uso dos classificadores: *Rede Neural*\n",
        "- **k-Fold cross-validation: Esse código executou a rodada 4**.\n",
        "- Matriz de confusão;\n",
        "- Semente 42\n",
        "- 50 épocas\n",
        "- BATCH_SIZE = 8\n",
        "- Limite 242 tokens\n",
        "- optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.csv   ou  dataset_pre_processado_1.csv  ou  dataset_pre_processado_stem_2.csv\n",
        "#     CSV1                  CSV2                                   CSV3\n",
        "dataset = \"dataset.csv\""
      ],
      "metadata": {
        "id": "vXMRttjhRQ2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lembre-se estamos usando o dataset: \" + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJkf84RR64K",
        "outputId": "fd8446c1-e963-45f6-d5cc-ba4c49b762f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lembre-se estamos usando o dataset: dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "particao=4 # O KFold apresenta 5 partições (rodadas)\n",
        "particao-=1  #Funcionará com o índice de kfold_train e kfold_test"
      ],
      "metadata": {
        "id": "k2TIOYIqQNps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melhor_modelo = 'best_model_TRL_bertimbau_base_neural_' + str(particao+1) + '.bin'\n",
        "melhor_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lFCPPlujqL5L",
        "outputId": "1844639c-622e-4972-ae6b-9f5fc2356b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_model_TRL_bertimbau_base_neural_4.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 242"
      ],
      "metadata": {
        "id": "_6clpQcISMQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jlIjUzOgRY"
      },
      "source": [
        "- 63,7% treinamento - 16% validação...............20% teste\n",
        "- EPOCHS = 50\n",
        "- lr=2e-5\n",
        "- BATCH_SIZE = 8\n",
        "- Quantidade de tokens máxima = 242"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Msvyn0MfsOS"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC81VL_gQ-5"
      },
      "source": [
        "### Bibliotecas e ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZxPMZOfICS",
        "outputId": "89bde308-f4d5-4bbd-aa8b-1d73649797fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5RDBcpVf0TS",
        "outputId": "80239b6d-2015-46f8-a8db-279b2bc21539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmcqgWcf0WZ",
        "outputId": "88e4b46d-d58c-467c-c544-d9f002462b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 21 18:22:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoneZ3CngEDZ",
        "outputId": "9a985fad-c4e5-4e2a-ff92-d0cf9306c439"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWHFe7ef0Zp",
        "outputId": "fbb62433-3d18-4373-da4a-bfa098d10285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy       : 1.23.5\n",
            "pandas      : 1.5.3\n",
            "torch       : 2.1.0+cu118\n",
            "transformers: 4.34.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particoes KFOLD"
      ],
      "metadata": {
        "id": "oENoyHsUPjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para mais detalhes, consulte o código *1-kfold.ipynb* em que as partições foram sorteadas. As células a seguir recriam o resultado obtido."
      ],
      "metadata": {
        "id": "qL3FKCGlPtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = np.array([  0,   1,   2,   3,   5,   7,   9,  10,  11,  12,  13,  14,  15,\n",
        "         16,  17,  19,  20,  21,  22,  23,  25,  26,  27,  28,  30,  32,\n",
        "         33,  34,  35,  36,  37,  39,  40,  44,  45,  46,  47,  48,  49,\n",
        "         50,  52,  53,  54,  55,  56,  57,  59,  60,  61,  63,  64,  66,\n",
        "         68,  69,  70,  71,  72,  75,  76,  77,  78,  79,  80,  81,  82,\n",
        "         83,  84,  85,  87,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
        "        102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
        "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130,\n",
        "        131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145,\n",
        "        146, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
        "        164, 165, 166, 167])\n",
        "\n",
        "array2 = np.array([  0,   1,   3,   4,   5,   6,   7,   8,  10,  13,  14,  15,  16,\n",
        "         17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "         34,  36,  37,  38,  40,  41,  42,  43,  44,  45,  47,  49,  51,\n",
        "         52,  53,  54,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  70,  71,  73,  74,  75,  76,  77,  80,  83,\n",
        "         84,  86,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99,\n",
        "        100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115,\n",
        "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "        129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
        "        147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162,\n",
        "        163, 165, 166, 167])\n",
        "array3 = np.array([  1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
        "         17,  18,  20,  21,  22,  23,  24,  25,  28,  29,  30,  31,  32,\n",
        "         33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,\n",
        "         48,  50,  51,  52,  54,  55,  56,  57,  58,  59,  62,  63,  65,\n",
        "         67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "         82,  84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,\n",
        "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111,\n",
        "        112, 113, 114, 118, 120, 121, 122, 123, 124, 126, 128, 129, 131,\n",
        "        132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 146, 147,\n",
        "        148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162,\n",
        "        163, 164, 165, 166])\n",
        "array4 = np.array([  0,   2,   3,   4,   6,   8,   9,  10,  11,  12,  15,  16,  18,\n",
        "         19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  31,  32,  33,\n",
        "         34,  35,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
        "         49,  50,  51,  53,  56,  57,  58,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  71,  72,  73,  74,  75,  76,  78,  79,  80,\n",
        "         81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
        "         94,  95,  98, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111,\n",
        "        112, 114, 115, 116, 117, 119, 121, 123, 124, 125, 127, 128, 129,\n",
        "        130, 131, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145,\n",
        "        146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159,\n",
        "        162, 163, 164, 166, 167])\n",
        "array5 = np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
        "         14,  15,  16,  17,  18,  19,  21,  23,  24,  26,  27,  28,  29,\n",
        "         30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,  43,\n",
        "         44,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,\n",
        "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  72,  73,  74,\n",
        "         77,  78,  79,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
        "         92,  94,  95,  96,  97,  99, 101, 102, 103, 105, 106, 107, 108,\n",
        "        110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 127,\n",
        "        128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 143, 144,\n",
        "        145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160,\n",
        "        161, 162, 164, 165, 167])\n",
        "# Criar a lista composta\n",
        "kfold_train = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "Cn4vhJ9LPncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as cinco arrays numpy\n",
        "array1 = np.array([  4,   6,   8,  18,  24,  29,  31,  38,  41,  42,  43,  51,  58, 62,  65,  67,  73,  74,  86,  88,  89,  90,  95, 101, 107, 111, 128, 133, 144, 147, 149, 152, 156, 162])\n",
        "array2 = np.array([  2,   9,  11,  12,  21,  28,  32,  33,  35,  39,  46,  48,  50, 56,  72,  78,  79,  81,  82,  85,  87,  91, 103, 106, 108, 131, 134, 140, 141, 146, 148, 158, 159, 164])\n",
        "array3 = np.array([  0,   3,  15,  16,  19,  26,  27,  34,  40,  49,  53,  60,  61, 64,  66,  68,  69,  83,  92,  94, 110, 115, 116, 117, 119, 125, 127, 130, 137, 143, 145, 154, 157, 167])\n",
        "array4 = np.array([  1,   5,   7,  13,  14,  17,  23,  30,  36,  44,  52,  54,  55, 59,  70,  77,  96,  97,  99, 102, 105, 113, 118, 120, 122, 126, 132, 136, 138, 151, 160, 161, 165])\n",
        "array5 = np.array([ 10,  20,  22,  25,  37,  45,  47,  57,  63,  71,  75,  76,  80,  84,  93,  98, 100, 104, 109, 112, 114, 121, 123, 124, 129, 135, 139, 142, 150, 153, 155, 163, 166])\n",
        "\n",
        "\n",
        "# Criar a lista composta\n",
        "kfold_test = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "c3Ia2J4DPpO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4aPPergNnL"
      },
      "source": [
        "### Bertimbau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl6AUm0K3yE"
      },
      "source": [
        "- https://huggingface.co/neuralmind/bert-base-portuguese-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RQrbYpK5vC"
      },
      "source": [
        "```\n",
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "22d9201d156d4ef4aeda5bdbb28e83bb",
            "c9792202f8954c6fbd438ae2e811d036",
            "db357efab7c34f769b0ae523e246737a",
            "730e1581913443a6b393b5b09a1fd93d",
            "5d5e3bfa566441508b1667e04ce56ade",
            "7f55f7ebbfac4438a30cbd46282b7241",
            "43dde93e1da849abae630440a5de5bbd",
            "9c58b43d33bb474fb1e9e8ba73f8cef6",
            "c2c15b280c934f11a4b1f1b1ba286a2e",
            "df297a58a6164692b429b463eeafabf9",
            "50c2fabe8d3646ef908c1d3ef2fc9e00",
            "8311a71769cb441781604485df54cdfe",
            "c61f21fc37024374b3129fdc875e4adc",
            "b4eb1e05a7e7409083a189b117375cbd",
            "8f82915d9c604602bc47c3b10ab59717",
            "87c4e4b6bc4044a38b934ac2cd7cda6b",
            "6551af25b03e40f8a75fb776c0c44683",
            "254bf9e407854545bba1c298c889e5bf",
            "210737bf79314a4098dd36d86195c8ed",
            "d47540ab978e464e963607621a9ba59e",
            "765cc743fa2b48d59fcb5cba48f7b078",
            "061b1da69ac0427fb7b490f9200de5fd",
            "0f10dedc7a6a44b8ba48931409a92194",
            "3f0482d3d9d940e48744052719e03f91",
            "29a5244a0a714797993920aa584f7dfd",
            "cc9a095572e043b191d50f4d722ba9f4",
            "6dab0f34ec604b7baf504ad275e77f3d",
            "8d81198a7e9744348d485b32b48aaa4d",
            "c1fa03b326f4426887abefefc67efc98",
            "88e2e819e3af4b0796f50bfd872c3f59",
            "faac88760e9247ff893e3bf70b9f88b1",
            "b5406fea6aae4bbc8c69128bed0ed9f1",
            "d0e7f6a9f106430ebfc8f8cec46c6ca0",
            "e0748a05ea3948208b41b39f3b4ed281",
            "0b082e81e72b40c4b2f15a99acad7ffc",
            "01cc0c80355b45c98c2d15e867417c75",
            "5b4109026f6c448da15b1a263ad5ff65",
            "681dc3b6859d4726ad4023309694830f",
            "064a9d5069f8465b94ad44c0ae488578",
            "fe4afe3f1a1a45bb9a0e7591c7c453af",
            "d1d21c687d264cfab86838482fc93190",
            "9e1af2d09da74ef1bcc8e94f3ac36132",
            "b5a53747155d464abfe2e7545b3b21f1",
            "853f636163744ff990d80c14f2b72fc6",
            "fa1a08f5b196474398315ff41d8f87c4",
            "218f862e522d4c17bb7a6f7f303df0b0",
            "60284ac2371841619b4a7eebebc81f4a",
            "be946c0ad4064d40997f43edde765a2f",
            "903b89ce9c974eb4861e957867177f4d",
            "eac7aeb9a117400f8009f377f229179e",
            "d8d93672887748548c8068022b8855f9",
            "a5082f26e06e4e0598cfbf2be4058025",
            "be1e58903fc14a2093e59947497db8ec",
            "1e4d1f17349a4edd8bf903a2e7683954",
            "f25f5240e5f4496c87f29c32d8be1938"
          ]
        },
        "id": "WPj7c-IBgWRx",
        "outputId": "c1e7a768-720f-496a-e05d-9d4662fccc01"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22d9201d156d4ef4aeda5bdbb28e83bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8311a71769cb441781604485df54cdfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f10dedc7a6a44b8ba48931409a92194"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0748a05ea3948208b41b39f3b4ed281"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa1a08f5b196474398315ff41d8f87c4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "daec87bbf8594ca0bf8ef7526829fba0",
            "96b483fbe0744ad98a543c8429a61e0f",
            "84e53758776e4510b8c7ef79ce24c882",
            "0e501952fe374519944b067669dfcb0a",
            "3e5264997f5e4230a4abf0959c28fbd9",
            "e519f0844b42429a8889baf6a388eeac",
            "97b3909bbb194aab9433edf2fe913f0e",
            "3a585739f24f4cb8bf46085fc1260fbc",
            "b222648feec94c828ffa063e628993f5",
            "75dc0997312145b3885667f50e058e43",
            "baacd25317734a4da7242906a5d20b7b"
          ]
        },
        "id": "qSErznNMh4P5",
        "outputId": "3a98640b-540a-4697-8db5-8e3b2dbf599b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daec87bbf8594ca0bf8ef7526829fba0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s_rPsFgWUr"
      },
      "outputs": [],
      "source": [
        "frase = 'A avaliação de prontidão tecnológica em tecnologias de interesse militar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJbGMogWaZ",
        "outputId": "8750e0cd-1d3f-4678-cb61-49780e14bef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  frase,\n",
        "  max_length=20,   #Perceba que irei cortar um token da minha frase anterior\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fSHD8KgqDb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYjQrfrglJS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvfsVlnFglPp"
      },
      "outputs": [],
      "source": [
        "novo_df = df[[\"resumo\", \"rotulo\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nth2Ui-qg6Ix"
      },
      "outputs": [],
      "source": [
        "def adapto_faixa(faixa):\n",
        "    faixa -=1\n",
        "    return faixa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKj3lo8Qg9Vj",
        "outputId": "cfbee6f0-b7d4-4a4e-baa5-dedebf67294e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3dc243303e67>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)\n"
          ]
        }
      ],
      "source": [
        "novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_cANKWhOhCix",
        "outputId": "bc085c9e-af45-43dd-87f8-2acbeba9ad55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                resumo  rotulo\n",
              "163  Rio de Janeiro (RJ) – O Centro de Avaliações d...       2\n",
              "164  Este trabalho apresenta um sistema para contro...       2\n",
              "165  No contexto das comunicações táticas baseadas ...       2\n",
              "166  O valor da velocidade de alvos móveis em image...       0\n",
              "167  Neste artigo é apresentada a análise da seção ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bab03a13-532b-4181-b532-078d0038c7bc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resumo</th>\n",
              "      <th>rotulo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Rio de Janeiro (RJ) – O Centro de Avaliações d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Este trabalho apresenta um sistema para contro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>No contexto das comunicações táticas baseadas ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>O valor da velocidade de alvos móveis em image...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Neste artigo é apresentada a análise da seção ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bab03a13-532b-4181-b532-078d0038c7bc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bab03a13-532b-4181-b532-078d0038c7bc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bab03a13-532b-4181-b532-078d0038c7bc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d9f5a5a3-a8d9-4db8-bf44-73e51d793b5d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d9f5a5a3-a8d9-4db8-bf44-73e51d793b5d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d9f5a5a3-a8d9-4db8-bf44-73e51d793b5d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "novo_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZK6EQ4xhFHI"
      },
      "outputs": [],
      "source": [
        "class_names = ['Faixa 1', 'Faixa 2', 'Faixa 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmol6gshNCR"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFoXmzH5hFOo"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      #padding='longest',\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K-OuLWzhYhB"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.resumo.to_numpy(),\n",
        "    targets=df.rotulo.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBrznGAWiJWg"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-yCbjniOLI",
        "outputId": "bccf6022-e5c7-4181-a576-8a2d02ea8603"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxhW_89kRiv"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiZnSuuTiTOZ"
      },
      "outputs": [],
      "source": [
        "class TRLClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(TRLClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    #The last_hidden_state is a sequence of hidden states of the last layer of the model\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kX7ClUGiiCQ"
      },
      "outputs": [],
      "source": [
        "model = TRLClassifier(len(class_names)) #Pois tenho 3 classes\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXw5SIa0hkah"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTSemYwhnlI"
      },
      "source": [
        "### Separação e preparação do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lembrar que estamos utilizando o Kfold"
      ],
      "metadata": {
        "id": "z09fs6pHk5FG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLzT3ttq-0t"
      },
      "source": [
        "    - 63,7% treinamento - 16% validação                20% teste\n",
        "\n",
        "      ------------ 80%----------------------------------20%--"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o índice dos conjuntos\n",
        "index_df_train = kfold_train[particao]\n",
        "index_df_test = kfold_test[particao]"
      ],
      "metadata": {
        "id": "FeqFToMFnaMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = novo_df.iloc[index_df_train], novo_df.iloc[index_df_test]"
      ],
      "metadata": {
        "id": "EpR4mtHLoDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Bhg2qAhSlg"
      },
      "outputs": [],
      "source": [
        "#Separando o treinamento em treino e validação\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, stratify= df_train['rotulo'], random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FAkW0qhW2J",
        "outputId": "887af7ab-73e9-4546-f40d-c3bc61b1fc94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((108, 2), (27, 2), (33, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxinAAqhW5A",
        "outputId": "468cc7a1-1de8-48de-c1bb-b591105b080a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fsBv27hW8L",
        "outputId": "5fbd4d93-0459-4e8a-8d09-a7884468c61a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbQEYoXhvVA",
        "outputId": "0ccbdd16-ff81-4e66-f5b7-c26a9a6d8443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7qyCztOhvX4",
        "outputId": "6c516cb9-f1d8-4fe6-f7bd-253ce72ae12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP6kiy4whvaa",
        "outputId": "b907b6a3-2041-4403-fc02-66191ee533f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzMSQeLlRHn"
      },
      "source": [
        "### Treinamento mesmo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oRhXpG_iBWB",
        "outputId": "24af0777-1751-4f6b-a061-84c6c5158af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8dJvCviFTI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKAqpPliFWJ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZisESWlkqH",
        "outputId": "d92b7d50-e874-4ad6-85ab-7668bb4edf1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.8726248613425663 accuracy 0.611111111111111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.63992964848876 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6864387967756816 accuracy 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7137816399335861 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6147911101579666 accuracy 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6411407217383385 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.4647774685706411 accuracy 0.8425925925925926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8894549161195755 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.21756421641579696 accuracy 0.9074074074074073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5220351219177246 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.12248444693562176 accuracy 0.9629629629629629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5643583356868476 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.015232529840432107 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5193447768688202 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0031045204045117964 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.7727362141013145 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.001718953118792602 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.256884917616844 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011380421446769364 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.9488261422229698 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0012335645525516675 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.9880591047694907 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0010084871235968812 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.017831456148997 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000895197835883924 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.037447603419423 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0008047930605243891 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.070686314254999 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007239005104306021 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.10375764593482 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006831762854874666 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.132000617682934 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006339641653799585 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.1580950170755386 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000649949988915718 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.183634229004383 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005991818720108963 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.20775705575943 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006017145309929869 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.2309459447860718 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005369397133888144 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.248982183635235 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005227281404326537 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.264104947447777 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005287668048237849 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.2784027755260468 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005070446308569185 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.2888528555631638 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004704295757359692 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3006742149591446 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004798974987352267 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3113154619932175 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004820800677407533 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.321172147989273 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00045686514515961917 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3304045647382736 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00042268813662563583 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3412112444639206 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004557982216023707 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3497288078069687 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004339252232706973 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.357030913233757 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003859852108040026 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3663187623023987 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00036825264604496103 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.373721942305565 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00038910679203192037 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3782412260770798 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003822890160206173 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3829628974199295 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003847183314584462 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3884319067001343 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00035630062172588495 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3927423506975174 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003875995961217476 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.397979512810707 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00037241834278185185 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4016162306070328 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000344386330522996 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4033374041318893 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003357499933502238 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4053052067756653 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00032877870086979655 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.408995896577835 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00033832198823802173 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4122674614191055 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003418709301123662 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4142054319381714 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003711450387657221 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4156909435987473 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003506286094696926 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4168391823768616 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00032119952707684467 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4179823994636536 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00033086701192327643 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4187233448028564 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003469364528427832 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4191756397485733 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00034964629906296196 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4193590730428696 accuracy 0.6296296296296295\n",
            "\n",
            "CPU times: user 2min 58s, sys: 1min 17s, total: 4min 15s\n",
            "Wall time: 5min 8s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     torch.save(model.state_dict(), melhor_modelo)\n",
        "#     best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjGnJKDmZVJ"
      },
      "source": [
        "- Gráfico de desempenho no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQOy_NBBlks7"
      },
      "outputs": [],
      "source": [
        "A= history['train_acc']\n",
        "A2 = [tensor.item() for tensor in A]\n",
        "B= history['val_acc']\n",
        "B2 = [tensor.item() for tensor in B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "hHRH2aNQlkvy",
        "outputId": "bfa95762-dcef-470c-8fbe-1a83116259e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 1017,
              "height": 717
            }
          }
        }
      ],
      "source": [
        "plt.plot(A2, label='train accuracy')\n",
        "plt.plot(B2, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd9c9eGmecP"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jFRntT6mgoW",
        "outputId": "f5bb1ecd-38b6-48a3-8637-af98186a93b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7878787878787878"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBG2MIlgmkHv"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXTHRhD5mqnn",
        "outputId": "72c3de84-0a59-4c28-df54-9b59094ad774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTdojX6Jmqqm",
        "outputId": "4ef6ab3a-50c3-4a18-fa71-7d4c0a0a0937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Faixa 1       0.83      1.00      0.90        19\n",
            "     Faixa 2       0.67      0.25      0.36         8\n",
            "     Faixa 3       0.71      0.83      0.77         6\n",
            "\n",
            "    accuracy                           0.79        33\n",
            "   macro avg       0.74      0.69      0.68        33\n",
            "weighted avg       0.77      0.79      0.75        33\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "HQ54xluu8RL3",
        "outputId": "b98e0cac-5e9a-43ca-f403-56475c59ba0c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 998,
              "height": 723
            }
          }
        }
      ],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('Faixa TRL verdadeira')\n",
        "  plt.xlabel('Faixa TRL predita');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZTajeJuiP_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22d9201d156d4ef4aeda5bdbb28e83bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9792202f8954c6fbd438ae2e811d036",
              "IPY_MODEL_db357efab7c34f769b0ae523e246737a",
              "IPY_MODEL_730e1581913443a6b393b5b09a1fd93d"
            ],
            "layout": "IPY_MODEL_5d5e3bfa566441508b1667e04ce56ade"
          }
        },
        "c9792202f8954c6fbd438ae2e811d036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f55f7ebbfac4438a30cbd46282b7241",
            "placeholder": "​",
            "style": "IPY_MODEL_43dde93e1da849abae630440a5de5bbd",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "db357efab7c34f769b0ae523e246737a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c58b43d33bb474fb1e9e8ba73f8cef6",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2c15b280c934f11a4b1f1b1ba286a2e",
            "value": 43
          }
        },
        "730e1581913443a6b393b5b09a1fd93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df297a58a6164692b429b463eeafabf9",
            "placeholder": "​",
            "style": "IPY_MODEL_50c2fabe8d3646ef908c1d3ef2fc9e00",
            "value": " 43.0/43.0 [00:00&lt;00:00, 606B/s]"
          }
        },
        "5d5e3bfa566441508b1667e04ce56ade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f55f7ebbfac4438a30cbd46282b7241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43dde93e1da849abae630440a5de5bbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c58b43d33bb474fb1e9e8ba73f8cef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c15b280c934f11a4b1f1b1ba286a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df297a58a6164692b429b463eeafabf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c2fabe8d3646ef908c1d3ef2fc9e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8311a71769cb441781604485df54cdfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c61f21fc37024374b3129fdc875e4adc",
              "IPY_MODEL_b4eb1e05a7e7409083a189b117375cbd",
              "IPY_MODEL_8f82915d9c604602bc47c3b10ab59717"
            ],
            "layout": "IPY_MODEL_87c4e4b6bc4044a38b934ac2cd7cda6b"
          }
        },
        "c61f21fc37024374b3129fdc875e4adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6551af25b03e40f8a75fb776c0c44683",
            "placeholder": "​",
            "style": "IPY_MODEL_254bf9e407854545bba1c298c889e5bf",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "b4eb1e05a7e7409083a189b117375cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_210737bf79314a4098dd36d86195c8ed",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d47540ab978e464e963607621a9ba59e",
            "value": 209528
          }
        },
        "8f82915d9c604602bc47c3b10ab59717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_765cc743fa2b48d59fcb5cba48f7b078",
            "placeholder": "​",
            "style": "IPY_MODEL_061b1da69ac0427fb7b490f9200de5fd",
            "value": " 210k/210k [00:00&lt;00:00, 2.69MB/s]"
          }
        },
        "87c4e4b6bc4044a38b934ac2cd7cda6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6551af25b03e40f8a75fb776c0c44683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "254bf9e407854545bba1c298c889e5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "210737bf79314a4098dd36d86195c8ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d47540ab978e464e963607621a9ba59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "765cc743fa2b48d59fcb5cba48f7b078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "061b1da69ac0427fb7b490f9200de5fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f10dedc7a6a44b8ba48931409a92194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f0482d3d9d940e48744052719e03f91",
              "IPY_MODEL_29a5244a0a714797993920aa584f7dfd",
              "IPY_MODEL_cc9a095572e043b191d50f4d722ba9f4"
            ],
            "layout": "IPY_MODEL_6dab0f34ec604b7baf504ad275e77f3d"
          }
        },
        "3f0482d3d9d940e48744052719e03f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d81198a7e9744348d485b32b48aaa4d",
            "placeholder": "​",
            "style": "IPY_MODEL_c1fa03b326f4426887abefefc67efc98",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "29a5244a0a714797993920aa584f7dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88e2e819e3af4b0796f50bfd872c3f59",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faac88760e9247ff893e3bf70b9f88b1",
            "value": 2
          }
        },
        "cc9a095572e043b191d50f4d722ba9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5406fea6aae4bbc8c69128bed0ed9f1",
            "placeholder": "​",
            "style": "IPY_MODEL_d0e7f6a9f106430ebfc8f8cec46c6ca0",
            "value": " 2.00/2.00 [00:00&lt;00:00, 36.8B/s]"
          }
        },
        "6dab0f34ec604b7baf504ad275e77f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d81198a7e9744348d485b32b48aaa4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1fa03b326f4426887abefefc67efc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88e2e819e3af4b0796f50bfd872c3f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faac88760e9247ff893e3bf70b9f88b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5406fea6aae4bbc8c69128bed0ed9f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e7f6a9f106430ebfc8f8cec46c6ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0748a05ea3948208b41b39f3b4ed281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b082e81e72b40c4b2f15a99acad7ffc",
              "IPY_MODEL_01cc0c80355b45c98c2d15e867417c75",
              "IPY_MODEL_5b4109026f6c448da15b1a263ad5ff65"
            ],
            "layout": "IPY_MODEL_681dc3b6859d4726ad4023309694830f"
          }
        },
        "0b082e81e72b40c4b2f15a99acad7ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_064a9d5069f8465b94ad44c0ae488578",
            "placeholder": "​",
            "style": "IPY_MODEL_fe4afe3f1a1a45bb9a0e7591c7c453af",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "01cc0c80355b45c98c2d15e867417c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d21c687d264cfab86838482fc93190",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e1af2d09da74ef1bcc8e94f3ac36132",
            "value": 112
          }
        },
        "5b4109026f6c448da15b1a263ad5ff65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5a53747155d464abfe2e7545b3b21f1",
            "placeholder": "​",
            "style": "IPY_MODEL_853f636163744ff990d80c14f2b72fc6",
            "value": " 112/112 [00:00&lt;00:00, 4.47kB/s]"
          }
        },
        "681dc3b6859d4726ad4023309694830f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "064a9d5069f8465b94ad44c0ae488578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4afe3f1a1a45bb9a0e7591c7c453af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1d21c687d264cfab86838482fc93190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e1af2d09da74ef1bcc8e94f3ac36132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5a53747155d464abfe2e7545b3b21f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "853f636163744ff990d80c14f2b72fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa1a08f5b196474398315ff41d8f87c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_218f862e522d4c17bb7a6f7f303df0b0",
              "IPY_MODEL_60284ac2371841619b4a7eebebc81f4a",
              "IPY_MODEL_be946c0ad4064d40997f43edde765a2f"
            ],
            "layout": "IPY_MODEL_903b89ce9c974eb4861e957867177f4d"
          }
        },
        "218f862e522d4c17bb7a6f7f303df0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eac7aeb9a117400f8009f377f229179e",
            "placeholder": "​",
            "style": "IPY_MODEL_d8d93672887748548c8068022b8855f9",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "60284ac2371841619b4a7eebebc81f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5082f26e06e4e0598cfbf2be4058025",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be1e58903fc14a2093e59947497db8ec",
            "value": 647
          }
        },
        "be946c0ad4064d40997f43edde765a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e4d1f17349a4edd8bf903a2e7683954",
            "placeholder": "​",
            "style": "IPY_MODEL_f25f5240e5f4496c87f29c32d8be1938",
            "value": " 647/647 [00:00&lt;00:00, 12.9kB/s]"
          }
        },
        "903b89ce9c974eb4861e957867177f4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac7aeb9a117400f8009f377f229179e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d93672887748548c8068022b8855f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5082f26e06e4e0598cfbf2be4058025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be1e58903fc14a2093e59947497db8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e4d1f17349a4edd8bf903a2e7683954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25f5240e5f4496c87f29c32d8be1938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daec87bbf8594ca0bf8ef7526829fba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96b483fbe0744ad98a543c8429a61e0f",
              "IPY_MODEL_84e53758776e4510b8c7ef79ce24c882",
              "IPY_MODEL_0e501952fe374519944b067669dfcb0a"
            ],
            "layout": "IPY_MODEL_3e5264997f5e4230a4abf0959c28fbd9"
          }
        },
        "96b483fbe0744ad98a543c8429a61e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e519f0844b42429a8889baf6a388eeac",
            "placeholder": "​",
            "style": "IPY_MODEL_97b3909bbb194aab9433edf2fe913f0e",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "84e53758776e4510b8c7ef79ce24c882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a585739f24f4cb8bf46085fc1260fbc",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b222648feec94c828ffa063e628993f5",
            "value": 438235074
          }
        },
        "0e501952fe374519944b067669dfcb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75dc0997312145b3885667f50e058e43",
            "placeholder": "​",
            "style": "IPY_MODEL_baacd25317734a4da7242906a5d20b7b",
            "value": " 438M/438M [00:03&lt;00:00, 138MB/s]"
          }
        },
        "3e5264997f5e4230a4abf0959c28fbd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e519f0844b42429a8889baf6a388eeac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97b3909bbb194aab9433edf2fe913f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a585739f24f4cb8bf46085fc1260fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b222648feec94c828ffa063e628993f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75dc0997312145b3885667f50e058e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baacd25317734a4da7242906a5d20b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}