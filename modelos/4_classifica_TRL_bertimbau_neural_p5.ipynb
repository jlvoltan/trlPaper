{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jKuooZfNFU"
      },
      "source": [
        "# Classificação faixa TRL - Bertimbau + Rede Neural [kfold][P5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9-gf3IMD5C"
      },
      "source": [
        "- BERTimbau Base (aka \"bert-base-portuguese-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i41YvsyfSUR"
      },
      "source": [
        "## Código Versão 1.0 - 21 OUT 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfR22FfgbJ"
      },
      "source": [
        "- Uso dos classificadores: *Rede Neural*\n",
        "- **k-Fold cross-validation: Esse código executou a rodada 5**.\n",
        "- Matriz de confusão;\n",
        "- Semente 42\n",
        "- 50 épocas\n",
        "- BATCH_SIZE = 8\n",
        "- Limite 242 tokens\n",
        "- optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.csv   ou  dataset_pre_processado_1.csv  ou  dataset_pre_processado_stem_2.csv\n",
        "#     CSV1                  CSV2                                   CSV3\n",
        "dataset = \"dataset.csv\""
      ],
      "metadata": {
        "id": "vXMRttjhRQ2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lembre-se estamos usando o dataset: \" + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJkf84RR64K",
        "outputId": "3c9798eb-7c02-4cc7-f68a-fa8f9bc174c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lembre-se estamos usando o dataset: dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "particao=5 # O KFold apresenta 5 partições (rodadas)\n",
        "particao-=1  #Funcionará com o índice de kfold_train e kfold_test"
      ],
      "metadata": {
        "id": "k2TIOYIqQNps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melhor_modelo = 'best_model_TRL_bertimbau_base_neural_' + str(particao+1) + '.bin'\n",
        "melhor_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lFCPPlujqL5L",
        "outputId": "d1907a89-dd2a-4a48-e0fe-e40d70d6eb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_model_TRL_bertimbau_base_neural_5.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 242"
      ],
      "metadata": {
        "id": "_6clpQcISMQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jlIjUzOgRY"
      },
      "source": [
        "- 63,7% treinamento - 16% validação...............20% teste\n",
        "- EPOCHS = 50\n",
        "- lr=2e-5\n",
        "- BATCH_SIZE = 8\n",
        "- Quantidade de tokens máxima = 242"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Msvyn0MfsOS"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC81VL_gQ-5"
      },
      "source": [
        "### Bibliotecas e ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZxPMZOfICS",
        "outputId": "bec3f749-eaea-4650-e7b2-f83a100e53f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5RDBcpVf0TS",
        "outputId": "dc5f97dc-83bd-41fa-dc6f-cbe3fb07fc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.2/1.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmcqgWcf0WZ",
        "outputId": "44af92e9-2dbb-435c-a9d2-9fdaa5a89f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 21 18:36:56 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoneZ3CngEDZ",
        "outputId": "fd1666ee-fb3b-48a2-a193-0939cecd4206"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWHFe7ef0Zp",
        "outputId": "07d6230a-f9ec-4c51-c166-3eb9ff291e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy       : 1.23.5\n",
            "pandas      : 1.5.3\n",
            "torch       : 2.1.0+cu118\n",
            "transformers: 4.34.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particoes KFOLD"
      ],
      "metadata": {
        "id": "oENoyHsUPjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para mais detalhes, consulte o código *1-kfold.ipynb* em que as partições foram sorteadas. As células a seguir recriam o resultado obtido."
      ],
      "metadata": {
        "id": "qL3FKCGlPtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = np.array([  0,   1,   2,   3,   5,   7,   9,  10,  11,  12,  13,  14,  15,\n",
        "         16,  17,  19,  20,  21,  22,  23,  25,  26,  27,  28,  30,  32,\n",
        "         33,  34,  35,  36,  37,  39,  40,  44,  45,  46,  47,  48,  49,\n",
        "         50,  52,  53,  54,  55,  56,  57,  59,  60,  61,  63,  64,  66,\n",
        "         68,  69,  70,  71,  72,  75,  76,  77,  78,  79,  80,  81,  82,\n",
        "         83,  84,  85,  87,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
        "        102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
        "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130,\n",
        "        131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145,\n",
        "        146, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
        "        164, 165, 166, 167])\n",
        "\n",
        "array2 = np.array([  0,   1,   3,   4,   5,   6,   7,   8,  10,  13,  14,  15,  16,\n",
        "         17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "         34,  36,  37,  38,  40,  41,  42,  43,  44,  45,  47,  49,  51,\n",
        "         52,  53,  54,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  70,  71,  73,  74,  75,  76,  77,  80,  83,\n",
        "         84,  86,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99,\n",
        "        100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115,\n",
        "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "        129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
        "        147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162,\n",
        "        163, 165, 166, 167])\n",
        "array3 = np.array([  1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
        "         17,  18,  20,  21,  22,  23,  24,  25,  28,  29,  30,  31,  32,\n",
        "         33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,\n",
        "         48,  50,  51,  52,  54,  55,  56,  57,  58,  59,  62,  63,  65,\n",
        "         67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "         82,  84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,\n",
        "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111,\n",
        "        112, 113, 114, 118, 120, 121, 122, 123, 124, 126, 128, 129, 131,\n",
        "        132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 146, 147,\n",
        "        148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162,\n",
        "        163, 164, 165, 166])\n",
        "array4 = np.array([  0,   2,   3,   4,   6,   8,   9,  10,  11,  12,  15,  16,  18,\n",
        "         19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  31,  32,  33,\n",
        "         34,  35,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
        "         49,  50,  51,  53,  56,  57,  58,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  71,  72,  73,  74,  75,  76,  78,  79,  80,\n",
        "         81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
        "         94,  95,  98, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111,\n",
        "        112, 114, 115, 116, 117, 119, 121, 123, 124, 125, 127, 128, 129,\n",
        "        130, 131, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145,\n",
        "        146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159,\n",
        "        162, 163, 164, 166, 167])\n",
        "array5 = np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
        "         14,  15,  16,  17,  18,  19,  21,  23,  24,  26,  27,  28,  29,\n",
        "         30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,  43,\n",
        "         44,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,\n",
        "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  72,  73,  74,\n",
        "         77,  78,  79,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
        "         92,  94,  95,  96,  97,  99, 101, 102, 103, 105, 106, 107, 108,\n",
        "        110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 127,\n",
        "        128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 143, 144,\n",
        "        145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160,\n",
        "        161, 162, 164, 165, 167])\n",
        "# Criar a lista composta\n",
        "kfold_train = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "Cn4vhJ9LPncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as cinco arrays numpy\n",
        "array1 = np.array([  4,   6,   8,  18,  24,  29,  31,  38,  41,  42,  43,  51,  58, 62,  65,  67,  73,  74,  86,  88,  89,  90,  95, 101, 107, 111, 128, 133, 144, 147, 149, 152, 156, 162])\n",
        "array2 = np.array([  2,   9,  11,  12,  21,  28,  32,  33,  35,  39,  46,  48,  50, 56,  72,  78,  79,  81,  82,  85,  87,  91, 103, 106, 108, 131, 134, 140, 141, 146, 148, 158, 159, 164])\n",
        "array3 = np.array([  0,   3,  15,  16,  19,  26,  27,  34,  40,  49,  53,  60,  61, 64,  66,  68,  69,  83,  92,  94, 110, 115, 116, 117, 119, 125, 127, 130, 137, 143, 145, 154, 157, 167])\n",
        "array4 = np.array([  1,   5,   7,  13,  14,  17,  23,  30,  36,  44,  52,  54,  55, 59,  70,  77,  96,  97,  99, 102, 105, 113, 118, 120, 122, 126, 132, 136, 138, 151, 160, 161, 165])\n",
        "array5 = np.array([ 10,  20,  22,  25,  37,  45,  47,  57,  63,  71,  75,  76,  80,  84,  93,  98, 100, 104, 109, 112, 114, 121, 123, 124, 129, 135, 139, 142, 150, 153, 155, 163, 166])\n",
        "\n",
        "\n",
        "# Criar a lista composta\n",
        "kfold_test = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "c3Ia2J4DPpO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4aPPergNnL"
      },
      "source": [
        "### Bertimbau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl6AUm0K3yE"
      },
      "source": [
        "- https://huggingface.co/neuralmind/bert-base-portuguese-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RQrbYpK5vC"
      },
      "source": [
        "```\n",
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "96949ca5782f43f6b542515139092137",
            "cdbd252b6da547d5879e869ef69128a3",
            "e1045e3ee22047e5a51ca33aa9ac99b4",
            "8c9cb52447c442ea8f1fbad16554123b",
            "e43167b589d445179407be614ab32037",
            "e133b87d9f64414c8a7043ef0a1468ce",
            "564878778f1e4393b713a164c7a2e7f7",
            "359a6c4726504c1f9a54317af105dbb9",
            "67da336388d749e6982b02725ad36218",
            "29d93540769c4d21b2ef02754c7d4db6",
            "76a0611bfc4a475ca700144dddeabf8f",
            "b1bf33e78a414b8891b10b4a0515f4e1",
            "c1403210467243c7ac503804448eb27c",
            "188107a234a64a13ab5acbe9c9943702",
            "31b0e0708b4643b8aa5329de00666fd5",
            "391be781f1224063a07e12c26bfc19a2",
            "56a7c4be6bfa4dc8a713cbf5743bede2",
            "f22b03ba96bf4d10872d4028f20fb1cf",
            "e5f0c8c9931a45318b1899e98010a469",
            "ee9cb0e5f1504197aa6476ed347eafd0",
            "af62a7baa3e74992a5e4fcd391713698",
            "1428bb7ca8864c90a009c3b1b574cf4d",
            "8e50c57ea793494eb4f070ec3abdd64d",
            "61756ffc7f4a40ce8ff4151579308d2e",
            "8e43225819424fa898bb728bb9705098",
            "e86560f085a740eaab4075cdb437bb80",
            "642867622c0e47a89512d9c4a0cecf68",
            "33d194ad69ff4c1d9636ecaef8a2dd90",
            "5d0aa83108af4a40b9895916ad6f5f14",
            "03eb7ace2c56456281d4c380a591ab4f",
            "cf9855f9d1ec43778e2a57a7f874c62a",
            "23dca3bce5674d12b95b3ff985f54b34",
            "7cc36b7605aa42858f334821167743a1",
            "74f0d44a4bd54721898b134099a22b94",
            "4cd4a494936547239ca9103c2afc7e89",
            "bcebadc99e4843b08ec911cdbf60b441",
            "935f92dc634c4f9f8ed97cc81dfe78ef",
            "bdd5ae13de414951b14972a335865826",
            "472f407c82cd4dbc82072f283e5382a4",
            "f77c1acd26134cb092ae462857272ae0",
            "21a927f11f3e476d9071d2f1b1c096ee",
            "2e790d3bc8fa4ef0b9a48719f1125ce7",
            "12bd3dd6f9a14d11a05819cf4ccece86",
            "fe8d52335dba477f8046bfef2f0b6336",
            "c4d8ee44b6e342c2aa05244cb4cf6891",
            "01a486329dd04781a7c08c931e0b9728",
            "cec644ce78934681bcfa2b0c0d16c1ba",
            "fad43d477b45411ebcc8154c23cd6934",
            "5b31a102e3f14f80a5cce4dbd0f26a63",
            "3703d8a6b66048aeb554209c54aab7e7",
            "e902323ea9294edebd9165b44a724550",
            "b6b76fa6f30d41a09692351824a9def8",
            "a55b7d16c18a4c2286a97a5bdda7a7cd",
            "55e544227d2a4e19a02c93a230c1b819",
            "0f2c73262e644a128c7882c4d4fb6d7a"
          ]
        },
        "id": "WPj7c-IBgWRx",
        "outputId": "31fe5c19-43bc-41c1-da40-d5942fd710e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96949ca5782f43f6b542515139092137"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1bf33e78a414b8891b10b4a0515f4e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e50c57ea793494eb4f070ec3abdd64d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74f0d44a4bd54721898b134099a22b94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4d8ee44b6e342c2aa05244cb4cf6891"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "13701d23dcca479a8393372f01eb6743",
            "5d0730901efd416da0290b9443f1d138",
            "252f90b860fc4ccf81d1c8efcc4ac063",
            "6f223a39b93e40ba9b6f11b0006d9803",
            "977e6f737e0447108d8232fb8e6fa414",
            "14a734d954cc490ba831da6850ed30cd",
            "ed971930caac4e93aec77df349e077fc",
            "9a5f50e81ae9433db2df8eccb87f4d3e",
            "e7154d55dc424e80b28c1810f8c6e807",
            "ed744c2e322d4795ab6ac866fff39ae2",
            "de7f19981362478b82c766d1f14ca358"
          ]
        },
        "id": "qSErznNMh4P5",
        "outputId": "8a352f10-98ba-4d51-f612-77d22c7ad526"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13701d23dcca479a8393372f01eb6743"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s_rPsFgWUr"
      },
      "outputs": [],
      "source": [
        "frase = 'A avaliação de prontidão tecnológica em tecnologias de interesse militar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJbGMogWaZ",
        "outputId": "4e7442cc-c599-4740-a744-417cfa362457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  frase,\n",
        "  max_length=20,   #Perceba que irei cortar um token da minha frase anterior\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fSHD8KgqDb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYjQrfrglJS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvfsVlnFglPp"
      },
      "outputs": [],
      "source": [
        "novo_df = df[[\"resumo\", \"rotulo\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nth2Ui-qg6Ix"
      },
      "outputs": [],
      "source": [
        "def adapto_faixa(faixa):\n",
        "    faixa -=1\n",
        "    return faixa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKj3lo8Qg9Vj",
        "outputId": "09698293-2cb1-4e56-d3ac-50e4222c739b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3dc243303e67>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)\n"
          ]
        }
      ],
      "source": [
        "novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_cANKWhOhCix",
        "outputId": "34e6919e-81b7-42a7-be76-cfd58c57bcb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                resumo  rotulo\n",
              "163  Rio de Janeiro (RJ) – O Centro de Avaliações d...       2\n",
              "164  Este trabalho apresenta um sistema para contro...       2\n",
              "165  No contexto das comunicações táticas baseadas ...       2\n",
              "166  O valor da velocidade de alvos móveis em image...       0\n",
              "167  Neste artigo é apresentada a análise da seção ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30a4439c-0047-4aef-a0b4-ace6c004eb57\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resumo</th>\n",
              "      <th>rotulo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Rio de Janeiro (RJ) – O Centro de Avaliações d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Este trabalho apresenta um sistema para contro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>No contexto das comunicações táticas baseadas ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>O valor da velocidade de alvos móveis em image...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Neste artigo é apresentada a análise da seção ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30a4439c-0047-4aef-a0b4-ace6c004eb57')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-30a4439c-0047-4aef-a0b4-ace6c004eb57 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-30a4439c-0047-4aef-a0b4-ace6c004eb57');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-667241b7-e75f-42a7-a7a2-d2f6891027ef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-667241b7-e75f-42a7-a7a2-d2f6891027ef')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-667241b7-e75f-42a7-a7a2-d2f6891027ef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "novo_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZK6EQ4xhFHI"
      },
      "outputs": [],
      "source": [
        "class_names = ['Faixa 1', 'Faixa 2', 'Faixa 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmol6gshNCR"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFoXmzH5hFOo"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      #padding='longest',\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K-OuLWzhYhB"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.resumo.to_numpy(),\n",
        "    targets=df.rotulo.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBrznGAWiJWg"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-yCbjniOLI",
        "outputId": "88238981-b658-4a86-8e00-d5a6602e18b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxhW_89kRiv"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiZnSuuTiTOZ"
      },
      "outputs": [],
      "source": [
        "class TRLClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(TRLClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    #The last_hidden_state is a sequence of hidden states of the last layer of the model\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kX7ClUGiiCQ"
      },
      "outputs": [],
      "source": [
        "model = TRLClassifier(len(class_names)) #Pois tenho 3 classes\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXw5SIa0hkah"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTSemYwhnlI"
      },
      "source": [
        "### Separação e preparação do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lembrar que estamos utilizando o Kfold"
      ],
      "metadata": {
        "id": "z09fs6pHk5FG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLzT3ttq-0t"
      },
      "source": [
        "    - 63,7% treinamento - 16% validação                20% teste\n",
        "\n",
        "      ------------ 80%----------------------------------20%--"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o índice dos conjuntos\n",
        "index_df_train = kfold_train[particao]\n",
        "index_df_test = kfold_test[particao]"
      ],
      "metadata": {
        "id": "FeqFToMFnaMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = novo_df.iloc[index_df_train], novo_df.iloc[index_df_test]"
      ],
      "metadata": {
        "id": "EpR4mtHLoDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Bhg2qAhSlg"
      },
      "outputs": [],
      "source": [
        "#Separando o treinamento em treino e validação\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, stratify= df_train['rotulo'], random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FAkW0qhW2J",
        "outputId": "7bb2dfbd-6d3a-477b-f05a-20cc213199d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((108, 2), (27, 2), (33, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxinAAqhW5A",
        "outputId": "83c1777f-972b-4bcf-dd41-c76c3d0445f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fsBv27hW8L",
        "outputId": "aa6a7e1b-3bb5-4a2c-ad96-2d0125771dff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbQEYoXhvVA",
        "outputId": "94921e5c-8793-4107-cd9c-a01886ff62f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7qyCztOhvX4",
        "outputId": "f6c5f5fa-9758-4c5e-810f-4d22336b5557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP6kiy4whvaa",
        "outputId": "f138909f-d094-4bf1-b224-48221b85227d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzMSQeLlRHn"
      },
      "source": [
        "### Treinamento mesmo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oRhXpG_iBWB",
        "outputId": "b583f14b-01cb-46ac-b0f3-2125132bb367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8dJvCviFTI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKAqpPliFWJ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZisESWlkqH",
        "outputId": "f6657a79-1b65-40e8-ab60-d727943d3ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.8633821776935032 accuracy 0.6666666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7631054297089577 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6586743380342212 accuracy 0.7407407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7862058356404305 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.590038633772305 accuracy 0.7592592592592592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7968940436840057 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5311989039182663 accuracy 0.8055555555555555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1118742227554321 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.41897085096154896 accuracy 0.8333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7348763672634959 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2695599618766989 accuracy 0.9259259259259258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0784989232197404 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.17117205135790364 accuracy 0.9351851851851851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4088580012321472 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.12332776834123901 accuracy 0.9722222222222222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3729746454628184 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.035133082835402875 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9484593779779971 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.040059881451140554 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3417878974287305 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.017608892496874824 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1940527376136743 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.02496766671538353 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.276522019179538 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.016242682484776845 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2034927863860503 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.018443281097071513 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.315124868953717 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.021064073732954318 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1709418846294284 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013806352146535314 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1668652944499627 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0178996759184104 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1176479181449395 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.016536016790528914 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2443614490766777 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013167159078875557 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2394120169337839 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01504248781878102 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2877884472691221 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011205777031136677 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.212992673565168 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013645518284257767 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2703599438391393 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011783346126321703 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3416932345789974 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0164290571660136 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2646220407768851 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0152067207631522 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3949202376315952 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013072376002258222 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3930887163223815 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01618783939712947 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3939245511646732 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.012273701772625958 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2896471398853464 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.015871447982915145 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3405051100780838 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.017017824974443232 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.478776119511167 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.017925300317334143 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4665478186580003 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01585321778631104 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4744431210347102 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01264772449420499 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4573593162058387 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013470829346001014 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4826264827352134 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.015208465955635932 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4568979862742708 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01463318482066305 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4257403413430438 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01159620363198753 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.397425560055126 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.015508963458289924 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3947514873289037 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013422253990678914 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4147919929528143 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.014153695014621397 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4019345645428984 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.012571875985096475 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4326064324268373 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.012050037596574319 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.451717958028894 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013204423047552285 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.441973759996472 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.009460248649702407 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.404012790590059 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01282446035682889 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3905887498040101 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.014434669193828345 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3782516002756893 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01481032905576285 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3882542445207946 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013577523402221101 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3911037440993823 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.012973828372196294 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3946355482548825 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011019749032649477 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3945680857286789 accuracy 0.7407407407407407\n",
            "\n",
            "CPU times: user 3min 14s, sys: 1min 22s, total: 4min 36s\n",
            "Wall time: 5min 20s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     torch.save(model.state_dict(), melhor_modelo)\n",
        "#     best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjGnJKDmZVJ"
      },
      "source": [
        "- Gráfico de desempenho no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQOy_NBBlks7"
      },
      "outputs": [],
      "source": [
        "A= history['train_acc']\n",
        "A2 = [tensor.item() for tensor in A]\n",
        "B= history['val_acc']\n",
        "B2 = [tensor.item() for tensor in B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "hHRH2aNQlkvy",
        "outputId": "cd28d016-85e9-4c01-d74e-3acc77c549e7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 1017,
              "height": 717
            }
          }
        }
      ],
      "source": [
        "plt.plot(A2, label='train accuracy')\n",
        "plt.plot(B2, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd9c9eGmecP"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jFRntT6mgoW",
        "outputId": "d0832fb8-7456-4daa-bdc9-92b5d9d54475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.696969696969697"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBG2MIlgmkHv"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXTHRhD5mqnn",
        "outputId": "eb7431a8-90ac-4b0a-cf8c-d7e3137acbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTdojX6Jmqqm",
        "outputId": "c0a7007f-b8ba-4679-9ccf-9a49401cdfbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Faixa 1       0.71      0.94      0.81        18\n",
            "     Faixa 2       0.00      0.00      0.00         9\n",
            "     Faixa 3       0.75      1.00      0.86         6\n",
            "\n",
            "    accuracy                           0.70        33\n",
            "   macro avg       0.49      0.65      0.56        33\n",
            "weighted avg       0.52      0.70      0.60        33\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "HQ54xluu8RL3",
        "outputId": "f0ff4c0d-957d-4cc1-ec24-7660633331a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 981,
              "height": 723
            }
          }
        }
      ],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('Faixa TRL verdadeira')\n",
        "  plt.xlabel('Faixa TRL predita');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZTajeJuiP_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96949ca5782f43f6b542515139092137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdbd252b6da547d5879e869ef69128a3",
              "IPY_MODEL_e1045e3ee22047e5a51ca33aa9ac99b4",
              "IPY_MODEL_8c9cb52447c442ea8f1fbad16554123b"
            ],
            "layout": "IPY_MODEL_e43167b589d445179407be614ab32037"
          }
        },
        "cdbd252b6da547d5879e869ef69128a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e133b87d9f64414c8a7043ef0a1468ce",
            "placeholder": "​",
            "style": "IPY_MODEL_564878778f1e4393b713a164c7a2e7f7",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "e1045e3ee22047e5a51ca33aa9ac99b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_359a6c4726504c1f9a54317af105dbb9",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67da336388d749e6982b02725ad36218",
            "value": 43
          }
        },
        "8c9cb52447c442ea8f1fbad16554123b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29d93540769c4d21b2ef02754c7d4db6",
            "placeholder": "​",
            "style": "IPY_MODEL_76a0611bfc4a475ca700144dddeabf8f",
            "value": " 43.0/43.0 [00:00&lt;00:00, 895B/s]"
          }
        },
        "e43167b589d445179407be614ab32037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e133b87d9f64414c8a7043ef0a1468ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564878778f1e4393b713a164c7a2e7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "359a6c4726504c1f9a54317af105dbb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67da336388d749e6982b02725ad36218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29d93540769c4d21b2ef02754c7d4db6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76a0611bfc4a475ca700144dddeabf8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1bf33e78a414b8891b10b4a0515f4e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1403210467243c7ac503804448eb27c",
              "IPY_MODEL_188107a234a64a13ab5acbe9c9943702",
              "IPY_MODEL_31b0e0708b4643b8aa5329de00666fd5"
            ],
            "layout": "IPY_MODEL_391be781f1224063a07e12c26bfc19a2"
          }
        },
        "c1403210467243c7ac503804448eb27c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56a7c4be6bfa4dc8a713cbf5743bede2",
            "placeholder": "​",
            "style": "IPY_MODEL_f22b03ba96bf4d10872d4028f20fb1cf",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "188107a234a64a13ab5acbe9c9943702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5f0c8c9931a45318b1899e98010a469",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee9cb0e5f1504197aa6476ed347eafd0",
            "value": 209528
          }
        },
        "31b0e0708b4643b8aa5329de00666fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af62a7baa3e74992a5e4fcd391713698",
            "placeholder": "​",
            "style": "IPY_MODEL_1428bb7ca8864c90a009c3b1b574cf4d",
            "value": " 210k/210k [00:00&lt;00:00, 7.92MB/s]"
          }
        },
        "391be781f1224063a07e12c26bfc19a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a7c4be6bfa4dc8a713cbf5743bede2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f22b03ba96bf4d10872d4028f20fb1cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5f0c8c9931a45318b1899e98010a469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee9cb0e5f1504197aa6476ed347eafd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af62a7baa3e74992a5e4fcd391713698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1428bb7ca8864c90a009c3b1b574cf4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e50c57ea793494eb4f070ec3abdd64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61756ffc7f4a40ce8ff4151579308d2e",
              "IPY_MODEL_8e43225819424fa898bb728bb9705098",
              "IPY_MODEL_e86560f085a740eaab4075cdb437bb80"
            ],
            "layout": "IPY_MODEL_642867622c0e47a89512d9c4a0cecf68"
          }
        },
        "61756ffc7f4a40ce8ff4151579308d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d194ad69ff4c1d9636ecaef8a2dd90",
            "placeholder": "​",
            "style": "IPY_MODEL_5d0aa83108af4a40b9895916ad6f5f14",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "8e43225819424fa898bb728bb9705098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03eb7ace2c56456281d4c380a591ab4f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf9855f9d1ec43778e2a57a7f874c62a",
            "value": 2
          }
        },
        "e86560f085a740eaab4075cdb437bb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23dca3bce5674d12b95b3ff985f54b34",
            "placeholder": "​",
            "style": "IPY_MODEL_7cc36b7605aa42858f334821167743a1",
            "value": " 2.00/2.00 [00:00&lt;00:00, 148B/s]"
          }
        },
        "642867622c0e47a89512d9c4a0cecf68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d194ad69ff4c1d9636ecaef8a2dd90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d0aa83108af4a40b9895916ad6f5f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03eb7ace2c56456281d4c380a591ab4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf9855f9d1ec43778e2a57a7f874c62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23dca3bce5674d12b95b3ff985f54b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cc36b7605aa42858f334821167743a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74f0d44a4bd54721898b134099a22b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4cd4a494936547239ca9103c2afc7e89",
              "IPY_MODEL_bcebadc99e4843b08ec911cdbf60b441",
              "IPY_MODEL_935f92dc634c4f9f8ed97cc81dfe78ef"
            ],
            "layout": "IPY_MODEL_bdd5ae13de414951b14972a335865826"
          }
        },
        "4cd4a494936547239ca9103c2afc7e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_472f407c82cd4dbc82072f283e5382a4",
            "placeholder": "​",
            "style": "IPY_MODEL_f77c1acd26134cb092ae462857272ae0",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "bcebadc99e4843b08ec911cdbf60b441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21a927f11f3e476d9071d2f1b1c096ee",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e790d3bc8fa4ef0b9a48719f1125ce7",
            "value": 112
          }
        },
        "935f92dc634c4f9f8ed97cc81dfe78ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12bd3dd6f9a14d11a05819cf4ccece86",
            "placeholder": "​",
            "style": "IPY_MODEL_fe8d52335dba477f8046bfef2f0b6336",
            "value": " 112/112 [00:00&lt;00:00, 8.99kB/s]"
          }
        },
        "bdd5ae13de414951b14972a335865826": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "472f407c82cd4dbc82072f283e5382a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f77c1acd26134cb092ae462857272ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21a927f11f3e476d9071d2f1b1c096ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e790d3bc8fa4ef0b9a48719f1125ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12bd3dd6f9a14d11a05819cf4ccece86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe8d52335dba477f8046bfef2f0b6336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4d8ee44b6e342c2aa05244cb4cf6891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01a486329dd04781a7c08c931e0b9728",
              "IPY_MODEL_cec644ce78934681bcfa2b0c0d16c1ba",
              "IPY_MODEL_fad43d477b45411ebcc8154c23cd6934"
            ],
            "layout": "IPY_MODEL_5b31a102e3f14f80a5cce4dbd0f26a63"
          }
        },
        "01a486329dd04781a7c08c931e0b9728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3703d8a6b66048aeb554209c54aab7e7",
            "placeholder": "​",
            "style": "IPY_MODEL_e902323ea9294edebd9165b44a724550",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "cec644ce78934681bcfa2b0c0d16c1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6b76fa6f30d41a09692351824a9def8",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a55b7d16c18a4c2286a97a5bdda7a7cd",
            "value": 647
          }
        },
        "fad43d477b45411ebcc8154c23cd6934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e544227d2a4e19a02c93a230c1b819",
            "placeholder": "​",
            "style": "IPY_MODEL_0f2c73262e644a128c7882c4d4fb6d7a",
            "value": " 647/647 [00:00&lt;00:00, 43.9kB/s]"
          }
        },
        "5b31a102e3f14f80a5cce4dbd0f26a63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3703d8a6b66048aeb554209c54aab7e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e902323ea9294edebd9165b44a724550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6b76fa6f30d41a09692351824a9def8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a55b7d16c18a4c2286a97a5bdda7a7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55e544227d2a4e19a02c93a230c1b819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f2c73262e644a128c7882c4d4fb6d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13701d23dcca479a8393372f01eb6743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d0730901efd416da0290b9443f1d138",
              "IPY_MODEL_252f90b860fc4ccf81d1c8efcc4ac063",
              "IPY_MODEL_6f223a39b93e40ba9b6f11b0006d9803"
            ],
            "layout": "IPY_MODEL_977e6f737e0447108d8232fb8e6fa414"
          }
        },
        "5d0730901efd416da0290b9443f1d138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14a734d954cc490ba831da6850ed30cd",
            "placeholder": "​",
            "style": "IPY_MODEL_ed971930caac4e93aec77df349e077fc",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "252f90b860fc4ccf81d1c8efcc4ac063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a5f50e81ae9433db2df8eccb87f4d3e",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7154d55dc424e80b28c1810f8c6e807",
            "value": 438235074
          }
        },
        "6f223a39b93e40ba9b6f11b0006d9803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed744c2e322d4795ab6ac866fff39ae2",
            "placeholder": "​",
            "style": "IPY_MODEL_de7f19981362478b82c766d1f14ca358",
            "value": " 438M/438M [00:33&lt;00:00, 23.1MB/s]"
          }
        },
        "977e6f737e0447108d8232fb8e6fa414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14a734d954cc490ba831da6850ed30cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed971930caac4e93aec77df349e077fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a5f50e81ae9433db2df8eccb87f4d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7154d55dc424e80b28c1810f8c6e807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed744c2e322d4795ab6ac866fff39ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de7f19981362478b82c766d1f14ca358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}