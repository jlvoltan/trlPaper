{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jKuooZfNFU"
      },
      "source": [
        "# Classificação faixa TRL - Bertimbau + Rede Neural + 512 tokens  [kfold][P2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9-gf3IMD5C"
      },
      "source": [
        "- BERTimbau Base (aka \"bert-base-portuguese-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i41YvsyfSUR"
      },
      "source": [
        "## Código Versão 1.0 - 21 OUT 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfR22FfgbJ"
      },
      "source": [
        "- Uso dos classificadores: *Rede Neural*\n",
        "- **k-Fold cross-validation: Esse código executou a rodada 2**.\n",
        "- Matriz de confusão;\n",
        "- Semente 42\n",
        "- 50 épocas\n",
        "- BATCH_SIZE = 8\n",
        "- Limite 512 tokens\n",
        "- optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.csv   ou  dataset_pre_processado_1.csv  ou  dataset_pre_processado_stem_2.csv\n",
        "#     CSV1                  CSV2                                   CSV3\n",
        "dataset = \"dataset.csv\""
      ],
      "metadata": {
        "id": "vXMRttjhRQ2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lembre-se estamos usando o dataset: \" + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJkf84RR64K",
        "outputId": "ce594b25-ac12-4be5-bb92-356123aa5f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lembre-se estamos usando o dataset: dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "particao=2  # O KFold apresenta 5 partições (rodadas)\n",
        "particao-=1  #Funcionará com o índice de kfold_train e kfold_test"
      ],
      "metadata": {
        "id": "k2TIOYIqQNps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melhor_modelo = 'best_model_TRL_bertimbau_base_neural_' + str(particao+1) + '.bin'\n",
        "melhor_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lFCPPlujqL5L",
        "outputId": "c4b5559b-fc1e-4fe8-ef8f-125098b2cc6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_model_TRL_bertimbau_base_neural_2.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512"
      ],
      "metadata": {
        "id": "_6clpQcISMQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jlIjUzOgRY"
      },
      "source": [
        "- 63,7% treinamento - 16% validação...............20% teste\n",
        "- EPOCHS = 50\n",
        "- lr=2e-5\n",
        "- BATCH_SIZE = 8\n",
        "- Quantidade de tokens máxima = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Msvyn0MfsOS"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC81VL_gQ-5"
      },
      "source": [
        "### Bibliotecas e ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZxPMZOfICS",
        "outputId": "42deafe3-ff7b-4f7b-ba63-84c6908693f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5RDBcpVf0TS",
        "outputId": "4af13b77-6576-403d-9864-4c39e69fb4a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.4/1.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmcqgWcf0WZ",
        "outputId": "3f3614ef-d873-4acc-d2ea-c76b5eb0e34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 25 23:41:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoneZ3CngEDZ",
        "outputId": "0fc52ff8-76e4-48ed-936c-e2936a945954"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWHFe7ef0Zp",
        "outputId": "876fab91-d67b-4b7c-ded1-fae662c23b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy       : 1.23.5\n",
            "pandas      : 1.5.3\n",
            "torch       : 2.1.0+cu118\n",
            "transformers: 4.34.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particoes KFOLD"
      ],
      "metadata": {
        "id": "oENoyHsUPjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para mais detalhes, consulte o código *1-kfold.ipynb* em que as partições foram sorteadas. As células a seguir recriam o resultado obtido."
      ],
      "metadata": {
        "id": "qL3FKCGlPtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = np.array([  0,   1,   2,   3,   5,   7,   9,  10,  11,  12,  13,  14,  15,\n",
        "         16,  17,  19,  20,  21,  22,  23,  25,  26,  27,  28,  30,  32,\n",
        "         33,  34,  35,  36,  37,  39,  40,  44,  45,  46,  47,  48,  49,\n",
        "         50,  52,  53,  54,  55,  56,  57,  59,  60,  61,  63,  64,  66,\n",
        "         68,  69,  70,  71,  72,  75,  76,  77,  78,  79,  80,  81,  82,\n",
        "         83,  84,  85,  87,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
        "        102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
        "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130,\n",
        "        131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145,\n",
        "        146, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
        "        164, 165, 166, 167])\n",
        "\n",
        "array2 = np.array([  0,   1,   3,   4,   5,   6,   7,   8,  10,  13,  14,  15,  16,\n",
        "         17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "         34,  36,  37,  38,  40,  41,  42,  43,  44,  45,  47,  49,  51,\n",
        "         52,  53,  54,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  70,  71,  73,  74,  75,  76,  77,  80,  83,\n",
        "         84,  86,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99,\n",
        "        100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115,\n",
        "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "        129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
        "        147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162,\n",
        "        163, 165, 166, 167])\n",
        "array3 = np.array([  1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
        "         17,  18,  20,  21,  22,  23,  24,  25,  28,  29,  30,  31,  32,\n",
        "         33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,\n",
        "         48,  50,  51,  52,  54,  55,  56,  57,  58,  59,  62,  63,  65,\n",
        "         67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "         82,  84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,\n",
        "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111,\n",
        "        112, 113, 114, 118, 120, 121, 122, 123, 124, 126, 128, 129, 131,\n",
        "        132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 146, 147,\n",
        "        148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162,\n",
        "        163, 164, 165, 166])\n",
        "array4 = np.array([  0,   2,   3,   4,   6,   8,   9,  10,  11,  12,  15,  16,  18,\n",
        "         19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  31,  32,  33,\n",
        "         34,  35,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
        "         49,  50,  51,  53,  56,  57,  58,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  71,  72,  73,  74,  75,  76,  78,  79,  80,\n",
        "         81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
        "         94,  95,  98, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111,\n",
        "        112, 114, 115, 116, 117, 119, 121, 123, 124, 125, 127, 128, 129,\n",
        "        130, 131, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145,\n",
        "        146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159,\n",
        "        162, 163, 164, 166, 167])\n",
        "array5 = np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
        "         14,  15,  16,  17,  18,  19,  21,  23,  24,  26,  27,  28,  29,\n",
        "         30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,  43,\n",
        "         44,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,\n",
        "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  72,  73,  74,\n",
        "         77,  78,  79,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
        "         92,  94,  95,  96,  97,  99, 101, 102, 103, 105, 106, 107, 108,\n",
        "        110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 127,\n",
        "        128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 143, 144,\n",
        "        145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160,\n",
        "        161, 162, 164, 165, 167])\n",
        "# Criar a lista composta\n",
        "kfold_train = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "Cn4vhJ9LPncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as cinco arrays numpy\n",
        "array1 = np.array([  4,   6,   8,  18,  24,  29,  31,  38,  41,  42,  43,  51,  58, 62,  65,  67,  73,  74,  86,  88,  89,  90,  95, 101, 107, 111, 128, 133, 144, 147, 149, 152, 156, 162])\n",
        "array2 = np.array([  2,   9,  11,  12,  21,  28,  32,  33,  35,  39,  46,  48,  50, 56,  72,  78,  79,  81,  82,  85,  87,  91, 103, 106, 108, 131, 134, 140, 141, 146, 148, 158, 159, 164])\n",
        "array3 = np.array([  0,   3,  15,  16,  19,  26,  27,  34,  40,  49,  53,  60,  61, 64,  66,  68,  69,  83,  92,  94, 110, 115, 116, 117, 119, 125, 127, 130, 137, 143, 145, 154, 157, 167])\n",
        "array4 = np.array([  1,   5,   7,  13,  14,  17,  23,  30,  36,  44,  52,  54,  55, 59,  70,  77,  96,  97,  99, 102, 105, 113, 118, 120, 122, 126, 132, 136, 138, 151, 160, 161, 165])\n",
        "array5 = np.array([ 10,  20,  22,  25,  37,  45,  47,  57,  63,  71,  75,  76,  80,  84,  93,  98, 100, 104, 109, 112, 114, 121, 123, 124, 129, 135, 139, 142, 150, 153, 155, 163, 166])\n",
        "\n",
        "\n",
        "# Criar a lista composta\n",
        "kfold_test = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "c3Ia2J4DPpO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4aPPergNnL"
      },
      "source": [
        "### Bertimbau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl6AUm0K3yE"
      },
      "source": [
        "- https://huggingface.co/neuralmind/bert-base-portuguese-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RQrbYpK5vC"
      },
      "source": [
        "```\n",
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "ad2dbb2d7f984ad1ab4993ea6844ae5b",
            "1f70847c897e49b1a1fc78b010a97900",
            "70b1061c935743498963dc8472eaf404",
            "763801631c884164a4b7b097eab11cdf",
            "e9d958c8283f493ba63e370660f58b34",
            "aa47fc762c9d48ee961a205e84b1e4e9",
            "bc864cc9def64dc4bf2ae07445688f79",
            "e8a0d6d755cb45f7bbe4f516d0f24f0c",
            "36d5083a0b2f4402bb411d8052b9a748",
            "b9a063a1d5a94c359da0c5fcac875416",
            "28f3ae7a04b548ce8654ae88209c028a",
            "e127530f156b4b42b95b6d2ff7647b11",
            "898e3ddfe6cd4af2a26862b1d05ee3ef",
            "6e7d655a8b9d41cf89f378beb69dc201",
            "bce9ec6645144acfbec6ee21268bf24e",
            "624d5e5110fa4f6185fde2e9948ba8e6",
            "8113b327f30e43118513c9ff0dc43f39",
            "efcaaafab4374aa7b67abdf81a9bbdd7",
            "c8f7ea0b6b7e458f88395aaca0f5c391",
            "e304d0ac7160401ea1900a297c44acc2",
            "3a8069b70ce048b5b33816b854e7f310",
            "c996f18cc73d4688836cb7aa44ed92a4",
            "1a2e38b141544a44929fde16e91fbd20",
            "63986f683aea4f1082c41d5b6a27bb83",
            "6fd88f5a0fa34cf9b3e608ef5f721d4f",
            "83ae1551c18345cfbe0d24eb49ce1ec7",
            "741f0469f6cc487fb560e10a516d69bd",
            "b593f114c01f46179b0708ef2efbbec4",
            "57612c0d17754e2c897fd70814081dd3",
            "8b8515962a734bb0aadfac54868d71e6",
            "d811322334d3488e94f71ed86671eef6",
            "ec67d8fb6f094967b522b0abd5e904ee",
            "a05f71a528ee4e36a5e258a37312d806",
            "2226a154f0f84f75b353fb171616cabc",
            "f3c6644881b94682b734f7d47bd4b008",
            "297fbd7058744a3eba01fd6a37a122d5",
            "5fd1501c1a944a27b8370795299aefb0",
            "b376d7c0a93a4948a381cb07f4b1b412",
            "5bb9d2c8d5f348089d4c1c2060c306f9",
            "f19a4a21b6c34902af9985a3f21731ff",
            "d2304a5c72064f998ce6081119d5d742",
            "792e3d50c86349a7b4b0cdef02262e7a",
            "a8e6288bc9284089a763025fd21eafea",
            "4775e597223842928f571508938c5bf7",
            "1a064b9451e64da1b66c64ec686febe0",
            "f6b556126c45491cb291664ab66ac18e",
            "5fe39a7da7324652bdd748e9cde6a76b",
            "aada7691529d4dbbba5e2b228cd0c0b7",
            "11c4bd4b875f405c82d1b85094eb1598",
            "84d4e4c8d6fc4b668bc81adcef0ba8ec",
            "853850129a654046b2e21012ff7cec98",
            "e8892c5441ec42faa405687b8439ee06",
            "12d45457cadd4ad8a054ba204df24af3",
            "4d7ce11ae9ff4011bf4b963754b1e4de",
            "a3ab5da0ece54a8da02242fa438072db"
          ]
        },
        "id": "WPj7c-IBgWRx",
        "outputId": "3d1a7ac4-f187-490f-8797-b0dc52fb18db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad2dbb2d7f984ad1ab4993ea6844ae5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e127530f156b4b42b95b6d2ff7647b11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a2e38b141544a44929fde16e91fbd20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2226a154f0f84f75b353fb171616cabc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a064b9451e64da1b66c64ec686febe0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e9f69fe64b6c41deae7713335a6e711a",
            "0c27c43d8d14497cb87b909c11c135cd",
            "e88d31cb26d0403599867198ff8753f8",
            "75baee6367594123b139de9177e6366b",
            "b8c3ff897ac5425d92a3c0302310a332",
            "f3285d5b705c4ddebab7a1b2fb3d3446",
            "fbd276fdda614883a0d21998c65ea9b8",
            "9216a425b3284ed3b6155df8610d8ef8",
            "43e3dda1a733457ea467ae456946d8c1",
            "da599c47048043b7a1a22fd664d73371",
            "0b812457bd094dd2a93caae70f18449f"
          ]
        },
        "id": "qSErznNMh4P5",
        "outputId": "99aed2f0-fb77-4b2e-9694-c4471d705e0d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9f69fe64b6c41deae7713335a6e711a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s_rPsFgWUr"
      },
      "outputs": [],
      "source": [
        "frase = 'A avaliação de prontidão tecnológica em tecnologias de interesse militar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJbGMogWaZ",
        "outputId": "04be98cb-d219-46cb-abf7-83cc932d1734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  frase,\n",
        "  max_length=20,   #Perceba que irei cortar um token da minha frase anterior\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fSHD8KgqDb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYjQrfrglJS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvfsVlnFglPp"
      },
      "outputs": [],
      "source": [
        "novo_df = df[[\"resumo\", \"rotulo\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nth2Ui-qg6Ix"
      },
      "outputs": [],
      "source": [
        "def adapto_faixa(faixa):\n",
        "    faixa -=1\n",
        "    return faixa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKj3lo8Qg9Vj",
        "outputId": "0f78ebc6-2fe4-4add-ec81-7fb3bf044c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3dc243303e67>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)\n"
          ]
        }
      ],
      "source": [
        "novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_cANKWhOhCix",
        "outputId": "6f9f3db2-f68b-48e6-e162-0edc25a8f028"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                resumo  rotulo\n",
              "163  Rio de Janeiro (RJ) – O Centro de Avaliações d...       2\n",
              "164  Este trabalho apresenta um sistema para contro...       2\n",
              "165  No contexto das comunicações táticas baseadas ...       2\n",
              "166  O valor da velocidade de alvos móveis em image...       0\n",
              "167  Neste artigo é apresentada a análise da seção ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6bc2c96e-a6e0-4a4c-b942-22cb0f5fb6ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resumo</th>\n",
              "      <th>rotulo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Rio de Janeiro (RJ) – O Centro de Avaliações d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Este trabalho apresenta um sistema para contro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>No contexto das comunicações táticas baseadas ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>O valor da velocidade de alvos móveis em image...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Neste artigo é apresentada a análise da seção ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bc2c96e-a6e0-4a4c-b942-22cb0f5fb6ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6bc2c96e-a6e0-4a4c-b942-22cb0f5fb6ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6bc2c96e-a6e0-4a4c-b942-22cb0f5fb6ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc0e7e1b-7df3-4aa3-a984-e00a1536bad0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc0e7e1b-7df3-4aa3-a984-e00a1536bad0')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc0e7e1b-7df3-4aa3-a984-e00a1536bad0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "novo_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZK6EQ4xhFHI"
      },
      "outputs": [],
      "source": [
        "class_names = ['Faixa 1', 'Faixa 2', 'Faixa 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmol6gshNCR"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFoXmzH5hFOo"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      #padding='longest',\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K-OuLWzhYhB"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.resumo.to_numpy(),\n",
        "    targets=df.rotulo.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBrznGAWiJWg"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-yCbjniOLI",
        "outputId": "033c66a0-b948-47ca-df90-37ed2d73f1ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxhW_89kRiv"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiZnSuuTiTOZ"
      },
      "outputs": [],
      "source": [
        "class TRLClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(TRLClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    #The last_hidden_state is a sequence of hidden states of the last layer of the model\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kX7ClUGiiCQ"
      },
      "outputs": [],
      "source": [
        "model = TRLClassifier(len(class_names)) #Pois tenho 3 classes\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXw5SIa0hkah"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTSemYwhnlI"
      },
      "source": [
        "### Separação e preparação do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lembrar que estamos utilizando o Kfold"
      ],
      "metadata": {
        "id": "z09fs6pHk5FG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLzT3ttq-0t"
      },
      "source": [
        "    - 63,7% treinamento - 16% validação                20% teste\n",
        "\n",
        "      ------------ 80%----------------------------------20%--"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o índice dos conjuntos\n",
        "index_df_train = kfold_train[particao]\n",
        "index_df_test = kfold_test[particao]"
      ],
      "metadata": {
        "id": "FeqFToMFnaMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = novo_df.iloc[index_df_train], novo_df.iloc[index_df_test]"
      ],
      "metadata": {
        "id": "EpR4mtHLoDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Bhg2qAhSlg"
      },
      "outputs": [],
      "source": [
        "#Separando o treinamento em treino e validação\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, stratify= df_train['rotulo'], random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FAkW0qhW2J",
        "outputId": "7767bf7f-1f3a-43bf-cb54-279329989e9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((107, 2), (27, 2), (34, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxinAAqhW5A",
        "outputId": "7f32d128-43ca-4f73-d529-99f71847c760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fsBv27hW8L",
        "outputId": "47709c1b-c711-4139-9597-bca68820c38b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbQEYoXhvVA",
        "outputId": "c697d597-e12f-4f8f-882e-5b0010c3977b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7qyCztOhvX4",
        "outputId": "bd48fb27-34ba-4ca8-aab1-9151555ebefc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512])\n",
            "torch.Size([8, 512])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP6kiy4whvaa",
        "outputId": "158b260d-539e-4863-af9e-caa4bd04f76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512])\n",
            "torch.Size([8, 512])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzMSQeLlRHn"
      },
      "source": [
        "### Treinamento mesmo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oRhXpG_iBWB",
        "outputId": "73788d66-d8bd-41e6-8ac3-188f17f09ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8dJvCviFTI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKAqpPliFWJ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZisESWlkqH",
        "outputId": "57d61630-7cd8-48d9-9ec3-8298961515ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.8936463807310376 accuracy 0.6074766355140186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5363057218492031 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7114284049187388 accuracy 0.6822429906542056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6003204062581062 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6698245427438191 accuracy 0.7570093457943925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5676515027880669 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5432152982269015 accuracy 0.794392523364486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5278529822826385 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2953362331858703 accuracy 0.9065420560747662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6070940718054771 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.17045484802552632 accuracy 0.9532710280373831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5114901587367058 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.11913096635336322 accuracy 0.9719626168224298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9054040793562308 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.008787999794419323 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2643339857459068 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0339148909468869 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2478240480704699 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.009591820491810463 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.047046911320649 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0015495863709864871 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8935572279733606 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.001392160725247647 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9355557635426521 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.001140177295643038 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9463891460909508 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0010255138414712356 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9526502620719839 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0009412557452118822 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9613417038344778 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007978539215400815 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9706270742753986 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0008080971892923117 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9808720998116769 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007645355196603175 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9914149894466391 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000775679582147859 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9950231002731016 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006297982846652823 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0005362868396332 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005994391540298238 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.007108349527698 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006117197418851512 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.013893987168558 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005784934369980224 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0202813399228035 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005681709839596546 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0265614930540323 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005441930095132973 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0324977121345 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005215957394934126 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0379467648745049 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005044553046380835 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.042728552580229 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005063922788914559 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0473167424206622 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004513985415022554 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0517823287082138 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00046355021394057464 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0557684635423357 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004348458751337603 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.059207943821093 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004422777024696448 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0624619457303197 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004644424999631675 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0657645779720042 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004425518584737022 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0690650705146254 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00042970863952567536 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0721083192984224 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004237729943789808 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0746679708536249 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003944257353266169 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0769449762665317 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00040046742443727065 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0790441757053486 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00039646074895115035 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0806709942480666 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00038250093036497546 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.082136191886093 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00037826670242273916 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0835360281853355 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00041639914187336605 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0849456403811928 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003844278378112774 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0861983101567603 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00038735436932516417 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0872586893601692 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00038857881008880213 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0881615917023737 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003819179068419284 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0889016154324054 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003905962741035702 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0894509063437 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00037475543337807594 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.089872420176107 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0003670425482726257 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0901269614478224 accuracy 0.8518518518518519\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000381203436908046 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.090203899977496 accuracy 0.8518518518518519\n",
            "\n",
            "CPU times: user 8min 57s, sys: 20.8 s, total: 9min 18s\n",
            "Wall time: 10min 16s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     torch.save(model.state_dict(), melhor_modelo)\n",
        "#     best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjGnJKDmZVJ"
      },
      "source": [
        "- Gráfico de desempenho no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQOy_NBBlks7"
      },
      "outputs": [],
      "source": [
        "A= history['train_acc']\n",
        "A2 = [tensor.item() for tensor in A]\n",
        "B= history['val_acc']\n",
        "B2 = [tensor.item() for tensor in B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "hHRH2aNQlkvy",
        "outputId": "03454976-a301-48bb-9c8c-e36afa89b5d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB/MAAAWaCAYAAAAHDkSJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAB7CAAAewgFu0HU+AAEAAElEQVR4nOzdd3wUdf7H8femk04oAUKkCUEEBSkKqCA2UEEsiB31LHiCnucPkbtT8e4UxNM7ARtnQbEXFARFRZqIBJDepUloCSG9J5v5/ZFjyKRuyG5mk7yej4ePm+93vzPz2dmwcHnP9zsOwzAMAQAAAAAAAAAAAAAAr+FjdwEAAAAAAAAAAAAAAMCKMB8AAAAAAAAAAAAAAC9DmA8AAAAAAAAAAAAAgJchzAcAAAAAAAAAAAAAwMsQ5gMAAAAAAAAAAAAA4GUI8wEAAAAAAAAAAAAA8DKE+QAAAAAAAAAAAAAAeBnCfAAAAAAAAAAAAAAAvAxhPgAAAAAAAAAAAAAAXoYwHwAAAAAAAAAAAAAAL0OYDwAAAAAAAAAAAACAlyHMBwAAAAAAAAAAAADAyxDmAwAAAAAAAAAAAADgZQjzAQAAAAAAAAAAAADwMoT5AAAAAAAAAAAAAAB4GcJ8AAAAAAAAAAAAAAC8DGE+AAAAAAAAAAAAAABehjAfAAAAAAAAAAAAAAAvQ5gPAAAAAAAAAAAAAICXIcwHAAAAAKAO3XHHHYqLi1NcXJyGDBlidzmKj48364mLi9PcuXPtLslrPfHEE5Zr5QmHDh2ynGPGjBkeOQ8AAAAAwPv52V0AAAAAAKDxOXTokC699FKPnmPcuHEaP368R88BAAAAAADgKczMBwAAAAAAgCRWBgAAAAAAb0KYDwAAAAAAAAAAAACAl2GZfQAAAABAnWvVqpV+/PFHl8b++c9/1qZNm8z2Sy+9pHPPPbfa/cLDw0+7PgAAAAAAALsR5gMAAAAA6pyfn5/atm3r0tjAwEBLu3nz5i7v643mzJljdwkW559/vnbt2mV3Gfiftm3b8nkAAAAAACSxzD4AAAAAAAAAAAAAAF6HMB8AAAAAAAAAAAAAAC/DMvsAAAAAgEZj9+7d2rNnj44fP67c3FzFxMRo+PDhlY7PycnRb7/9pv379ys1NVV5eXkKCwtTVFSUunfvrjPOOKMOqy8vISFB27Zt07Fjx+R0OtWsWTP17t1bsbGxttRTWFiodevW6dChQ0pJSVFYWJjatWunPn36lHtcQk1t27ZNu3btUnJyskJCQtSqVSv16tVLUVFRbqq+9pKSkrRp0yYdPXpU+fn5ioqK0jnnnKPOnTvXyfkTExO1fft2HTlyRFlZWZKkoKAgtWjRQrGxsYqLi1NAQECd1FLWzp07tXv3bqWkpKigoEDNmjVT27Zt1atXL7fXtHnzZh08eFBJSUkqKipS586ddckll7j1HAAAAABQFwjzAQAAAAANxpAhQ3T48GFJUr9+/czn03/xxRd655139Ntvv1nGh4WFlQvzDx8+rIULF2rp0qXasmWLCgsLKz1fTEyM7rzzTt18880KCgpyqcY77rhDa9asMfdfsmRJjcdu2rRJL730kuLj42UYRrn9zj33XE2aNEm9evWqtp74+HjdeeedZnvKlCm6/vrrazS2oKBAr776qj755BOlpKSU2y84OFhjxozR2LFjXb5OJ3311VeaMWOGDh06VO41f39/XXbZZXr88cfVpk2bGr0Xd9q3b59eeOEFrVixQkVFReVe79ixoyZOnKjBgwdXe6xDhw7p0ksvNdvjxo3T+PHjq9xn8eLFevPNN7Vhw4Yqx/n7+6tnz5666qqrdOutt1peK/2zVtrMmTM1c+bMCo9X3c9vXl6eZs+erY8++kjHjh2rcExwcLCGDh2qRx55RK1ataqy/pPi4uLM7euuu05Tp05VcXGx3nnnHX344Yflfla6du2qSy65RDfffLN5jQIDA/XTTz8pIiLCpXOeNG7cOP3www+SJB8fHy1evFgxMTE1OgYAAAAAuIpl9gEAAAAADVZBQYEeeeQR/eUvfykX5FfE6XTq0ksv1Ysvvqj169dXGeRLJcH/lClTNHr0aPMmAk+bM2eObrvtNq1evbrCIF8qCfvvuOMOffPNNx6v59ixY7rlllv02muvVRjkSyUrHLz22mu65557zBnj1SksLNTDDz+siRMnVhjknxzz7bff6rrrrlN8fPxpv4faWLRokW644QYtWbKkwiBfKgn7H3jgAc2ePdut53Y6nZo4caIeeuihaoN8qeR6rV27Vi+99JJb66jInj17dNVVV+nf//53pUG+VPKzMXfuXF155ZWaP3/+aZ0rPT1dY8aM0bRp0yr9WZGkm2++2dzOz8+v8fmSk5O1bNkysz1gwACCfAAAAAAexcx8AAAAAECD9eyzz2rRokWSJIfDoW7duikmJkYOh0MJCQnlgj/DMCwBucPhUNu2bdWuXTuFh4fL4XAoNTVVO3bsUGpqqjlu586duueeezR37lyFhIR47P3MmzdP//znP812ly5ddMYZZyggIEAHDx7Utm3bzPoLCws1adIkdevWTe3bt/dIPbm5uXrggQe0c+dOSVJoaKjOOeccRUVFKTs7Wxs3brRcp19//VVTpkzRs88+W+2xH3vsMX333XeWvqCgIJ177rlq0aKFMjIytHXrVqWkpCgtLU3jx4/XX/7yF/e+wWrEx8frscceM0P89u3bq2PHjgoODtaRI0e0efNmS8A/depUde/eXX369HHL+adPn66vvvrK0hccHKyzzjpLLVq0kL+/v7Kzs5WUlKS9e/cqNzfXLeetzs6dOzVmzBilpaVZ+tu2bavOnTsrMDBQCQkJ2r59u/nzmpeXp8cff1y5ubkaPXq0y+cyDEMTJkwwVxXw8/NTjx491KpVK+Xn5+v33383xw4dOlTPPfec0tPTJUmff/657rjjDpfP9eWXX1pu8Lnxxhtd3hcAAAAATgdhPgAAAACgQdq6dasZ8I0YMUKPPfZYuWW8K5rF6+fnp0svvVRDhw7VRRddpLCwsHJjiouL9fPPP2vatGnavXu3JOnAgQP617/+paefftoD70ZKTU3Vk08+KUnm0vLt2rWzjNm7d68effRR7dq1S1JJQPqf//xH//nPfzxS0/Tp05WWlqbIyEhNmDBBI0eOlJ/fqV81FBUV6e2339ZLL71khraff/657r77bp155pmVHvfzzz+3BPm+vr564IEHdN999yk4ONjsdzqdWrhwoZ599lmlpaVpypQpHniXlXv44YdVVFSkPn366C9/+YvOPvtsy+tHjx7VxIkTzVUDDMPQ888/r88++6zW505LS9Nbb71ltoODgzVp0iSNHDmywmfQO51ObdiwQT/88IO5THxpL730kvLz83Xs2DHddtttZv+dd96pMWPGVFhD6c/6pLy8PP35z3+2BPlnnHGG/v73v6t///6WsQkJCXrmmWf0008/SSq5Pv/85z917rnnqmvXrlVfgP/5/vvvlZOTI4fDoTFjxujBBx9UZGSkZczJP+dBQUEaMWKE+fiNnTt3asuWLerRo4dL5/r888/N7aioKMvjEAAAAADAE1hmHwAAAADQIOXk5EiS7r//fr3wwgsVPo+7bdu2lravr69++OEHTZ8+XVdddVWFQb5U8qzsiy66SJ988ol69uxp9s+dO7fcbGR3ycnJUX5+vm677TbNnDmzXJAvSZ06ddLbb7+t8PBws+/HH380ZyK728kg/8MPP9SNN95YLtz18/PT/fffr/vvv9/SP3fu3EqPmZ+frxdeeMHS99xzz+mRRx6xBPlSyec1YsQIvfvuuwoLC/PYta9MWlqaLrvsMs2ePbtckC9JrVu31qxZsxQbG2v2bd68WXv27Kn1uVetWmWZJT558mTddNNNFQb5Usm16tOnjyZNmqRvv/223OstWrRQ27Zty/05CQ8PV9u2bSv8r6I/U2+//bb27t1rttu1a6ePP/64XJAvSbGxsZo1a5aGDh1q9hUUFGjy5MnVvv+TTv45nzx5siZNmlQuyJesf85LL7UvyeUbK9auXasDBw6Y7cpumgAAAAAAdyLMBwAAAAA0WGeddZb+9Kc/uTze4XCoTZs2Lo8PDg7WM888Y7bz8vK0ZMmSmpRYI126dNGkSZPkcDgqHdO8eXPdcsstZrugoEAbN270WE1PPvmkOnXqVOWY++67T4GBgWZ77dq1lY799ttvLaH80KFDNXLkyCqP37VrVz366KMu1etOzZo109SpU+Xv71/pmKCgIN13332WvpMrRtTGkSNHLO3LL7/c5X1LfxbuVFhYqI8++shsOxwOTZs2Tc2aNat0Hx8fHz377LNq2bKl2bdhwwZt2bLF5fNecskl5UL6ypx55pk677zzzPbChQtdevxA2dCfJfYBAAAA1AXCfAAAAABAgzVmzBj5+vp69Bxdu3a1zPzdtGmTx841ZsyYKoPjky6++GJL++Sy++4WExOjq666qtpxYWFhlgB1165d5rL7ZS1atMjSLhuEV2bUqFEVzsr2pNGjR1e6ekNpgwYNsrR37tzp9lpSUlLcfsyaio+PV1JSktm+6KKLLCtXVCY0NFT33nuvpW/+/Pkun/eee+5xeaxU8rmdlJWVVe5nrqzMzEzLYx/OO++8am9gAQAAAAB3IMwHAAAAADRYl1xyiduOlZ+frxMnTujw4cM6dOiQ5b/SIfK+ffvcds6yLrroIpfGdezY0dL2VNA7cOBA+fi49quF0jXl5+crOzu7wnGlVxGIiYlR9+7dXTp+QECABg8e7NJYd3H182jVqpXlEQGpqam1PneHDh0s7RdffFFOp7PWx62NDRs2WNpXX321y/tec801lhUnyh6rMmFhYerbt6/L55GkYcOGKSIiwmx//vnnVY7/+uuvlZeXZ7ZvuummGp0PAAAAAE6XX/VDAAAAAACof9q0aVOrmdoHDhzQggULFB8fr927d7v8PPaMjIzTPmdVQkNDFR0d7dLYsrPFs7KyPFFSjWYnl60pOztboaGhlr6kpCRL0N2tW7ca1dOtWzd99dVXNdqnNmry/kNDQ83nu7vj8+jfv7+aNm1qXq9vvvlGO3fu1OjRo3XZZZdZVouoK9u2bbO0zz33XJf3bdasmdq2bauEhARJJasXOJ3OalfW6Nq1a5WPnahIYGCgrr32Wr333nuSpHXr1mn//v3lbpA4qXTYHxYWpqFDh9bofAAAAABwupiZDwAAAABokJo2bXpa+2VkZOivf/2rhg4dqhkzZmjNmjUuB/mS54JzV5ZzP6nsUvxFRUXuLkeSyoXxVfHzs84nKCwsLDem7HVu1apVjepp3bp1jcbX1ul+Ju74PIKDg/XUU09Zgux9+/ZpypQpuvTSSzVkyBBNmDBBn3zyifbv31/r87mi9AoQDodD7dq1q9H+pcP0wsJCZWZmVrtPVFRUjc5xUuml9iXps88+q3Dcjh07LDcpXH311WrSpMlpnRMAAAAAaoowHwAAAADQIIWEhNR4n/T0dI0ZM0aff/55pc90r87p7lcdV5ezr0vurqlseFvTz7AmNxe4g92fyVVXXaVXX321wpseDh8+rPnz5+upp57S0KFDdfXVV+udd95Rbm6ux+opvSpFkyZNanx9yt4c4coqF6UfX1ATZ555pnr37m22582bV+FNFp9++qmlzRL7AAAAAOqS9/0mAAAAAAAAm0ydOlXbt28324GBgRo5cqSmTZumr776SqtWrdLGjRu1Y8cO7dq1y/yvX79+NlbdcNR2RYGCggJ3llMvDBkyRN9//72ef/55DRo0qNJwe8+ePZo6daqGDRvm8vPoG7rSs/OTk5O1dOlSy+t5eXlasGCB2e7WrZvOPvvsOqsPAAAAAPyqHwIAAAAAQMN39OhRffnll2a7ZcuWevfdd9WxY8dq983OzvZkaY1GRESEpe3KzOzS0tPT3VlOvXHyppORI0eqqKhIO3bs0Pr167VmzRqtWrVKOTk55tijR4/q3nvv1WeffebSz3ZNhIeHm9u5ubkqLi6u0ez8siszlD6eJwwdOlTPPfec+XiHzz77TJdffrn5+qJFiyw/gzfeeKNH6wEAAACAspiZDwAAAACApOXLl1uWyJ8wYYLLYefx48c9VVaj0rJlS/n6+prt3377rUb779mzx90l1Tt+fn7q0aOHxowZo1deeUXx8fGaNm2aWrdubY7JysrS9OnT3X7u0s+vNwxDBw8erNH+Bw4cMLf9/f3LLbvvboGBgbr22mvN9sqVK5WYmGi2v/jiC3M7KChII0aM8Gg9AAAAAFAWYT4AAAAAAJJ+//13S/vCCy90ab+jR48qKSnJEyU1Ok2aNFHnzp3N9vbt25WVleXy/mvXrvVEWfVaQECArr32Wr3zzjtq0qSJ2b98+XI5nc5y4x0Ox2mfq+wS9Js2bXJ535SUFCUkJJjtrl27Wm7s8JTSS+07nU4zwP/999+1Zs0a87WhQ4d6/OYCAAAAACiLMB8AAAAAAKlcaBwaGurSfl9//bUnymm0zj//fHM7Pz9f33zzjUv77du3j2fBV6FDhw7q2bOn2c7JyTGXly8tICDA0i4sLHT5HL169bK0v/32W5f3XbBggWVljNK1elKnTp3Up08fsz137lwZhqHPPvvMMm7UqFF1Ug8AAAAAlEaYDwAAAACAVG7WbeklvyuTkpKi2bNne6agRqpsaDp9+nSlp6dXuY9hGHruuec8WVaDUPYGFX9//3Jjyv45qMkjJM4//3y1aNHCbC9fvlxbt26tdr/s7Gy99dZblr66XNK+9Oz8hIQErVy5Ul999ZXZ16FDB0vgDwAAAAB1hTAfAAAAAABJXbp0sbTfeeedKsfn5ubq0Ucf1YkTJzxZVqPTuXNnXXLJJWb7+PHjeuCBB5Samlrh+MLCQj3zzDP66aef6qpEr7Bo0SLt2bPH5fHJycn65ZdfzHbz5s0VHh5eblxQUJBat25tttetW1fhcvwV8ff3180332y2i4uL9fjjj1f62Z0c8+STT+rYsWNmX8+ePXXOOee4dE53GDp0qCIjI832k08+abmJgVn5AAAAAOxCmA8AAAAAgKSLL77Y8kzxuXPnasqUKRU+s33dunW65ZZbtHr1ajkcDksQiNqbPHmyZRb5hg0bNGzYMM2YMUPr1q3T/v37tXnzZr3//vu67rrr9NFHH0kqCWUbi2XLlumaa67RXXfdpU8//VRJSUmVjl23bp3GjBlj+VkePnx4peNLz0I/ePCgHn74YS1fvlz79u3ToUOHzP9KB/An3XvvverQoYPZ3rt3r2655RbL8+dPSkhI0NixY7Vw4UKzz9/fX5MnT660Nk8ICAjQyJEjzfbRo0ct9Vx33XV1Wg8AAAAAnORndwEAAAAAAHiDqKgo3X333Xr11VfNvtmzZ+vTTz9Vz5491axZM2VlZWnXrl06cuSIOebuu+/W1q1bKwwrcXpatWqlV155RWPHjlVubq4kKTU1VTNnztTMmTMr3OfKK6/UrbfeqkWLFpl9DoejTuq1i2EY+uWXX8wZ99HR0erYsaMiIiLk7++v9PR07dq1S4mJiZb9YmJi9NBDD1V63Ntuu83yDPvFixdr8eLF5cbFxMRoyZIllr6goCC99NJLGjNmjDIyMiRJ+/fv1x133KEzzjhDnTt3VkBAgA4dOqStW7ea55BKPq+//OUvOuuss07vgtTCTTfdVOEjM4YMGaKoqKg6rwcAAAAAJMJ8AAAAAABM48aN0969e/Xdd9+ZfTk5OVq1alWF40ePHq0JEyZozJgxdVVio3HBBRdo9uzZmjRpkvbt21fl2HvuuUf/93//p5UrV1r6g4ODPVmi10lMTCwX3JfVpUsXvfHGGwoLC6t0TK9evTRx4kS98MILLi+xX1q3bt30/vvva+zYsZYbXw4ePKiDBw9WuE9gYKD+/ve/W2bI16VOnTqpb9++Wrt2raX/xhtvtKUeAAAAAJAI8wEAAAAAMPn6+urll1/WnDlzNGvWLMtzs0vr1auX7rnnHl1xxRV1XGHj0rNnT82bN08LFy7UokWLtHv3biUnJyskJEStW7dWv379dOONN6pz586SpMzMTMv+VQXW9d2jjz6q7t27a9myZdqwYUOFj4MorUuXLho9erRuvvlm+flV/+ugu+++WxdddJHmzp2r9evX6/fff1dWVpYKCgpcqi8uLk7ffPON3nnnHX300UeVPgYgODhYV155pR5++GG1adPGpWN7yujRoy1hfps2bXThhRfaWBEAAACAxs5hlF7PDAAAAAAASJIKCwu1efNm7dq1SxkZGQoNDVWLFi3UrVs3xcbG2l0eKjB9+nS98sorZnv+/PmKi4uzsaK6UVxcrH379unAgQM6duyYsrOzJUkhISFq1aqVzjrrLMXExNha444dO7Rr1y6lpqaqsLBQTZs2VWxsrM477zwFBATYWttJy5Yt0wMPPGC2x48fr3HjxtlYEQAAAIDGjjAfAAAAAAA0CGPGjNHq1asllSzbvn79epdmoQOS9PDDD5uP2PDx8dGSJUvUunVrm6sCAAAA0Jj52F0AAAAAAABAbR08eFDx8fFmu1u3bgT5cFlycrKWLFliti+88EKCfAAAAAC24//VNhAFBQVat26dDh8+rJSUFEVFRSkmJkZ9+vTxmuXqAAAAAADwBMMwNHnyZJVefPCaa66xsSLUNx988IEKCwvN9i233GJjNQAAAABQgjC/hgoKCrRr1y5t3bpVW7Zs0ZYtW7R37145nU5zzK5du+qsnry8PE2fPl1ffPGF0tLSyr0eGRmpG264QQ8//LCCgoLqrC4AAAAAAGpj1qxZioyM1MiRI6u8ST0rK0t/+9vf9PPPP5t9YWFhGjFiRF2UiQbg0KFDmj17ttmOjY3VoEGD7CsIAAAAAP6HML8GbrzxRu3cudNyp7adDh8+rPvvv1979uypdExaWpreeustLV++XLNmzVJMTEwdVggAAAAAwOk5duyYXnzxRb344ou68sor1bt3b3Xo0EERERHKzc3VsWPHFB8fr7lz55a7uf2vf/2rwsPD7SkcXu/QoUOSpOzsbG3dulUzZ85UTk6O+fof//hH+fr62lUeAAAAAJgcRuk16FCluLg4l8bVxcz8rKws3XLLLdq9e7fZ16lTJ1111VWKjo7WsWPH9M0332jfvn3m6126dNFHH32k0NBQj9cHAAAAAEBt/P3vf9cHH3xQ4/3uvfdeTZgwwQMVoaGo6vc7vXr10ocffigfH586rAgAAAAAKsbM/NMUGhqqbt26qUePHlq/fr02bNhQp+f/17/+ZQny//CHP2jChAlyOBxm37hx4zRt2jS9/fbbkqTdu3frxRdf1NNPP12ntQIAAAAAUFMRERE1Gh8dHa0///nPGjlypGcKQoPXtm1b/fvf/ybIBwAAAOA1mJlfA//85z/VvXt39ejRQx07djSD8yeeeEJffvmlOc7TM/MTEhI0bNgwc7n/Sy65RK+//nql48eOHaulS5dKkvz9/fXtt98qNjbWozUCAAAAAFBbv//+u1asWKENGzZo3759OnbsmLKzs2UYhsLCwtSsWTP16NFDAwYM0JVXXqmAgAC7S0Y9UHpmflBQkNq1a6fLLrtMd999t8LCwmysDAAAAACsCPPdoK7D/GnTpumtt96SJDkcDi1atEjt27evdPyBAwd05ZVXmu0//OEPevzxxz1aIwAAAAAAAAAAAADg9LFuWD30448/mtt9+/atMsiXpPbt26tv374V7g8AAAAAAAAAAAAA8D6E+fXM77//rgMHDpjtAQMGuLRf6XEHDhzQwYMH3V0aAAAAAAAAAAAAAMBNCPPrmd27d1vaPXv2dGm/Xr16VXkcAAAAAAAAAAAAAID3IMyvZ/bu3Wtpn3HGGS7tFxsbW+VxAAAAAAAAAAAAAADegzC/njl06JC57ePjo+joaJf2i46Olo/PqY87ISHB7bUBAAAAAAAAAAAAANzDz+4CUDNZWVnmdkhIiPz8XPsI/f391aRJE2VnZ0uS+b91paCgQGlpaWY7MDBQvr6+dVoDAAAAAAAAAAAAAHiC0+lUfn6+2Y6MjFRAQECtjkmYX8/k5OSY24GBgTXaNygoyAzxSx+nLqSlpbEaAAAAAAAAAAAAAIBGo2XLlrXan2X265nSd3P4+/vXaN/Sd37k5eW5rSYAAAAAAAAAAAAAgHsR5tczpWfjFxYW1mjfgoICczsoKMhtNQEAAAAAAAAAAAAA3Itl9uuZ4OBgc7v0LH1XlJ6NX/o4daHsIwFiY2PrvIaGZs+ePXI6nfL19dWZZ55pdzkAqrA3x9Cf9khpRdb+c0Ok5ztJu3OlpanSijTpRFGFhyjnzCbSJZHSJU2l2CCHu0s+LcWGoS1ZJe9lWZrr76VTUMn7uKSpdIaXvBe+YwHAc/iOBQDP4nsWADyH71gA8JyG8B2bk5Njeex4TR+ZXhHC/HomNDTU3M7JyVFRUZH8/Kr/GIuKipSbm2u2Q0JCPFJfZXx9fS3t4OBgy3tBzfn4+MjpdMrHx4drCXixLVmGLt8tJZdZTOWiCGnmOVKon0PRkdJFrSWnYWhlmvTZcemL41JiQUVHLLEpS/oiS9Ih6dxQaVQLaVRLqXNw3YbhxYahn9Olz5KkucelI1XUXFqPEOnGliV1dw3xjgC/NL5jAcBz+I4FAM/iexYAPIfvWADwnIb4HVs2Hz0dhPn1TNu2bc1tp9OpxMRExcTEVLvfsWPHVFxcbLZjY2M9Uh8A4JQtWYYu3VhxkL/wf0F+ab4OhwY1lQY1lV7uXBLsf3q8JCSvLtjflCX9bb/UM9TQjR4O9htqgA8AAAAAAAAAgDchzK9nOnbsaGkfPHjQpTC/9JIOFR0HAOBeW2sY5JdVOtif3tnQT2klM/arC/Y3ZpX85+5gv9gwtCpd+rSGAX73kJLzE+ADAAAAAAAAAFAzhPn1TFxcnKW9ceNG9e/fv9r9NmzYYGl36dLFrXUBAE7ZmmVoyMbyQf6FLgb5Zfk6HBrcVBpcJtj/IklKKqx8v9oG+ycD/JPnqkmAf/JcZxHgAwAAAAAAAABwWgjz65l27dqpXbt2+v333yVJq1at0oMPPljtfqtWrTK327dvr3bt2nmsRgBozKoK8r85jSC/rIqC/ZOz5d0R7BPgAwAAAAAAAADgHQjz66FLL71Ub7/9tiRp7dq1OnDggNq3b1/p+AMHDmjt2rVme8iQIZ4uEQAaJU8H+WWVDvZndDG0Iu3Uc+xrEuyPaimdFyp9k1KzAP/skJLl8wnwAQAAAAAAAABwP8J8LzFkyBAdPnxYkhQTE6MlS5ZUOvaWW27RnDlzVFhYKMMw9Pzzz+u1116rdPzUqVPNbX9/f916663uKxwAIKnug/yyfB0OXdJUuuQ0g31XnV1qBn43AnwAAAAAAAAAADzGx+4CUHNnnHGGrr/+erO9ZMkSvfDCCzIMwzLOMAxNmzZNS5cuNftuuOEGxcbG1lmtANAYbM0ydOnGioP8hXUQ5JdVEuw79GqcQ4cHSj/2lB5oI7X0P73jnR0iPd1e2tpP2tLPoac7OAjyAQAAAAAAAADwMGbm18B7772nOXPmlOs/ceKEpX355ZeXG9OqVasK9z1djz/+uH799Vft2bNHkvTmm29q2bJlGjZsmKKjo5WYmKiFCxdq37595j6dO3fWhAkT3FYDAOBUkH+8kiA/rI6D/LIsM/Y7G1qRfmrGftmaS+sWXDL7nhn4AAAAAAAAAADYgzC/BtLT03Xw4MFqx1U0xul0urWW0NBQvfHGG7rvvvvMwH7Pnj2aMWNGheM7duyo119/XaGhoW6tAwAaM28P8svy83FoSFNpSJlg/8v/BftnBUs3/i/AP5sAHwAAAAAAAAAAWxHm12Nt27bVl19+qZdffllffPGF0tPTy42JiIjQDTfcoEceeURBQUE2VAkADVNlQf5ALw3yyyod7L8WZ3c1AAAAAAAAAACgLML8Ghg/frzGjx/vkWMvWbLktPYLCgrSxIkT9eijj2rt2rU6fPiwUlNT1bRpU8XExKhv374KCAhwc7UA0Lhty648yP+mHgT5AAAAAAAAAADA+xHmNxABAQEaOHCg3WUAQIO3LdvQkA0E+QAAAAAAAAAAwLN87C4AAID6giAfAAAAAAAAAADUFWbmAwDggsqC/AHhBPkAAAAAAKDmDMNQbm6usrKylJOTI6fTqeLiYrvLQhWKiorM//3tt99srgYAGpa6+o719fWVn5+fwsLCFBYWJj8/747Lvbs6AAC8QFVB/rfnEuQDAAAAAICaSUtLU1JSkpxOp92loAZ8fX3N7ZOhEwDAPerqO7aoqEj5+fnKzs7WsWPHFB4ertatW8vHxzsXtCfMBwCgCtuyDV1KkA8AAAAAANzAMAwlJycrOTm53Gs+Pj5eGySghMNx6vdApUMnAEDt1dV3rNPplGEYZjsjI0NOp1Nt27b1yr+HCfMBAKjEySA/iSAfAAAAAAC4wfHjx3XixAmzHRoaqrCwMIWEhMjf39/GyuCKnJwcGYYhh8Oh4OBgu8sBgAalrr5jDcNQfn6+MjIylJqaquLiYmVnZ+vo0aOKiYnx2HlPF2E+AAAVqCrI/4YgHwAAAAAA1FBxcbFSU1PNdnR0tKKiomysCACAxsfhcCgoKEhBQUEKDQ1VQkKCiouLlZGRoejoaPn5eVd87n1rBQAAYLPqgvxwgnwAAAAAAFBDmZmZKi4uliRFREQQ5AMAYLPg4GA1bdrUbGdmZtpYTcUI8wEAKGU7QT4AAAAAAPCAjIwMczsyMtK+QgAAgCk8PNzcJswHAMCLbc82NKSCIL8/QT4AAAAAAKilwsKSXzg4HA41adLE5moAAIAkBQYGyuEo+d1/UVGRzdWUR5gPAICqDvK/JcgHAAAAAAC15HQ6JUm+vr5maAAAAOzlcDjk6+sr6dTf1d6EMB8A0OgR5AMAAAAAAAAAAG9DmA8AaNQI8gEAAAAAAAAAgDcizAcANFrbsw1dupEgHwAAAAAAAAAAeB/CfABAo3QyyE8ssPZfQJAPAAAAAAAAAAC8AGE+AKDRqSrIX0SQDwAAAAAAAAAAvABhPgCgUSHIBwAAAAAAAAAA9QFhPgCg0VieytL6AAAAAAAAAACgfiDMBwA0eNlOQw/vNnTJxsqD/AiCfAAAAAAAAMBixowZiouLU1xcnO644w67ywGARsfP7gIAAPCkFWmG7tkh7csr/xpBPgAAAAAAAAAA8FaE+QCABinbaegv+6QZhyp+fXCk9GUPgnwAAAAAAAC4T3x8vNasWSNJiomJ0fXXX29zRQCA+owwHwDQ4KxIM/SHndLe3PKvBflIz3aUHm4r+ToI8gEAAAAAAOA+a9as0cyZMyVJ/fr1I8wHANQKYT4AoMGobjb+gHDp7bOkLsGE+AAAAAAAAEB1xo8fr/Hjx9tdBgA0WoT5AIAG4ac0Q/dUMRv/nx2kR2KZjQ8AAAAAAAAAAOoHwnwAQL12cjb+zEOSUcHrA8Klt86S4piNDwAAAAAAAAAA6hHCfABAvcVsfAAAAAAAADR0xcXF2rBhgw4ePKjjx48rKChIF110kTp06FDh+OTkZO3evVu///67MjMz5XA4FBkZqY4dO+qcc86Rv79/ndafl5en+Ph4HTp0SNnZ2WratKl69uypzp07e/zcRUVF+u2337R3714lJycrNzdXYWFhatasmc477zxFR0fX+hwpKSlav369jh8/rvT0dAUEBKhly5aKi4vTmWeeKUcNfzeZlZWlX3/9VYmJiUpNTZWvr6+aN2+uzp07q2vXrvL19a11ze6WmZmpNWvWKCkpSRkZGYqKitLIkSMr/FkzDEN79+7Vnj17dOzYMeXm5io4OFjNmjXTOeecozPOOKPW9dTHawhUhjAfAFDv5PxvNv6MSmbj9w+X3mY2PgAAAAAAAOpIXFxcub41a9ZU2C9J48aNszyLPj4+XnfeeafZ3rVrlwzD0Lvvvqt33nlHx44ds+w/adIkS5i/e/duzZs3T0uXLtXevXsrrTM4OFg33XSTHnjgAUVFRVX7vmbMmKGZM2dKkvr166c5c+a4PK6goEAzZszQxx9/rIyMjHL7dO/eXZMnT1aPHj2qraMm8vLy9P333+ubb77RmjVrlJ2dXenY7t27a9y4cbrkkktqfJ7ly5frtdde08aNG2UYFf2WUmrevLmGDRume++9V61ataryeBs2bNDMmTO1evVqFRUVVTgmPDxcl112me6991516tTJ8tqhQ4d06aWXmu0ff/xRbdu2rfZ9PPHEE/ryyy8lSdddd52mTp3q8rjk5GRNmTJF33//vQoKCizjr7zySjPMLyoq0rJly7Rw4UKtWrVKaWlpldbToUMHjR07Vtdee22Nb4Q43WuYl5enCy+8UJmZmZLK//mszldffaWJEydKkhwOhxYvXuzStQdc4WN3AQAA1MRPaYbOXStNryDID/KRXugkrTiPIB8AAAAAAAD1V2FhoR544AFNmTKlXJBfkSeeeEJvvvlmlUG+JOXk5Gj27Nm64YYbtHv3bneVW056erpuv/12zZo1q8IgX5K2bt2qO+64Q2vXrnXruX/55RdNmDBBS5curTLIP1nD2LFjNXXq1EoD+bJyc3P10EMP6f7779eGDRuq3C85OVlz5szRqlWrKh3jdDo1efJk3XzzzVq5cmWlIbQkZWRkaO7cufrmm29cqtWTtm3bpmuvvVYLFiwoF+SXtW/fPj300EP65ptvqgzyJWn//v2aOHGiHnvssWqPe1Jtr2FQUJCuvvpqs/3ll1+6/PMgSXPnzjW3L7jgAoJ8uBUz8wEA9UKO09Bf91Uc4kvMxgcAAAAAAIB9Ti4Nnp6ervT0dElSYGBgpcu4R0REVHm8559/XsuXL5dUMnt88ODBatWqlbKzs7V9+3YFBQVVuJ/D4VC3bt3Us2dPnXHGGQoLC1NeXp7279+vJUuW6PDhw5KkI0eOaOzYsZo/f75CQ0NP6z1Xpri4WH/+85+1adMm+fr66uKLL1afPn0UGRmplJQU/fjjj9q4caOkkmB8woQJWrhwoUJCQtxahyRFRkaqd+/e6tatm5o1ayZ/f3+dOHFCGzZs0IoVK+R0OiVJ77zzjtq0aWNZHaEi+fn5GjNmjDZt2mT2+fv7q3///urTp4+aNWum/Px8HTlyROvXr9fGjRtVXFxc6fEMw9DDDz+sxYsXm30+Pj7q06ePzj//fEVHR6uoqEiJiYnatGmT1q5dq8LCwlpeldpLT0/X+PHjlZycrMDAQF1yySXq1auXQkJClJycrKVLl1Y6qz44OFi9e/dW9+7d1aJFCwUFBSktLU2bN2/W0qVLlZ+fL0lauHChWrRooUmTJlVZi7uu4ahRo/Txxx9Lkg4fPqzVq1erf//+1V6LQ4cOac2aNWb7hhtuqHYfoCYI8wEAXm9lmqF7dkp7csu/FuQj/aOD9KdYybeGyy4BAAAAAAAA7vDDDz9Isi43f+6551a6LH115syZo4CAAE2ZMkXXXHNNteNDQkI0duxYjRo1qtJZwZMmTdLbb7+tF198UYZh6PDhw3rttdc0YcKE06qxMuvXr1dxcbFiY2M1c+ZMde3a1fL6/fffr9dee03/+c9/JElHjx7VF198UW2QXhO9evXSfffdp4svvrjC57ZLJTPAH3nkEe3atUuS9OKLL2r48OFq2rRppcd97rnnLEF+v3799Oyzz1b6nPdjx47p3XffVZMmTSp8/b///a8lhO7SpYuef/55devWrcLxKSkp+vTTTz1y40NNLFmyRJJ01llnacaMGYqNjbW8/uCDD5bbp3Pnzrr//vt1+eWXV3o9kpKS9Nhjj5nh+Lvvvqsbb7xRnTt3rrQWd13D7t2766yzztKOHTsklcy2dyXMnzt3rjmLPzw8XFdccUW1+wA1wTL7AACvleM09OhvhgZtqDjI7x8ubegrPXaGgyAfAAAAAAAADco//vEPl4J8SXrzzTf16KOPVrm8t6+vr+677z5L0Pr555+7vJS5q4qLixUWFqZ33323XJB/0oMPPqg+ffqY7YULF7rt/AMGDNDHH3+sSy+9tNIgXyp5Nvvbb7+tqKgoSSXPTT/5TPiKbN++3Zy5LZUE+W+++WalQb4ktWrVShMnTtSwYcPKvXb8+HHNmDHDbHfq1Envv/9+pSG0JEVFRWns2LG64447Kh1TV5o1a6a33367XJBfkfbt22v+/PkaMWJEpUG+JLVs2VJvvPGGOnbsKKlk1n3pa16Wu6/hqFGjzO0ffvhBWVlZVb4vwzD01Vdfme2rr75agYGBVe4D1BRhPgDAK61MM9RrrfRyBcvqB/pI0zpJK85jWX0AAAAAANDwOQ1Dxwv4r7r/nDV4xrW369Gjh0aOHOny+JoEiPfff7+Cg4MlSWlpadq6dWtNy3PpHDExMVWOKR2cbt++vcrnnNdETa5F8+bNddttt5ntlStXVjr2nXfesZxjypQptQpuP/jgA8uNFM8991y1j1/wJg899JB5I0R1AgIC5OPjWiQZHBysBx54wGxX9Zm4+xoOHz7cfIRFbm6uvvnmmyrHr1692nx0hcQS+/AMltkHAHiVHKehv+6TplcQ4kvSBeHS212lriGE+AAAAAAAoOH7LMnQ+N1Skv2PyfZ6Lf2lGV0MjWpZ/39vdO2113rs2E2aNFHPnj21atUqSdK2bdt03nnnufUc1113XbVjevbsaW4XFBTo8OHDateunVvrcEX//v3N2d3btm2rcIzT6bQs5T506NAqV0FwxXfffWdu9+nTx3I9vJ2vr6/Lq0acjtLL2//+++/KyspSaGhouXHuvoYnl8mfP3++pJIl9G+66aZKx3/++efmdlxcnHr06FGr8wMVYWY+AMBr/OzCbPyfziPIBwAAAAAAjcf9uwjyXZVUWHK9GgJPB7vNmjUztxMTE9167JiYGLVo0aLacS1btrS0MzIy3FqHq5o3b25up6WlKT8/v9yYHTt2KCcnx2xfdtlltTpnSkqK9u/f77bj1bWOHTt6dBWB0j+fhmFU+DPqqWtYesWIDRs2aN++fRWOy8zMtNzgcf3117vl/EBZzMwHANgux2nob/sqDvElZuMDAAAAAACgcanqOexVSU5O1sKFC7Vu3Trt3r1bqampys7OrnIJ+8zMzNMts0Klw/GqnFzq/6Tc3Fy31lFcXKz4+HgtXrxY27dvV0JCgrKysqo9T2ZmZrnl8/fu3Wtpn3322bWqbd++fTJKPRaitsera7Gxsae97+bNm/Xtt99q27ZtOnDggDIzM5Wbm2u5HmVV9Ox6T13Dfv36qX379jpw4ICkktn5//d//1du3MKFC5WXlydJ8vf314gRI9xyfqAswnwAgK1+TjN0z07ptwr+DR3oI/29g/TnWMnXQZAPAAAAAAAan1lxYpl9F5Uss293Fe4REhJSo/EFBQWaOXOm3n77bRUW1uyHpfQzx93hdJ8jX1WYW1ObN2/Wk08+qZ07d9Z434pm5qelpVnarqw8UJWyx3P1BghvUdOfT0nav3+/nnrqKa1Zs6bG+7rymbjzGt5www168cUXJUnz5s3To48+Kl9fX8uYL774wtweMmSIoqKi3HZ+oDTCfACALXKchp7cL/0noeLZ+OeHS+8wGx8AAAAAADRyo1o6dH0LQymE+dWK8m84E0L8/FyPb5xOpx5++GEtXbq03Gu+vr6KjIxUYGCg5ZgnTpxQdna2JPeG6N4gPj5e999/vzlrurSQkBCFhIQoMDBQjv/9rDidTh0+fNgcU9H1OHmtpJLPJiAgoFY1lj7eybrqk5r8fErSnj17dPvttys1NbXca02aNFFoaKgCAwPl43Pq6eAHDx40t6v7TCT3XsPrr79eL7/8soqKipSUlKSVK1dq0KBB5ut79uzR5s2bzfYNN9zgtnMDZRHmAwDq3Kp0Q3fvYDY+AAAAAACAK3wdDrWoXXaIBuzjjz+2BPldu3bV7bffrvPPP18xMTHlZhRL0sSJE/XVV1/VYZV1Iy8vT0888YRl+fObb75Zl19+uc4++2yFhoaW2ychIaHa562XDoqLiopUUFBQq0C/bPBcNphuSAzD0KRJk8wg3+Fw6Nprr9U111yj7t27q2nTphXu07Vr1yqP68lr2Lx5cw0ePFiLFy+WVDILv3SYX3pWfnR0tC688EK3nRsoizAfAFBncp2G/lbNbPy3u0pnMRsfAAAAAAAAcMl7771nbg8YMEBvvPFGtUFzRkaGp8uyxeLFi3XkyBFJko+Pj/773/+qf//+Ve6TmZlZ7XEjIyMt7ePHjysmJua06yx7vOTkZHXs2PG0jyfJXGmgpipawcCdNm7caJnF/uyzz1Y7k92Vn09PXMPSRo0aZYb5S5YsUWpqqpo2baqioiLNnz/fHDdy5MgKb5gB3MWn+iEAANTeqnRDvdZK/64gyA/0kaZ2lFaeR5APAAAAAAAAuCoxMVEHDhww23/6059cmjF+6NAhD1Zln9WrV5vbAwcOrDbIl1y7FmeeeaalvW3btpoXV0qnTp0s4XttjyeVLFdfmqsh/YkTJ2p97qqU/kw6duzo0pL0rnwmnriGpV100UVq1aqVJKmwsFALFiyQJC1fvlzJycnmuOuvv96t5wXKIswHAHjcCwcNXbRe2l3Bsvr9wqT1faTH2zlYVh8AAAAAAAD1WulniRcXF3v8fImJiZZ2dUuTS1JKSor27NnjqZJslZSUZG67ci0kKT4+vtoxXbt2tSzrfnLG9ulq2rSpOnXq5LbjSSr3CIHS16IyRUVF2rp1a63PXRVPfSaeuIal+fr66rrrrjPbc+fOtfyvJPXp00ft27d363mBsgjzAQAetTTV0MS9zMYHAAAAAABAwxccHGxuZ2Vl1fn58/Pzqx3z4Ycf1smNBnYwjFO/hXTlWmRmZmrevHnVjvP19dUVV1xhthctWqTDhw+fXpH/M3ToUHN73bp12rRpU62OFxAQYFn635Xjff/998rJyanVeatT08+kqKhIn3zyiUvHdvc1LOuGG24wZ/9v375dP//8s5YvX255HfA0wnwAgMcUFhsav7t8f+nZ+H4+BPkAAAAAAABoGEqHqb///rsKCgo8er6Ty4CftGzZsirH79q1S7NmzfJgRfZq3bq1uf3TTz9Ve9PCM888o8zMTJeOfdddd5nb+fn5euKJJ2r1+d56660KDAw025MmTVJ6evppH0+Szj33XHN73rx5KioqqnRsZmam/vWvf9XqfK4o/ZmsW7dO2dnZVY6fMWOG5dERVfHENSwtNjZWF1xwgdl+/PHHVVhYKEkKCQmx3EwAeAphPgDAY6YfkraXubFz4hnMxgcAAAAAAEDD1KNHD3Mmb25url5++WWXZiOfrpYtW6pz585m+/nnn9dvv/1W4dhffvlFd911l/Lz8+Xj0zDjoQEDBpjb+/fv15QpU+R0OsuNy8rK0qRJk/T111+7fC26du2q22+/3WyvWbNGf/jDH5SQkFDpPklJSfrXv/6lb7/9ttxrzZo105/+9CezvXfvXt1+++3asWNHpcdLT0/XrFmzNGfOnApfv/rqq83t/fv3a+rUqRXe0HDo0CGNGTNGhw8ftjx33hNKfybp6emaNGlShX8mCgoK9NJLL+n11193+TPxxDUsa9SoUeZ2cnKyuT1s2DDLShyAp/hVPwQAgJo7km/omQPWvvNCpX92lHw9/A9EAAAAAAAAwA7R0dEaOHCgVq5cKUl68803NWfOHMXExCggIMAcd/PNN+uWW25xyznvvfdeTZw4UVJJ2Hj99dfriiuuUK9evdSkSRMlJSXp559/1tq1ayVJXbp0UceOHbVo0SK3nN+bXHbZZWrfvr05s/u9997TqlWrdOWVVyomJkZ5eXnatWuXvv/+e6WmpkqSxo0bp+nTp7t0/Mcff1xbt27Vxo0bJZUE+sOGDdPAgQPVu3dvRUVFqaCgQEePHtXGjRu1bt06FRcXa8qUKRUe7+6779aGDRv0/fffS5J2796t66+/Xn379tX555+vli1byul0KjExUVu2bNHq1atVWFiocePGVXi8Sy65RN26ddP27dslSXPmzFF8fLyGDRum6OhoZWZmatOmTVq8eLEKCgrUpUsXdejQQd99952rl7jGevTooQsuuECrV6+WJH333XfasmWLrrrqKrVv315FRUXat2+ffvjhBx09elRSzT4Td1/Dsi6//HJFRkYqLS3N0s8S+6grhPkAAI+YsEfKKnPT6ytdCPIBAAAAAADQsE2ePFl33nmnjhw5IqlkSfZ9+/ZZxpSe4VtbI0eO1Jo1a/TFF19IKpnhvGDBAi1YsKDc2NjYWM2cOVOvvfaa287vTfz8/PTyyy/rjjvuUEZGhiRpz5492rNnT7mxDodDDz74oK699lqXg+PAwEDNnj1bjz76qJYuXSpJKiws1LJly6p9xEFFHA6H/vOf/2jy5Mn69NNPJUnFxcWKj49XfHx8jY/n6+ur559/Xnfeead5s8Lu3bu1e3f5Z6G2a9dOr776ql555ZUan6empk2bptGjR5th/ZEjR/Tmm29WOPa6667TH//4R5c/E3dfw7ICAgI0YsQIvffee2Zfx44ddd5559X62IArGuY6KgAAWy1NNfRRkrXvntbS+REE+QAAAAAAAGjYYmNjNW/ePE2cOFH9+/dXixYtLM/19oRnn31WkyZNUmRkZIWvBwcHa/To0frqq6/Url07j9Zit65du+rzzz/XwIEDqxzzxhtv6JFHHqnx8Zs0aaLXX39dM2fO1Nlnn13l2OjoaN1zzz268MILKx3j6+urf/zjH5ozZ4769u1b5RLzkZGRGj16tIYPH17pmC5duuijjz6q9P0HBgZq1KhRmjt3rmJjY6us312io6P1xRdfaNiwYZW+v3bt2mnq1KmaOnVqjZf+d/c1LGvkyJGW9vXXX1+j+oDacBiGYdhdBBq+rKws7dq1y2zHxcUpNDTUxorqv82bN6uwsFD+/v4655xz7C4HMBUWG+q1Vtqec6ov0k/adb7UIoAwH/UD37EA4Dl8xwKAZ/E9C3iv3377TUVFRfLz87M84xz1R05OjgzDkMPh8NpnZefn5+vXX3/Vnj17lJOTo6ZNm6pVq1bq16+fmjRpYnd5dS4hIUG//vqrkpKS5O/vrxYtWqhr164688wz3XaOY8eOacOGDUpOTlZmZqaCg4PVsmVLxcXFqVOnTjU+XkpKillzenq6goKC1Lx5c3Xu3FlxcXEuP09eKnn/69at0/HjxxUYGKg2bdqoX79+ioiIqHFd7pKYmKi1a9fq2LFjkqQWLVqoU6dO6t69u9vO4c5rKElfffWV+SgLPz8/LVu2TC1atHBbvShh53esu/6O9kQeyjL7AAC3mn7IGuRL0rMdCfIBAAAAAAAATwsMDNSAAQM0YMAAu0vxCrGxsR6ffd6qVSsNGzbMbceLiorS5Zdf7pZj1cX7r6no6Ghdc801Hj2HO6+hJPMRFpJ08cUXE+SjTrHMPgDAbY7kG3rmgLXvvFDp/ja2lAMAAAAAAAAAwGnbv3+/1q5da7ZvuukmG6tBY0SYDwBwmwl7pCyntW9mF8m3hs84AgAAAAAAAADAbm+88YZOPrG8TZs2uvjii22uCI0Ny+wDANxiWaqhj5KsfXe3li6IIMgHAAAAAAAAANQfxcXF+vDDD/XVV1+Zfffee698fX3tKwqNEmE+AKDWCosNjf/N2hfpJ03taE89AAAAAAAAAADUxI8//qjp06eruLhYR44cUVZWlvlap06dNGrUKBurQ2NFmA8AqLUZh6Rt2da+f3aUWgQwKx8AAAAAAAAA4P3S09O1c+fOcv3h4eF66aWXFBAQYENVaOwI8wEAtXIk39AzB6x9vUKlB9rYUg4AAAAAAAAAALXi5+en6OhoXXjhhRo7dqzatOEX3rAHYT4AoFYe3ytlOq19M7tIvg5m5QMAAAAAAAAA6ofrr79e119/vd1lABY+dhcAAKi/lqca+jDR2nd3a6l/BEE+AAAAAAAAAABAbRDmAwBOS2GxoXG/Wfsi/aSpHe2pBwAAAAAAAAAAoCEhzAcAnJaZh6Vt2da+f3SQWgQwKx8AAAAAAAAAAKC2CPMBADV2NN/Q5P3Wvl6h0tgYe+oBAAAAAAAAAABoaAjzAQA19vheKdNp7ZvZRfJ1MCsfAAAAAAAAAADAHQjzAQA1siLN0AeJ1r67Wkn9IwjyAQAAAAAAAAAA3IUwHwDgssJiQ+N2W/si/aSpneypBwAAAAAAAAAAoKEizAcAuOyVw9LWbGvfPzpILQOYlQ8AAAAAAAAAAOBOhPkAAJcczTf09H5rX89QaWyMPfUAAAAAAAAAAAA0ZIT5AACXTNwrZTqtfTO7SL4OZuUDAAAAAAAAAAC4G2E+AKBaK9IMvZ9o7burlTQggiAfAAAAAAAAAADAEwjzAQBVKiw2NG63tS/CT5rayZ56AAAAAAAAAAAAGgPCfABAlV45LG3Ntvb9o4PUMoBZ+QAAAAAAAAAAAJ5CmA8AqNSxfEOT91v7zg2Vxraxpx4AAAAAAAAAAIDGgjAfAFCpiXulDKe1b2Znyc+HWfkAAAAAAAAAAACeRJgPAKjQT2mG5iRa+8a0kgZGEuQDAAAAAAAAAAB4GmE+AKCcomJD43Zb+yL8pKmd7KkHAAAAAAAAAACgsSHMBwCU88phaUu2te/vHaToAGblAwAAAAAAAA3F3LlzFRcXp7i4OA0ZMqTScfHx8ea4uLg4t9dR+tjx8fFuP74n1efaAXg/wnwAgMWxfENP77f2nRsqPdjGnnoAAAAAAAAAAAAaIz+7CwAAeJeJe6UMp7VvZmfJz4dZ+QAAAAAAAAAanh07dmjx4sWSpLCwMN111132FgQA/0OYDwAw/ZRmaE6itW9MK2lgJEE+AAAAAAAAgIZpx44dmjlzpiQpJiaGMB+A1yDMBwBIkoqKDY3bbe2L8JOmdrKnHgAAAAAAAADe4fzzz9euXbvsLsMrcV0AeJKP3QUAALzDq0ekLdnWvr93kKIDmJUPAAAAAAAAAABQ1wjzAQBKLDD01D5r3zkh0oNt7KkHAAAAAAAAAACgsWOZfQCAJu6VMpzWvpldJD8fZuUDAAAAAAAA3iQ9PV27du3SgQMHlJaWJkmKjIxUbGysevXqpaCgIHsLLGPnzp3atm2bTpw4ocjISLVt21Z9+/aVv79/rY5b365DWcXFxdq4caP279+vEydOKDAwUM2bN1evXr3Upo17ZlllZmYqPj5eR48eVV5enpo3b64+ffooNjbWLcevSkFBgXbu3Kl9+/YpJSVF+fn5Cg8PV3R0tM477zxFRUXV+hzHjh3Txo0bdeLECWVkZKhJkyZq3bq1unbtqnbt2tX4eCkpKVq/fr2OHz+u9PR0BQQEqGXLloqLi9OZZ54ph8P7fl+enJys9evXKykpSdnZ2WrTpo2GDx9e4diioiL99ttv2rt3r5KTk5Wbm6uwsDA1a9ZM5513nqKjo2tdT328ht6OMB8AGrmVaYbeO2btu7OVdGEkf6kCAAAAAAAArrjnnnv0888/S5L69u2r999/3+V9jx8/rkGDBsnpLJlt8/e//12jR4+2jElISND8+fO1ePFi7dy5U8XFxRUey9/fX8OHD9e4ceMUExNzmu+mvPj4eN15551m25XnxG/YsEHPPPOMduzYUe61Zs2a6a677tJ9991Xo3DP3ddhyJAhOnz4sKXv8OHDiouLq3D8ddddp6lTp1r6So997733dP7551f5HvLy8vTmm2/q/fffV2pqaoVjunfvrscee0wDBgyo8liS9MQTT+jLL7+01JeVlaVp06Zp3rx5ysvLK7fPwIED9dRTT6l9+/bVHr8mMjIy9M0332jRokVav3698vPzKxzncDh0/vnn6+GHH1bv3r1rdI7i4mItWLBA//3vf7V79+5Kx8XExGj48OG65557FBERUeUxly9frtdee00bN26UYRgVjmnevLmGDRume++9V61atbK8djp/PiTpjjvu0Jo1ayRJ48aN0/jx410e9/vvv+vZZ5/VypUrze8OSQoLC7OE+Xl5efr+++/1zTffaM2aNcrOzi53/JO6d++ucePG6ZJLLnGp/tJO9xoePXpUQ4YMMf8sT548Wddee63L533llVc0ffp0SVJISIhWrlyp4ODgGtfvzVhmHwAasaJiQ+PK/Hsn3Fd6vpM99QAAAAAAAAD1UenwbN26dTpy5IjL+y5cuNAM4/z9/TV06NByY1544QVNnz5d27dvrzTAlqTCwkLNnTtX1113nRn+2eGzzz7TrbfeWmGQL0knTpzQiy++qAcffFBFRUUuH7e+XYeyjhw5omuvvVYzZsyoNMiXpK1bt+ruu+/WP//5z0qD0cocOnRIN9xwgz755JMKg3xJ+vnnn3XLLbdo7969NTp2debPn6+nn35av/zyS6VBviQZhqHVq1fr9ttv1+zZs10+fkpKim699VZNmDChyiBfKrkp4/XXX9fOnTsrHZObm6uHHnpI999/vzZs2FDltU5OTtacOXO0atUql+v1lBUrVui6667T8uXLLUF+RX755RdNmDBBS5curTLIl0p+7saOHaupU6e6/HNX22vYunVrDRw40GzPnz/fpfNKJT9HJ29kkaRhw4Y1uCBfYmY+ADRqrx2RNpf5+/vvHaXoAGblAwAAAAAAAK66/PLLNXnyZOXl5ckwDC1YsED333+/S/t+/fXX5vagQYOqnUV85plnqmfPnurUqZPCw8NVWFiohIQELV++XHv27JFUsgT9H//4R82fP99tS7a7avny5XrqqacsYXu/fv100UUXqWnTpkpMTNR3332n3bt3a+nSpZoxY8Zpnccd1yEmJka+vr7Kzs7WiRMnJEl+fn6VXrNmzZqdVq1SSRB9++23W1YCaN26tYYNG6YOHTooNzdXGzdu1OLFi1VQUCBJmjNnjhwOh/7617+6dI7c3Fz98Y9/1IEDBxQYGKghQ4aoZ8+eCg0NVWJiohYtWmSG4CkpKXr88cf12WefycfH/XN/W7Zsqd69e6tr165q2rSpfHx8lJiYqDVr1ig+Pl5SySz7KVOmKDY2VpdeemmVx0tJSdHo0aN18OBBsy84OFgXXXSRevTooaZNmyo3N1cHDx7Ur7/+qm3btlV5vPz8fI0ZM0abNm0y+/z9/dW/f3/16dNHzZo1U35+vo4cOaL169dr48aNVd5AUlcSEhL03nvvKTs7W6GhobriiivUtWtXBQcH69ixY+YKIRWJjIxU79691a1bNzVr1kz+/v46ceKENmzYoBUrVpg3Brzzzjtq06aNZbWBirjrGo4aNUo//fSTpJIVPRISEipdHaO0tWvXKiEhwWzfcMMN1e5THxHmA0AjlVhg6Ml91r5zQqQ/1u2/7QEAAAAAAIB6LzQ0VEOGDNE333wjqSSgdyXM379/v7Zu3Wq2R4wYUeE4f39/3Xrrrbr11lvVuXPnCsc8/vjj+vLLL/XUU0+poKBAmZmZmjZtmv7zn//U/A2dpuzsbEuQHxAQoBdeeKHcagMPPfSQ/vvf/+rFF1/UrFmzXD6+u6/DnDlzJElz587VpEmTJEnR0dH64YcfXK7JVf/4xz8sQf7o0aP117/+VYGBgWbfmDFjtHv3bv3xj380Q8r33ntPgwcPtsxersz333+v4uJide/eXS+//LLatm1reX3s2LF65pln9Mknn0gqmYm9dOnSaoN0VzkcDl188cX6wx/+oH79+lV6k8CmTZv0pz/9yVzB4plnntGgQYPk51dxbGkYhiZOnGgJ8q+88ko9+eSTatGiRYX77N+/X2+99Valx3zuuecsIXS/fv307LPP6owzzqhw/LFjx/Tuu++qSZMmFb5eV+bNmyep5FEJL7zwQrkbTCpaqr9Xr1667777dPHFF8vf37/C4+7fv1+PPPKI+YiAF198UcOHD1fTpk0rrcVd13DIkCFq1qyZTpw4IcMwNH/+fE2YMKHS8570xRdfmNsdO3bUeeedV+0+9RHL7ANAI/XEXimjzAo8M7pIfj7MygcAAAAAAABqqnQQv3v3bpeem116Vn5YWFilz6p+7rnn9PTTT1caYJ903XXX6emnnzbbixcv1vHjx6utw10++OADHTt2zGw/9dRTFT42wOFw6P7779eYMWNqNNu5vlyHsrZt22be6CGVrOTwzDPPWIL8k7p06aI333zTslz4tGnTXDpPcXGxYmJiNHv27HJBviT5+vrqb3/7myVsXbhwYU3eSpVuvPFG/fe//9UFF1xQ5Wz/c889V2+++aYZLCcmJurHH3+sdPzixYu1YsUKs33NNdfoP//5T6VBviR16NBB//znP9W7d+9yr23fvl0ff/yx2e7Xr5/efPPNSkNoSWrVqpUmTpyoYcOGVTqmrnTu3FmvvfaaSytFDBgwQB9//LEuvfTSSoN8qeR6vf3224qKipIk5eXlWZawL8ud19Df31/XXnut2V6wYEG13wtZWVn67rvvzPb1119f5fj6jDAfABqhn9MMvXvM2ndHtHRRJEE+AAAAAACA1zGckvM4/1X3n1H1s6M97eQy8ieVDuors2DBAnP7yiuvVEBAQIXjKgp9K3PDDTeYgVphYaFWr17t8r61VXqm7Nlnn60bb7yxyvEPP/xwlTN/y6ov16Gs0qFnQECA/vrXv8rhqPx3se3bt9e9995rtnfu3KkNGza4dK7/+7//U1hYWKWvBwQEaOTIkWZ78+bNLh3XFTX5fDp16qThw4eb7ZUrV1Y69p133jG3mzdvrsmTJ9fq0QCljxcYGKgpU6bUqHa7TZgwweV6a/K+mjdvrttuu81su/qZuOMajho1ytw+duyYfvnllyrHf/vtt8rNzZVU8miM0j/TDQ3L7ANAI1NUbGjcb9a+cF/p+U721AMAAAAAAIAqZH0mnRgnOZPsrsT7+baUms2UQkdVP9YD/Pz8NGzYMH344YeSSmY8P/bYY5WGtps3b9bvv/9utksHm7XhcDh0/vnnm0uSb9u2zW3Hrsr+/ft14MABs33jjTdWGVhLJY8nuOqqq/TBBx+4vR67rkNFli1bZm5ffPHFat26dbX7jB49Wq+88or5HPPly5erV69eVe4TEhKiK664otpj9+zZ09w+dOiQCgsLq5y17Sn9+/fX3LlzJanSZ9wnJyfr119/Nds33XRTlTcrVMfpdGrx4sVme+jQoRWuYuCtoqKidOGFF3rs+P3799eMGTMkVf6ZeOIaduzYUb169TJvWpk7d26Vj5YofePQRRddVOUqDfUdM/MBoJF57Yi0Kcva90wHqVUgs/IBAAAAAAC8TvJ9BPmuciaVXC8blV5q/8iRI1q3bl2lY+fPn29ut2rVSv369XNbHaWX305MTHTbcauyZcsWS9uVZ7zXZNzpsOM6lJWYmKikpFN/hi+66CKX9mvevLm6detmtste34qcffbZlT4jvrSWLVua24ZhKDMz06Wa3K158+bmdmWfT+kgX5Iuu+yyWp1zx44dysnJcdvx6to555wjX19fjx2/9GeSlpam/Pz8cmM8dQ1Lz67/4YcflJGRUeG4/fv3W1aqqG4FkPqOmfkA0IgkFhh6ar+1r0eI9FCMPfUAAAAAAAAADUmvXr0UGxurhIQESSVL7fft27fcOKfTqW+//dZsX3311S4tG56RkaHvvvtOv/zyi3bv3q3jx48rOztbhYWFle5TV0Ft6Vn5gYGBio2NdWm/Ll261Phc3nwdyip9XaSavd+4uDgzxC97nIqUDmKr0qRJE0v75HLl7lJYWKiffvpJS5Ys0c6dO3XkyBFlZWVVGAyfVNnns3fvXnPb39//tH5eKjueVHIDRH3i6p+rsoqLixUfH6/Fixdr+/btSkhIUFZWVrWffWZmZrnl8z11DS+//HK98MIL5s/KwoULdcstt5Qbd3I1B6nkhp3Bgwe75fzeijAfABqRSXul9CJr38wukp8Ps/IBAAAAAAC8UvP/ssy+q04us2+z4cOH69VXX5UkLVq0SH/7298UEBBgGbNq1SolJyeb7dIz+itiGIZmz56t6dOnW2bEuqKqANWdSs+ijYyMdPmZ5k2bNnX5HPXhOpRVdnZxVFSUy/uWHlvZLOXSTveZ5YZhnNZ+FVmxYoWeeeYZHTp0qEb7Vfb5pKWlmduRkZG1fhxA6eNJqnfLs4eEhNR4n82bN+vJJ5/Uzp07a7xvRZ+Lp65hkyZNNHToUH3++eeSSkL7smG+0+nUV199ZbavvfZal1ajqM8a9rsDAJhWpRuafczad0e0dFEkQT4AAAAAAIDXCh0lhVwvFafYXYn384mSHJ5bftpVI0aMMMP89PR0rVixotwy1AsWLDC3u3Tpoq5du1Z5zGeeeUYfffRRuX6Hw6HIyEgFBQVZQs709HSlp6fX5m3UWOkZvkFBQS7vV3aWeFXqw3Uoq+xNBzV5v6XH1vTmBTssWLBAEyZMUHFxcbnXwsLCFBwcbLnhIC8vz/IIgopkZ2eb28HBwbWusfTx/Pz8yt1o4+1qGlzHx8fr/vvvV15eXrnXQkJCFBISosDAQDkcJTmB0+nU4cOHzTEV3ejhyWs4cuRIM8zfvHmz9uzZozPPPNN8feXKlZafmRtuuMFt5/ZWhPkA0AgUFRsat9vaF+4rPd/JnnoAAAAAAABQAw5fybd+zR5tzDp06KDu3btr69atkkqW2i8d5ufl5emHH34w28OHD6/yeMuWLbME2LGxsbrzzjs1YMAAtWvXrsKZytOnT9crr7xS27dSI6WD54qCw8q4usR7fbkOZZWdSV2TJe1Lj3VHkO1Jx48f11NPPWUG+aGhobr99tt1ySWXKC4ursKbGFavXq0xY8ZUedzS188dNzSUPl5RUZEKCgrqXaDvqry8PD3xxBPmn0d/f3/dfPPNuvzyy3X22WcrNDS03D4JCQnlbj4qy5PXsFu3boqLi9OuXbskSV988YUmTpxovv7FF1+Y2+eee64l6G+oCPMBoBF4/Yi0McvaN7mD1CqQWfkAAAAAAACAu40YMcIM85cuXaqsrCwzOFuyZIk5s9XhcOiaa66p8lhz5swxt7t06aKPPvqowhCuNFeWZHe38PBwczs9PV3FxcUuLbWfmprq0vHry3Uoq/R1kaSUlBS1b9/epX1TUk6tyFH2ON5m7ty55s91kyZN9NFHH1X7fPvMzMxqjxsZGWlup6WlqbCwsFZL7Zc+nlRyE0JMTMxpH0+SOau9pmpy08vpWLx4sY4cOSJJ8vHx0X//+1/179+/yn1q+plI7rmGpV133XWaOnWqJGn+/Pl67LHH5Ofnp9TUVC1ZssQc1xhm5UuSaw8sAQDUW0kFhp7cb+3rESKNc9/frQAAAAAAAABKufrqq+XrW7Lkf35+vr7//nvztfnz55vbffr0UZs2bSo9TnFxseLj4832gw8+WG2ALanGzyt3h9IBdV5enhISElzab/fu3dWOqU/Xoax27dpZ2idnHLui9FhXbwCwy+rVq83ta6+9ttogX3Lt8yk987qwsNClnxdXjydJ27Ztq9XxpPKPlXB19YUTJ07U+txVKf2ZDBw4sNogX6r5ZyK55xqWdtVVV5nXNDk5WStWrJBUsspJYWGhpJIbRq6++mq3ntdbEeYDQAP3xF4pvcjaN6OL5OfDrHwAAAAAAADAE5o3b24Jzr7++mtJJTOLV65cafZXt8T+yZnIJ8XFxVV77oKCAm3YsKGmJddajx49LO2ff/7Zpf1cGefp61D6OeQVPe+9NqKjoxUdHW22S3/+VUlOTtb27dvN9jnnnOPWutyt9HPMu3bt6tI+pW/QqEzv3r0t7cWLF9essDK6du1qWSa+tseTyq+aUPpaVOb48eOWZ9N7gqc+E09cw9LCwsJ0xRVXmO25c+da/leSrrjiCpdu6GkICPMBoAFblW5o9jFr3+3R0sWRBPkAAAAAAACAJ40YMcLcXr16tZKSkrRo0SIzlPb399fQoUOrPIZhGJZ2QUFBtedduHCh0tLSal5wLXXo0MEye7x08FaZ7Oxsffvtt9WO8/R1KP08+qysrCpGnp7Bgweb2ytWrNDRo0er3eezzz6T0+ms8BjeqPRnlJ+fX+34hIQEc8Z1VZo1a6Z+/fqZ7c8++6xWn5Gvr68lKF60aFGtQ/WYmBjL0v+bNm2qdp8vv/yyVud0RU0/k8zMTM2bN6/acZ64hmXdeOON5vayZcv0888/a8eOHWZfY1liXyLMB4AGy2kYGldmxaEwX+n5TvbUAwAAAAAAADQml112mZo0aSKpZLb3N998Y87Ql6RBgwYpIiKiymNERkaax5BKQq2qJCYmatq0aadfdC2VDti2bNlSbaA/c+ZMy3PhK+Pp61D6ed+ZmZk6duxYFaNrbvTo0eZ2QUGBnn322XI3KJR28OBBzZo1y2yfddZZOvfcc91ak7u1bt3a3F6+fHmVYwsLC/WXv/zFcrNCVe666y5z+/jx43r66aervH41OV5+fr6eeOIJl24QqYy/v7+6detmtr/44osqxx8+fNjy+XpK6c/kp59+qnbViWeeeUaZmZkuHdvd17Cs888/33xERWFhoR5//HHztTPOOMNyg0dDR5gPAA3Um0ekjWVuUHymg9Q6kFn5AAAAAAAAgKeFhITo0ksvNdtz5szRr7/+arZLz9yvjK+vr84//3yzPWvWLK1Zs6bCsTt27NDtt9+ulJQU+fjYE//cdtttatWqldl++umn9f3335cbZxiG3nzzTb399tsu1erp69CpUyfL7Px//etfbp2hf/bZZ+uqq64y2z/88IMmT55cYfi5Z88e3XvvvcrJyTH7SgeZ3mrAgAHm9qpVq/T2229XOC45OVl//OMftWbNGpc/n0svvVSXXHKJ2V6wYIEeeeQRJScnV7rPwYMH9dRTT2n9+vXlXuvatatuv/12s71mzRr94Q9/UEJCQqXHS0pK0r/+9a9KV5Io/fmuXr1ab731VoXjdu7cqTvvvFOZmZlyODz7u/rSn8n+/fs1ZcqUCm+gyMrK0qRJk/T111+7/Jl44hqWVXp2funP+rrrrvP4tfMmftUPAQDUR++UWampe4g0LqbisQAAAAAAAADcb8SIEVqwYIEk6dChQ2Z/WFiYJZysyr333mvORM/JydGYMWN0ySWXqF+/fgoPD1dKSori4+O1cuVKFRcXq2XLlhoyZIg+/vhjt7+f6oSEhOiZZ57Rgw8+qOLiYhUUFGj8+PHq16+fLr74YjVt2lSJiYn6/vvvtXPnTknSAw88oNdee63aY3vyOgQEBGj48OH65JNPJElff/21Fi1apJiYGAUFBZnjhgwZokceeeQ0roz05JNPatOmTeZy5B9//LFWrFihYcOGqX379srLy9PGjRv1ww8/WEL+O++80xLKeqtRo0Zp1qxZ5qMNnn/+eX377bcaMmSIoqOjlZWVpW3btumHH35Qdna2fH199eCDD2rmzJkuHf+5557TLbfcogMHDkiSvvvuO/3000+6+OKLdc455ygyMlJ5eXlKSEjQr7/+qs2bN0uSrr766gqP9/jjj2vr1q3auHGjpJIwetiwYRo4cKB69+6tqKgoFRQU6OjRo9q4caPWrVun4uJiTZkypcLj3XjjjXr77beVmJgoSZo2bZp++OEHXXrppYqKilJaWprWrl2rFStWyOl0auDAgcrLy7Pc4ONul112mdq3b29es/fee0+rVq3SlVdeqZiYGOXl5WnXrl36/vvvlZqaKkkaN26cpk+f7tLx3X0Ny7ruuuv08ssvq6ioyOzz8fHR9ddf7/pFaAAI8wGgAcooMvRrmRtHn+so+fk0nrvVAAAAAAAAALsNHDhQzZo104kTJyz9V155pQICAlw6Rt++fTV+/HjNmDFDUsmS/T/++KN+/PHHcmOjoqI0c+ZMl55F7imDBw/W3//+dz311FPmst5r1qypcCb9kCFDNG7cOJfCfE9fhz//+c/asGGDdu8ueXZpYWGhGYKedNZZZ7l8vIpqev/993X33Xebxz1y5EilM7gl6Y477tBf/vKX0z5nXQoPD9dLL72ksWPHmjcjbN682QzVS/P399eTTz6p9u3bu3z8qKgoffTRRxo7dqz5TPqcnBwtWrRIixYtqnG9gYGBmj17th599FEtXbpUUslnvmzZsmof41CR0NBQTZs2TQ888IDy8vIkSRs2bNCGDRvKje3Ro4f+/e9/a9y4cTU+T034+fnp5Zdf1h133KGMjAxJJSs/7Nmzp9xYh8OhBx98UNdee63LYb67r2FZLVq00KBBgyx/xgcMGGBZ/aMxYJl9AGiAfk6XnKUeGRTgkIY0ta8eAAAAAAAAoDHy8/OzLL990vDhw2t0nHHjxumFF16wPAO7tICAAF111VWaN2+eVzxbfdSoUfrggw8qDb+joqL02GOP6dVXX5Wfn+vzTj15HSIjI/X555/rmWee0cUXX6xWrVpZZuW7Q5s2bTRv3jyNHz9eTZtW/gvbs88+W2+99Zb+9re/1avlxAcOHKgPP/xQ55xzTqVjzjvvPH3wwQcaPXp0jY8fFRWljz/+WM8++2y1NwK0a9dO48ePtzzLvqwmTZro9ddf18yZM3X22WdXebzo6Gjdc889uvDCCysdc8EFF2jOnDnq0aNHha+Hhobq3nvv1YcffqiIiIgqz+cuXbt21eeff66BAwdWOeaNN944rVUn3H0Nyxo5cqSlfcMNN9S4xvrOYRiGUf0woHaysrK0a9cusx0XF6fQ0FAbK6r/Nm/erMLCQvn7+1f5FyMap4l7Db1w8FT7oghp+Xn15x99gN34jgUAz+E7FgA8i+9ZwHv99ttvKioqkp+fnzp37mx3OTgNOTk5MgxDDofD8nz1ulRUVKSNGzdq165dyszMVHh4uKKjo9W3b1+Fh4fbUlN1du7cqS1btiglJUWRkZFq27at+vXrJ39//9M+Zn28DmU5nU5t3LhR+/btU2pqqgICAtS8eXP16tVLMTH1/3mpv/32mzZu3KiUlBQFBQWpRYsWOuecc9S2bVu3neP333/Xli1blJycrJycHIWEhKhNmzbq2rWrYmNja3y8Y8eOacOGDUpOTlZmZqaCg4PVsmVLxcXFqVOnTjU6Vun3HxoaqjZt2uiCCy5QkyZNalyXu5x8BEFSUpL8/f3VokULde3aVWeeeabbzlGba1jRd+zMmTPN1TgiIyP1008/ubyqSU246+9oT+ShLLMPAA3Q8lRre1CkLWUAAAAAAAAAcCM/Pz/16dNHffr0sbsUl3Xt2lVdu3Z16zHr43Uoy9fXV71791bv3r3tLsUjOnfu7PEbl9q1a6d27dq57XitWrXSsGHD3HKsunj/NRUbG3taNznUhDuvoWEY+uqrr8z28OHDPRLkezuW2QeABiazyNCvWda+wSyxDwAAAAAAAAAA6olVq1YpISHBbN900002VmMfwnwAaGB+TpecpR6gEuCQLqgfK0sBAAAAAAAAAADo9ddfN7fPO+88denSxcZq7MMy+wDQwCxLs7bPD5eCfR221AIAAAAAAAAAAOCqgoICvf7661qzZo3Z98ADD9hYkb0I8wGggVmeZm0PirSjCgAAAAAAAAAAgOp99NFH+vDDD1VUVKQjR44oLy/PfK1///4aPHiwfcXZjDAfABqQzCJD6zKtfYT5AAAAAAAAAADAWyUnJ2v37t3l+tu0aaOpU6faUJH3IMwHgAbk53TJaZxq+zuk/hH21QMAAAAAAAAAAOAqf39/xcTEaMiQIbr//vvVtGlTu0uyFWE+ADQgy9Ks7fPDpWBfhy21AAAAAAAAAAAAVGf8+PH6wx/+IMMw5HA4FBwcbHdJXsPH7gIAAO6zPM3aZol9AAAAAAAAAACA+okwHwAaiMwiQ+syrX2DI20pBQAAAAAAAAAAALVEmA8ADcTP6ZLTONX2d0j9I+yrBwAAAAAAAAAAAKePMB8AGohladb2+eFSsK/DlloAAAAAAAAAAABQO4T5ANBALE+ztgdF2lEFAAAAAAAAAAAA3IEwHwAagMwiQ+syrX2DI20pBQAAAAAAAAAAAG5AmA8ADcDP6ZLTONX2d0j9I+yrBwAAAAAAAAAAALVDmA8ADcCyNGv7/HAp2NdhSy0AAAAAAAAoz9fXV5LkdDptrgQAAJRWXFwsSfLx8b7o3PsqAgDU2PI0a3tQpB1VAAAAAAAAoDInw3zDMFRQUGBzNQAAQJIKCwvNMP/k39XehDAfAOq5rCJD6zKtfYMjbSkFAAAAAAAAlQgJCTG3MzMzqxgJAADqSnZ2trld+u9qb0GYDwD13M/pktM41fZ3SP0j7KsHAAAAAAAA5YWHh5vb6enpMgyjitEAAMDTDMOw3GAXGhpqYzUVI8wHgHpuWZq1fX64FOzrsKUWAAAAAAAAVCwgIEBBQUGSpPz8fB06dIhAHwAAG6WmpiorK0tSyRL7J/+e9iaE+QBQzy1Ps7YHRdpRBQAAAAAAAKrTsmVLORwlkzCysrK0f/9+JScnq6CgwObKAABoHAzDUHZ2to4cOaLExESzv/Tf0d7Ez+4CAACnL6vI0Noyj1gjzAcAAAAAAPBOISEhio2NVUJCggzDUH5+vo4fP67jx4/L4XDI19fX7hJRBafTaW7zWQGAe9XFd6xhGCouLi63Mk7z5s0VGRnpkXPWFmE+ANRjP6dLzlJ/5/g7pP4R9tUDAAAAAACAqp0M9JOSkpSXl2f2G4ahoqIiGytDdUqvoBAQEGBjJQDQ8NjxHevj46OmTZuqefPmdXK+00GYDwD12LI0a7tfuBTi633LwAAAAAAAAOCUkJAQdejQQQUFBcrMzFRWVpacTqdlViK8T25urgzDkMPhkJ8f8QoAuFNdfcf6+vrK399fERERCg0NlY+Pdz+Vnr9tAKAeW55mbbPEPgAAAAAAQP0REBCgZs2aqVmzZnaXAhds3rxZhYWF8vPzU+fOne0uBwAaFL5jK+bdtxoAACqVVWRobaa1b3CkLaUAAAAAAAAAAADAzQjzAaCe+jldchqn2v4OqX+EffUAAAAAAAAAAADAfQjzAaCeWpZmbfcLl0J8HbbUAgAAAAAAAAAAAPcizAeAemp5mrU9KNKOKgAAAAAAAAAAAOAJhPkAUA9lFRlam2ntGxxpSykAAAAAAAAAAADwAMJ8AKiHfk6XnMaptr9D6h9hXz0AAAAAAAAAAABwL8J8AKiHlqVZ2/3CpRBfhy21AAAAAAAAAAAAwP0I8wGgHlqeZm0PirSjCgAAAAAAAAAAAHgKYT4A1DNZRYbWZVr7BkfaUgoAAAAAAAAAAAA8hDAfAOqZVRlSkXGq7e+Q+kfYVw8AAAAAAAAAAADcjzAfAOqZZanWdr9wKcTXYU8xAAAAAAAAAAAA8AjCfACoZ5anWduDIu2oAgAAAAAAAAAAAJ5EmA8A9UhWkaG1mdY+wnwAAAAAAAAAAICGhzAfAOqRVRlSkXGq7eeQBkTYVw8AAAAAAAAAAAA8gzAfAOqRZanWdr8wKcTXYU8xAAAAAAAAAAAA8BjCfACoR5anWduDmtpSBgAAAAAAAAAAADyMMB8A6omsIkNrM619gyNtKQUAAAAAAAAAAAAeRpgPAPXEqgypyDjV9nNIAyLsqwcAAAAAAAAAAACeQ5gPAPXEslRru1+YFOLrsKcYAAAAAAAAAAAAeBRhPgDUE8vTrO1BTW0pAwAAAAAAAAAAAHWAMB8A6oGsIkNrM619gyNtKQUAAAAAAAAAAAB1gDAfAOqBVRlSkXGq7eeQBkTYVw8AAAAAAAAAAAA8izAfAOqBZanWdr8wKcTXYU8xAAAAAAAAAAAA8DjCfACoB5anWduDmtpSBgAAAAAAAAAAAOoIYT4AeLlsp6G1mda+wZG2lAIAAAAAAAAAAIA6QpgPAF5uVbpUZJxq+zmkARH21QMAAAAAAAAAAADPI8wHAC+3LM3a7hcmhfg6bKkFAAAAAAAAAAAAdYMwHwC83PJUa3tQU3vqAAAAAAAAAAAAQN0hzAcAL5btNLQm09o3ONKWUgAAAAAAAAAAAFCHCPMBwIutSpeKjFNtP4c0IMK+egAAAAAAAAAAAFA3CPMBwIstS7O2+4ZJIb4OW2oBAAAAAAAAAABA3SHMBwAvtjzV2h4UaUsZAAAAAAAAAAAAqGOE+QDgpbKdhtZkWvsGN7WnFgAAAAAAAAAAANQtwnwA8FKr0qUi41TbzyENCLevHgAAAAAAAAAAANQdwnwA8FLL0qztvmFSqJ/DlloAAAAAAAAAAABQtwjzAcBLLU+1tgdF2lIGAAAAAAAAAAAAbOBndwEAgPKynYbWZFr7Bje1pxYA8DpGoVScZXcVAE6TrzJU7CiUr/wlZ2r1OwAAaoTvWQDwHL5jATRIPiGSI8DuKlAJwnwA8EKr0qUi41TbzyENCLevHqBRKfxdnZrcqSYhO5RWNFQq/ljyaWJ3VTgp7d9S6t8kI8fuSgCcprNDSzV+t60MAGiw+J4FAM/hOxZAg+QIkyInSk3/anclqADL7AOAF1qWZm33DZNC/Ry21AI0KsW5UuK1CvHdJB9HgaL850sn/mR3VTgp6wsp5c8E+QAAAAAAAIC7GJklk2cKf7O7ElSAMB8AvNDyMqt0DYq0pQyg8TnxJ6lgk7Uvc5aU+YEt5aCUwr3S8XvsrgIAAAAAAABomAyj+jGoc4T5AOBlsp2G1mRa+wY3tacWoFHJfL8kuK9I8gNSwc66rQenFOdJiaMkI8PuSgAAAAAAAICGxREkRf5NCuhidyWogJ/dBQAArH5Jl4pK3QDn55AGhNtXD9AoFOwoCewrY2RLiTdKMWskn+C6qwslUv4sFWyw9oXeKkW9YE89AGpl+47tKioskp+/n7qd1c3ucgCgweF7FgA8h+9YAA2STyS/8/RihPkA4GWWpVnbfcOkUD+HLbUAjUJx9v9mfVufw55d1E0hfttPdRRuk5Ifklq+U8cFNnJZH0kZr1n7/LtKzd+QfELtqQlArRQZySo0CiXDX/JrY3c5ANDg8D0LAJ7DdywAoK6xzD4AeJnladb2oEg7qgAakeSHSoL6Uk4U3qBdmbOU64yzjs2aLWUS5teZgl3S8futfY4mUvTnBPkAAAAAAAAAGjzCfADwItlOQ2vKPBJ6cFN7agEahcx3pKx3rX0B5+pI/uMyFKTf86ZJjjDr68kPSQVb6q7Gxqo4R0oaJRlZ1v7mr0kBZ9tTEwAAAAAAAADUIcJ8APAiv6RLhcaptq9DGhBuXz1Ag5a/WUr+o7XPESZFfyZDQZKkAqO91OJN6xgjt2RZ/uLMuqmzsToxvvxNE2H3SGFj7KkHAAAAAAAAAOoYYT4AeJFladZ23zAp1M9hSy1Ag1ac+b9Z33nW/hZvSv6drX2hN0nhD1n7CndJyWMlwxA8IPNdKfNta59/d6nZDHvqAQAAAAAAAAAbEOYDgBdZnmZtD4q0owqggTOMkuewF+629oePKwnuK9LsRSmgt7Uv60Mpc5ZnamzMCrZKyQ9a+xyhUvTnkk+wPTUBAAAAAAAAgA0I8wHAS2Q7Da3JsPYNjrSlFKBhy3xDyv7Y2hfYR2r2r8r3cQRK0Z9JPhHW/hOPSPkb3F9jY1WcVfIIAyPX2t9ilhQQZ09NAAAAAAAAAGATwnwA8BK/pEuFpVbs9nVIAyMqHw/gNOSvl5Ifsfb5REgtPy0J7Kvi30FqMdvaZ+SXhM/F6W4ts1EyjJJHFxTutPaHjZVCb7GnJgAAAAAAAACwEWE+AHiJZWnWdt8wKdTPYUstQINUnF4SvKvA2t9idklQ74qQkVLEo9a+or3S8T+UhNE4fZlvSlkfWPsCeknN/m1PPQAAAAAAAABgM8J8APASy9Os7UGRdlQBNFCGISXdIxXts/ZH/LkkoK+JqKlS4AXWvuwvpIyZtSqxUcvfKJ0Yb+1zhP/v0QZBtpQEAAAAAAAAAHYjzAcAL5DtNLQmw9o3ONKWUoCGKWOGlDPX2hfYvySYrylHgBT9ieQTZe0/8ZiUt+b0a2ysijNKVkww8q39Ld6W/DvZUxMAAAAAAAAAeAHCfADwAr+kS4WlVuj2dUgDI+yrB2hQ8uKlE/9n7fOJKgnkHf6nd0y/M6SWc8p0FkpJN0nOlNM7ZmNkGNLxe6WiPdb+8Eek0BvsqQkAAAAAAAAAvARhPgB4gWVp1nbfMCnUz2FLLUCD4kwpCdhVaO1vOUfyi63dsYOvkiKfsPYV/S4dv6skpEb1Ml6Vsj+z9gX2k5pNs6ceAAAAAAAAAPAihPkA4AWWp1nbgyLtqAJoYIxi6fgYqeigtT/yiZIg3h2a/kMKusjal/O1lP6ie47fkOWvk048au3zaSq1/KTkUQYAAAAAAAAA0MgR5gOAzXKchtZkWPsGR9pSCtCwpL8o5Syw9gVdVBLAu4vDT2r5keTTwtqf8oSU97P7ztPQOFOlxFEqt2JCi3cl//Z2VAQAAAAAAAAAXocwHwBs9ku6VFhqRW5fhzQwwr56gAYhb6WUMsna59NCavlxSQDvTn4xUssPJJV+NIZTShwtOY+791wNgWFIx++Wig5Y+yMmSCHDbSkJAAAAAAAAALwRYT4A2GxZmrXdN0wK9XNUOBaAC5zHpcSbJTlLdTqklh9Kfm08c87gy6XIJ8vUcVhKuqNkuX+ckv5vKWeetS9woBT1rD31AAAAAAAAAICXIswHAJstT7O2B0XaUQXQQBjFJQG687C1P/IpKfgyz5676VNS0BBrX+53UtpUz563Psn7RUqZaO3zaS5Ffyw5/O2pCQAAAAAAAAC8FGE+ANgox2koPsPaNzjSllKAhiFtSkmAXlqTS6WmT1Y83p0cviWz/31bWftTn5Ryl3n+/N7OeaLk0QMqKtXpkFrOkfza2lUVAAAAAAAAAHgtwnwAsNEv6VKhcart65AGRNhXD1Cv5S6VUp+y9vm2klp8UBK01wW/aKnlx7L+E6tYSrpFKkqsmxq8kVEsJd0pOROs/ZF/kYKH2lMTAAAAAAAAAHg5wnwAsNGyNGu7T5gU5uewpRagXis6VhKYq/Tz6X1KgnW/6Lqtpckgqek/rH3OY1LSrZLhrNtavEX6NCn3G2tf0GCp6WQ7qgEAAAAAAACAeoEwHwBstDzN2h4UaUcVQD1nOEuCcmeZme9N/1ESrNsh8gmpSZkZ53lLpNS/21OPnXJXSCl/tfb5Rpc8ksDhZ09NAAAAAAAAAFAPEOYDgE1ynIbiM6x9gyNtKQWo31KfkfKWWvuaDC0J1O3i8Cl5FrxvjLU/7R9Szg/21GQHZ5KUdLPKr5jwoeTX2q6qAAAAAAAAAKBeIMwHAJv8ki4VGqfavg5pYIR99QD1Us73Uto/rX2+bUuCdIfN/8zxbS5Ffyqp9OxzQ0q6TSo6YldVdcdwlrxX51Frf9PJUpMhtpQEAAAAAAAAAPUJYT4A2GRZmrXdJ0wK83PYUgtQLxUdLgmLVequGPlJ0Z+UBOneIGiAFDXF2ld8vGS2ulFkT011Je1ZKXexta/J5VLkX+ypBwAAAAAAAADqGcJ8ALDJ8jRre1CkHVUA9ZRRVBKIFydb+6OmlgTo3iTiMSl4uLUv7ycp9Ul76qkLuT9KqZOtfb5tpJbvSw5fW0oCAAAAAAAAgPqGMB8AbJDjNBSfYe0bHGlLKUD9lPI3KW+ltS94hBTxZ3vqqYrDIbV4V/JrZ+1PmyrlLLSnJk8qOiol3Srrigm+UsuPJd+WdlUFAAAAAAAAAPUOYT4A2OCXdKmwVM7l65AGRthXD1CvZC+Q0p+39vm1l1rMLgnOvZFvU6nlp5L8rf1Jd0pFB20pySOMIinpFsmZZO2PelZqcpE9NQEAAAAAAABAPUWYDwA2WJZmbfcJk8L8vDSEBLxJ4e/S8TvLdPqXBOW+TW0pyWVB/aRmL1r7ilOkxNGSUWBPTe6W+rSUt9zaF3y1FDHBnnoAAAAAAAAAoB4jzAcAGyxPs7YHRdpRBVDPGAVS0mipONXa3+xFKaivPTXVVPg4KeRGa1/+aillkj31uFPOt1Lac9Y+39iSRww4+CcnAAAAAAAAANQUv1kFgDqW4zQUn2HtGxxpSylA/XJiopQfb+0LubEkIK8vHA6pxZuSXydrf/pLUvZXtpTkFkUJUtIdZTr9pOhPJd9mtpQEAAAAAAAAAPUdYT4A1LHVGVKhcart65AGRthXD1AvZM+VMv5j7fPrVBKMO+rZIyp8IqTozyRHoLX/+F1S4T5bSqoVo1BKvFkqPmHtj5omBV1gT00AAAAAAAAA0AAQ5gNAHVtWZoXwPmFSmF89CyOBulS4Vzp+j7XPEVgSiPvU0zthAntJzV629hWnS4k3SUa+PTWdrpS/SPmrrH3BI6WIP9lRDQAAAAAAAAA0GIT5AFDHlqdZ24Mi7agCqCeK80oC7uJ0a3+z6SWBeH0Wdr8Uequ1r+BX6cRj9tRzOrLnS+n/svb5dZBavFP/VkwAAAAAAAAAAC9DmA8AdSjHaSg+w9o3ONKWUoD6IeUxqWC9tS/0VinsPnvqcSeHQ2r+huQfZ+3PeEXK+tSemmqi8IB0fEyZzoCSFRN8I20oCAAAAAAAAAAaFsJ8AKhDqzOkAuNU29chDaynq4QDHpf1sZTxqrXPv2tJAN5QZn37hJaE344m1v7j90qFv9lTkyuMAinpJqk4zdrf7N9SYG9bSgIAAAAAAACAhoYwHwDq0LJUa7t3qBTm10BCScCdCnZJx8vMvnc0KQm+fULtqclTAnpIzV+x9hmZUuIoqTjXnpqqc2KClL/W2hcyWgp/0J56AAAAAAAAAKABIswHgDq0PM3aHtTUljIA71acKyWNkowsa3/zV6WA7vbU5Glhd0uhd1n7CjZJJx6xpZwqZX0uZUy39vl3llrMajgrJgAAAAAAAACAFyDMB4A6kuM0FJ9h7RscaUspgHc7MV4q2GLtC71bCrvLlnLqTPNXJP8yNytk/lfKfN+eeipSuEc6fo+1zxEktfxM8gm3pyYAAAAAAAAAaKAI8wGgjqzOkAqMU21fhzQwwr56AK+U+Z6U+Za1z7+71HymPfXUJZ/gkscIOEKs/ckPSAXb7amptOK8kqX/jUxrf7MZUuC59tQEAAAAAAAAAA2Yn90FAEBjsSzV2u4dKoX7NZIlqYuzpJzvJL9WUuAAluJGxQq2ScllnrnuCCkJuH2C7amprgV0LVmuPum2U31GjpR4oxT+gH11SVLucqlgo7Uv9A4p7A+2lAMAAAAAAAAADR1hPgDUkeVp1vagpraUUfeMAunoFVL+LyXtqKlS5ER7a4L3MYqkxJtLguvSWswqCbgbk9BbS4LzzFmn+gp3SCf+ZFtJFfI/S2r+GjfnAAAAAAAAAICHsMw+ANSBHKeh+Axr3+BIW0qpe9lfngryJSn171JxZuXj0TjlfCMVbrX2hT1QEmw3Rs1elgJ62l1F5RzBUvTnkk9I9WMBAAAAAAAAAKeFMB8A6sDqDKnAONX2dUgDI+yrp05lvmNtGzlS1qf21ALvVfbnxL+b1Ow/tpTiFXyCSh4v4Ai3u5KKNX9NCuhmdxUAAAAAAAAA0KCxzD4A1IFlqdZ271Ap3K8RLE1ddEjK/b58f+Y7UjjP2cb/OJOknAXWvohHSgLtxsz/TKnNT1L6NMmZaHc1JRwhJaslhN5kdyUAAAAAAAAA0OAR5gNAHVieZm0PampLGXUvc44ko3x//s9S4W+Sf+c6LwleKOtDSUWn2o4mUuho28rxKoHnSC3ft7sKAAAAAAAAAIANWGYfADws12koPsPaNzjSllLqlmFIWe9U/nrm7DorBV7MMMovsR9yveTTWJ5DAQAAAAAAAABAxZiZXwvFxcVav369Dh48qOTkZIWHh6t169bq27evgoOD66yOhIQEbdmyRcePH1dOTo6aNGmiqKgodevWTR07dpSPD/dsAHZanSEVlJqc7uuQBjaGnDJ/Vcns+8pkvis1/bvk8K27muB9CjZIBZutfaF32VIKAAAAAAAAAADehDD/NDidTr311luaM2eOkpKSyr0eHBysq6++WhMmTFBEhGcSO8Mw9Pnnn+vdd9/Vb79VHpbFxMTo5ptv1l133aWAgACP1AKgasvSrO3eoVK4n8OWWupU2dnWPs2k4hOn2s7DUu5iKfjKuq0L3qXsz4nfGVKTIfbUAgAAAAAAAACAF2HKdg1lZGTo9ttv14svvlhhkC9JOTk5+uyzzzRixAht377d7TVkZWXpzjvv1N/+9rcqg3xJOnz4sF588UVdf/31Onr0qNtrAVC95anW9qCm9tRRp4qzpaxPrH0Rj0kBvax9ZYNcNC5GvpT1obUvdIzk4J8nAAAAAAAAAAAwM78GioqK9Mgjj2j9+vVmX5s2bTRixAjFxMQoJSVFixcv1pYtWyRJx44d09ixY/XZZ58pOjraLTUYhqE//vGPWrNmjdnn7++vIUOGqFevXoqIiFBmZqa2bt2qH374Qbm5uZKk3377TXfddZe++uorNWnSxC21AKhertPQ6gxr3+BIW0qpW9lzJSOrVIePFHan5BMqndhwqjvnK8mZKvk2hjscUE7211JxirUv7C5bSgEAAAAAAAAAwNsQ5tfAO++8o1WrVpnta665RlOmTLEsXz927Fi99957eu6552QYhhITE/Xkk09q1qxZbqlhwYIFio+PN9vt27fX66+/rg4dOpQbm5iYqIceesi8ueDAgQN66623NG7cOLfUAqB6qzOkAuNU29chDfTM0ze8S9kZ902ukPxipNBbpRP/J6mgpN/Il7I/lsIfrPMS4QXK/pwEDZL8O9pTCwAAAAAAAAAAXoZ1bF2UlZWlN99802x369ZNzz//fIXPob/zzjt12223me3ly5fr119/dUsd8+bNM7d9fHw0ffr0CoN8SYqOjtarr76q4OBgs+/rr792Sx0AXLMszdo+L1QK93PYUkudKdwv5S219oXdXfK/vs2kkBHW11hqv3EqOiLlLrL2MSsfAAAAAAAAAAATYb6L5s2bp7S0NLM9YcIE+flVvrDBn/70J8ty9u+9955b6ti+fbu53aNHD8XFxVU5vmXLlrr44ovN9oEDB5SXl+eWWgD8P3t3HmZnXd6P/31myTqTlWwsooRFBWURaEWpigiyCsqigASQVlCx6FdUWrF++6O1KOKGdflKDSBVDCgBEbWAihRUCigKikKQVbKQdbLO8vz+SDOZZ5LAJDNzziyv13V5cT73PM/nuQPDoVff53OfF/azJeX16ybUpI3qWnFleV03IRnTJcDvHtiuvSdZ92B/d8VA03J1ko6N68rYZOwJNWsHAAAAAAAGGmF+D912222dr3fYYYe8+tWvft7rm5ubc/jhh3euf/7zn2fdunW97mPZsmWdr3faaace3fOiF71oi3sA/Wd1e5FfLC/XXj/Uvxq+6EhaZpdrTackdaM2rkcfntTPKF/jdP7wUhSb/jMfe1JS11SbfgAAAAAAYAAS5vfAmjVr8qtf/apzfdBBB6VSeeEx2QcddFDn65UrV/bJqP1x48Z1vl61alWP7lm9enXn6/r6+kyYMKHXfQAv7BfLk3XFxnVdkteOr1k71bHmZ0nb4+XahhH7G1QakqZ3lmst30yK1v7tjYFj7S+T1ofLte6/JwAAAAAAMMwJ83tg3rx5aW3dGDLtvffePbpv3333La0ffvjhLVzZc/vss0/n61//+tc9Ou3/y1/+svP1K17xiowcObLXfQAv7KdLy+tXNSfjGl74g0CDWvfT1o17JSNetel13YPb9vnJqh9ueh1DU/ffk4aZyajX1qYXAAAAAAAYoIT5PfDoo4+W1jvvvHOP7tthhx1SX1/fuZ43b16veznllFM6Xy9evDj//u///rzXX3vttfnjH//YuT7zTCcfoVp+tqS8ft2EmrRRPR3Lk5XXlWvNZyabm2Qy4qXJyL8u14zaHx46ViUt3y7Xms/Y/O8JAAAAAAAMY8L8HnjqqadK6xkzZmzhyrL6+vpMmTKlc/3kk0/2upeDDz44J510Uuf6y1/+ci688MI88sgjpeuefPLJ/Ou//ms+8YlPdNZOPvnkvPnNb+51D8ALW91e5BfLy7XXT6xNL1XT8p2kWN2lUJ80nbrl65vPKK9X3ZS0L+yPzhhIVn4vKbr+y1FJmmfVrB0AAAAAABioGmrdwGDQ0tJSWo8f3/MvvR43blyeffbZJMnKlSv7pJ9PfOITmTx5cr7+9a+ntbU13/3ud/Pd7343zc3NGTduXFpaWrJs2bLO65ubm/Oe97zHqXyool8sT9YVG9d1SV7b87eOwan7yfoxRyUN07Z8fdPbk+fOT4o1/1toS1quScaf308NMiC0dPs9GX1o0rBTbXoBAAAAAIABTJjfA6tWrSqtt+Y750eNGrXFfbZVfX19zj///LztbW/LRRddlLvvvjtJsmLFiqxYsaJ07Stf+cr8y7/8S3bfffc+eXZfeeSRR1JXZzBEb7S2tnb+9YEHHqhxN3R37aqpSTYG2S+rX5U/P/Tolm8Y5EZU/pyXjr2rVPvzc2/I8gXP/7u508hDMrHxB53r1Qu+kj89fki/9EjtNVaeyUvH3F6aqP/Ekjdm6cKB9x7mPRag/3iPBehf3mcB+o/3WID+MxTeYzs6Ovp8T2F+D6xdu7a0bmxs7PG9I0aM6Hy9Zs2a57ly61x77bW5/PLLs2DBgue97oEHHsjxxx+f448/Ph/96EfT1NTUZz30Rnt7e9rb22vdxpCx4Q2OgeOedWNK6/3qVwzpf05TRt1QWrd2TMxza16d5Pn/zAuLo0ph/uj6h9PQ8dusbn9pP3RJrU0eNTeVysaRFW0dTVm0+rUpXuD3pNaG8r+7ALXmPRagf3mfBeg/3mMB+o/32I2E+T3Q/SR+a2trj0/nr1u3rvN111P626qjoyMf/ehHM3fu3M7awQcfnFNPPTWvfOUrM27cuKxcuTIPPfRQrr/++nz/+99PW1tb5syZk9/85je56qqrMnFi7b+4u76+3sn8Xur6RrY1HzCh/60pKvld+9hS7cCRq4fwP6f2TB55c6mytO2oNDaOfsE71+SgrOuYnhF1z3bWpo66Oc+se0Wfd0mtdWS7kd8vVZa1vzkNjc016uf5eY8F6D/eYwH6l/dZgP7jPRag/wyF99iOjo4+P8wszO+BMWPKJ2zXrl3b4zC/62n87vtsi6985SulIP+CCy7I2WefXbpmwoQJOeigg3LQQQflkEMOyYc+9KF0dHTkj3/8Yz72sY/lS1/6Uq/76K1dd911wEwJGKweeOCBtLa2prGxMa985Str3Q5d/HRJkdbFG9d1Sd65z4szrqGyxXsGtVU/TJ4tTwmZ8pILMmVkD38vF5+dLL24c7ndqB9luz2+kVR6/pUmDAKrf5b85alSafLOH8rkUQPz/ct7LED/8R4L0L+8zwL0H++xAP1nKLzHtrS05OGHH+7TPR2N7oHuofOyZct6fG/X77AfO3bs81z5wpYsWZKvfvWrnetDDz10kyC/u6OOOiqnnXZa5/rWW28dtN8zAYPFT5eW169qztAN8pNkxTfK6xH7JT0N8pOk+YzyumNxsvKmXrfFANP996TxZcnIA2vTCwAAAAAADALC/B7YcccdS+u//OUvPbqvvb299J32O+20U6/6uP3220sn/U899dQe3df9ultvvbVXfQDP72dLy+vXTahFF1XSviRZeUO51nzm1u3RODMZ9TflWsvs3nTFQNPRkqy8rlxrPjOpDOEPuQAAAAAAQC8J83tgl112Ka2feOKJHt339NNPl74Xofs+W6v7WIa99tqrR/e9+MUvLk0XeOSRR3rVB7Blq9uL/GJ5ufb6ibXppSpavpVkXZfCiKTplK3fp/sHAFbdkrT17INTDAIr5yTFyi6F+qTptC1eDgAAAAAACPN7ZJdddkljY2Pn+te//nWP7rv//vtL6913371Xfaxevbq0Hj16dI/vHTNmTOfrtWvX9qoPYMv+cV6ytmPjui7Ja8fXrJ3+19JtdPrYtyT1k7Z+n7EnJJWuX0XSkbRc3avWGEC6j9gf8+akYUZtegEAAAAAgEFCmN8Do0ePzgEHHNC5vvvuu1MUxQved9ddd3W+HjNmTPbff/9e9TFu3LjS+rnnnuvRfa2trVmyZEnnevz4oZwsQu3MXVjkc0+Va6+bkIxrGKKjxNf9Lln7P+Va8xnbtlddUzL2pHJtxTeSHrzXMsC1PpKs+Xm51rSVX8UAAAAAAADDkDC/hw499NDO10899VTuvvvu571+xYoV+dGPftS5PvjggzNixIhe9bDzzjuX1v/93//do/vuueeetLa2bnEfoPf+vLrImX8o10bWJZfuWpt+qqL7aev6Gcnow7Z9v+4fBGj9Q7L2l9u+HwPDitnldd3kZOwxNWkFAAAAAAAGE2F+Dx177LGlE+2XXnpp2tratnj95z73udJY/NNPP32L1x5yyCHZY489sscee+SQQw7Z4nUHHXRQaf21r30tK1eu3MLV67W2tubzn/98qfaa17zmee8Bts66jiLveChZ2u0t4bO7Jvs2D9FT+UVr0vLNcq3p9KTSsO17jjo4aZhZrnX/wACDS9GerLiyXGs6Nan07sNtAAAAAAAwHAjze6i5uTlnn3125/rBBx/MRz/60dKJ9w2uvvrqXHPNNZ3rgw8+uNcj9pNkxx13LE0I+POf/5x3v/vdWbBgwWavX7ZsWd7//vfn17/+dWftla98ZZ/0Amx04bzkl8vLtZOnJu/evjb9VMWqW5L2bu89zb0cnV6pbHo6v+XbScfqzV7OILD69qS923dP9Pb3BAAAAAAAholeHKEcfs4888zceeed+eUv1499vummm3LfffflmGOOyY477pjFixfn1ltvzQMPPNB5z5QpU3LxxRf3WQ8f/ehHc99992Xx4sVJ1o/QP/TQQ3PooYfmla98ZcaNG5eVK1fmoYceyo9+9KPSyf0xY8bkE5/4RJ/1AiRzFxb57JPl2q6jk6/ukVQqQ/RUfrLpifmRr05G7NH7fZtnJUs+nqRYvy6WJ6u+lzSd0vu9qb7uvycj9k5G7lOTVgAAAAAAYLAR5m+FxsbGfPGLX8y73/3u3H///UmSp59+Ol/5ylc2e/3UqVPz5S9/OdOnT++zHnbaaad8/etfz3nnnZenn346SbJ27drcfPPNufnmm7d436RJk3LZZZdlzz337LNeYLh7fE2RM/9Qro2oJNfumYxrGMJBfvuCZNX3y7XuJ+q3VcNOyehDk9X/tbG24hvC/MGofen6D2J05VQ+AAAAAAD0mDH7W2n8+PG55ppr8oEPfCBTpkzZ7DVjxozJCSeckJtuuil77bVXn/ew55575sYbb8x73/veLfawwYQJE3LmmWfmpptuyqtf/eo+7wWGq3UdRd7+YLK0rVz/7G7Jvs1DOMhPkhXXJOnyB6+MTppO7rv9u38wYPVtSdsTfbc/1bHy20mxpkuhMWk6tWbtAAAAAADAYONk/jaor6/POeeck7/927/Nfffdl8cffzzPPfdcxo0blxkzZuTAAw/MmDFjerzf7bffvtU9NDU15f3vf3/OO++8zJs3Lw8++GAWL16cVatWZfTo0ZkwYUJe+tKXZvfdd099ff1W7w88v3+Yl/xyebl20tTknO1r00/VFEXS0m10+ti3JnXj++4ZY45fv1/Hsg0PTVZcmUy8qO+eQf/rPmJ/zDFJ/Xa16QUAAAAAAAYhYX4v1NfX54ADDsgBBxxQsx4qlUpmzpyZmTNn1qwHGG5uXFTksifLtZmjk6/tsf7fySFt3f3Jut+Wa309Or1udDL27cmKr26srZidTPhYMtT//g4V6x5K1v6qXDNiHwAAAAAAtoox+wBb4fE1Rc78fbk2opJ8Z89kXMMwCJq7n7ZueFEy6g19/5zuwW/bvGTNz/v+OfSPFbPL6/ppyZg316QVAAAAAAAYrIT5AD3U2lHkHQ8mS9rK9ct2S/ZtHgZBfrE2afnPcq1pVlLph/+UjDwwaXxZudb9gwQMTEVb0nJ1udb0zqRiGBAAAAAAAGwNYT5AD/3DvOQXy8u1E6ck525fm36qbuWNScficq35jP55VqWy6d4r5yQdLf3zPPrOqh8m7c+Wa0bsAwAAAADAVhPmA/TATYuKfObJcm3m6ORrL00qw+V73LuPTh/1uqRxl/57XtM7k9RvXBcr1wf6DGzdJyiMPDAZ8fLa9AIAAAAAAIOYMB/gBTyxpsgZvy/XRlSSa/dMxjcMkyC/7Zlk9Q/Ltf4+bd0wY9PvWe/+gQIGlvZFyaqbyjWn8gEAAAAAYJsI8wGeR2tHkbc/mCxpK9cv2y3Zr3mYBPnJ/34HesfGdaUpGXtC/z+3qVsQvOaOpPXR/n8u26blP5O0blxXRiZjT65ZOwAAAAAAMJgJ8wGexz/MS36xvFw7YUpy7va16acmimLT0eljT0zqxvb/s8cek9RNLteczh+4uv+ejDk+qZ9Ym14AAAAAAGCQE+YDbMH3FxX5zJPl2i6jkv/30qRSGUan8tf+Iml9uFyr1uj0yoik6dRybcWVSdFenefTc2t/naz7dblmxD4AAAAAAGwzYT7AZjyxpsgZvy/XRlSSa/dKxjcMoyA/2fQkfMOuyajXVu/5zWeU1+1PJqtvr97z6Znup/Lrd0xGv7E2vQAAAAAAwBAgzAfoprWjyDseTBa3leuf2TV5VfMwC/I7ViUt3y7Xms9IqjmZYOS+yYi9y7WW2dV7Pi+sWJe0XFOuNc9KKvW16QcAAAAAAIYAYT5AN/84L7l7ebl2wpTkPTvUpp+aWvm9pOj6N6OSNJ9e/T66j2tf+d2kfWn1+2DzVn0/6XiuXGueVZteAAAAAABgiBDmA3Tx/UVFLn2yXNtlVPL/XppUqnkafaBo6TY6ffShScNO1e+j6dQkjRvXxZpk5bXV74PN6z5if9Rrk8bdatMLAAAAAAAMEcJ8gP/15JoiZ/y+XBtRSa7dKxnfMAyD/NbHN/1u+u4n5KulfrtkzDHlWvcAmdpoezZZdUu51lSj3xMAAAAAABhChPkASVo7irzjwWRxW7l+6a7Jq5qHYZCfJC1XJSk2ruvGJ2OOq1U3SfMZ5fXaXybrfr/ZS6milquTtG9cV8YkTSfWrB0AAAAAABgqhPkAST72WHLX8nLtbVOS9+5Qm35qruhIVswu18a+I6kbXZN2kiRjjkjqp5Vr3XukuopiM78nJyZ1zTVpBwAAAAAAhhJhPjDs3byoyKefKNd2GZV8/aVJpTJMT+Wv+XnSNq9cq9WI/Q0qDUnTO8u1lquSom3z19P/1t6TtD5UrnWfoAAAAAAAAGwTYT4wrD25psisbpPaGyvJt/dMxjcM0yA/2fT76Btflow8oDa9dNX9AwXtzyarf1SbXtj096ThJcmov6lNLwAAAAAAMMQI84Fhq7WjyDseTBZ3O9h96a7J/uOGcZDfsSJZOadcaz4zGQhTCka8PBl5YLnWPVCmOjpWJyu/Va41n5FU/J8WAAAAAADQF/x/3IFh66LHkruWl2tvnZK8b4fa9DNgrLwuKVZ1KdRvOt6+lrqPcV95Y9K+qCatDGurbkg6lnUpVJKmWbXqBgAAAAAAhhxhPjAs/eC5Ip96olx7yajk63sklYFwAr2Wup90H3NE0jC9Nr1szti3J5WRXQqtScu3tng5/WTF7PJ69CFJ4841aQUAAAAAAIYiYT4w7Dy5psis35drjZXk2j2TCY3DPMhvfSRZ8/Nyrfv31Nda/cRkzPHlmlH71dX2ZLL6v8q1pjNq0goAAAAAAAxVwnxgWGntKPKOB5PnWsv1S3dN9h83zIP8ZNPT1nWTkzFH16SV59X9Awbr7k/W/qY2vQxHK65KUmxcV8YlY99as3YAAAAAAGAoEuYDw8pFjyV3LS/X3joled8OtelnQCnakxVXlmtNpyaVEbXp5/mMfmNSv2O55nR+dRTFph/6aDo5qRtTk3YAAAAAAGCoEuYDw8YtzxX51BPl2otHJV/fI6lUnMrP6tuT9qfKtYE2Yn+DSn3SPKtca7kmKdbVpp/hZM2dSdsj5dpA/T0BAAAAAIBBTJgPDAtPrSly+u/LtcZKcu2eyYRGQX6STU+2j9gnGblPLTrpme5hfseiZNXNtellOGmZXV437pGM/OuatAIAAAAAAEOZMB8Y8to6irzjoeS51nL907smB4wT5CdJ2pcmq75Xrg3009aNuyWjXluuGbXfvzpWJi3fKdeaz0hMtgAAAAAAgD4nzAeGvIseS/57Wbl2/HbJeTvUpp8BaeW3k2JNl0Jj0nRKzdrpsaZuHzhY9YOk7dna9DIcrLwuKVq6FOqSptNr1g4AAAAAAAxlwnxgSLvluSKXPFGuvXhUcsVLk4rTxBt1P9E+5pikfrva9LI1mk5MKmO6FNqTlm/WrJ0hr/vvyejDk4bta9MLAAAAAAAMccJ8YMh6ak2RWb8v1xorybf3TCY0CvI7rXsoWfurcm2gj9jfoK45GXtiubbiG0lR1Kafoax1XrLmZ+XaYPk9AQAAAACAQUiYDwxJbR1FTnkoWdRarn9qZnLgOEF+yYrZ5XX99GTMm2vSyjZpPqO8bn0oWfs/NWllSFtxZXldNzEZe2xtegEAAAAAgGFAmA8MSR9/LLlzWbl23HbJ+3esTT8DVtGWtFxdrjW9M6k01KafbTHqb5KGl5RrLd/Y/LVsm6IjaekW5jedklRG1qYfAAAAAAAYBoT5wJDzw+eK/NsT5dqLRyVXvDSpVJzKL1n1w6T92XKt+0n3ga5St2nPLd9KOtbUpJ0hac1PkrbHyzUj9gEAAAAAoF8J84Eh5ak1RU7/fbnWWEm+vWcysVGQv4kV3U6wjzwwGfHy2vTSG02zknT559uxNFl1Q42aGYK6/56MeEUyYr/a9AIAAAAAAMOEMB8YMto6ipzyULKotVy/ZGZy4DhB/ibaFyWrbirXButp68adk9GHlGvdA2i2TceyZOX15VrTmYkpFwAAAAAA0K+E+cCQ8U9/Tu5cVq69Zbvk73esSTsDX8t/JunyyYfKqGTs22vWTq81nVFer/6vpO2pmrQypLR8Jym6fmVBQ9J8as3aAQAAAACA4UKYDwwJty4u8sluX+m986jkP16aVJwg3rzuJ9fHHJ/UT6hJK31i7FuTyrguhSJZcVXN2hkyNvk9OSqpn1qbXgAAAAAAYBgR5gNDwkWPldeNleTaPZOJjYL8zVr762Tdr8u15jNq0EgfqhuTNJ1crq34RlIUtelnKFj3h2Tt3eXaYP0qBgAAAAAAGGSE+cCg9/uVRX65vFz75C7JgeME+VvU/bR1/Y7J6DfWppe+1D1obnskWfvftellKFgxu7yun5qMObImrQAAAAAAwHAjzAcGvSufLa+nNibn7VibXgaFYl3Sck251jwrqdTXpp++NPKvk8Y9yrXuH1ygZ4q2pKXb1xQ0nZZUGmvTDwAAAAAADDPCfGBQay+KfLNbmH/q9KSxzqn8LVr1/aTjuXJtsI/Y36BS2fTP0vKdpGNlTdoZ1Fb/V9L+l3JtqPyeAAAAAADAICDMBwa1Wxcnz6wr12ZNr00vg0b3k+qjDk4ad61NL/2h6fSU/vNWtCQrr69ZO4NW99+TEa9KRryiNr0AAAAAAMAwJMwHBrXuI/b3a0pe2eRU/ha1PZusuqVcazqjJq30m4btk9GHl2tG7W+d9sXJyrnlWvOZtekFAAAAAACGKWE+MGgtbS3yvUXl2qwZtell0Gi5Okn7xnVlTNJ0Ys3a6Tfdg+c1P01a59WklUGp5T+TdB15MSJpeketugEAAAAAgGFJmA8MWtcuSNZ2bFw3VpJ3TK1dPwNeUSQrZpdrY09M6ppr0k6/GntsUjepXFtxZW16GYy6TzIYe1xSP2mzlwIAAAAAAP1DmA8MWt1H7B+zXbLdCCP2t2jtPUnrQ+XaUB2dXhmZNJ1SrrVcmRQdm7+ejdY+kKy7r1wbqr8nAAAAAAAwgAnzgUHpDyuL/GJ5uTZrem16GTS6n7Zu2CUZdXBteqmG5jPK67bH14/b5/m1zC6v67dPRr+pJq0AAAAAAMBwJswHBqXup/KnNiZvNgV8yzpWJyu/Va41z0oqQ/g/AyP2S0a8olzr/oEGyorWZMU3y7Xm05NKfW36AQAAAACAYWwIpzjAUNVeFLm6W5h/6vSksc6I/S1adUPSsaxLoZI0zapVN9VRqSRN3cbDr7y+298HSlbdnHQsLNe6/z0EAAAAAACqQpgPDDq3Lk6eWVeuGbH/AlbMLq9HH5I07lyTVqqq+bQkDRvXxeqk5Ts1a2fA6z65YORByYjda9MLAAAAAAAMc8J8YNDpPmJ/36bklU1O5W9R25PJ6v8q14bLaev6KcmYo8u17h9sYL22+etP5nfVPEx+TwAAAAAAYAAS5gODytLWIjcsKtdmzahNL4PGiquSFBvXlXHJ2ONr1k7VNZ9RXq+9K1n3cE1aGdBarknSvnFdGZ00nVSzdgAAAAAAYLgT5gODyncWJms6Nq4bK8kpU2vXz4BXFJueRG86OakbU5N2amLMkUl9t1+Sltk1aWXAKopNR+yPfVtSN642/QAAAAAAAMJ8YHC58i/l9dGTk+1GGLG/RWvuTNoeKdeG2+j0SmPSdFq5tuKqpGjf/PXD0bp7k9bflWvD7fcEAAAAAAAGGGE+MGg8vKrI3cvLNSP2X0D3E+iNeyQj/7omrdRU92C6/Zlk9Y9r08tA1P1UfsOLk1Gvr0UnAAAAAADA/2qodQPAEND2TNLxXL8/5od/KbJX48b1pIbkiKYk67qczK+MSxpelFSc1k/HyqTlO+Va85nD8+/NiL2Skfsna/9nY235l5OGHWvX00BRtCct3yrXmmYlFZ/3AwAAAACAWhLmA9uuaE8WnpG0fLMqj/v7SvL323crPrOZC0cflkz73vD6XvjNWXldUrR0KdQlTe+sWTs113RGOcxfddP6/7Gp5lm17gAAAAAAAIY9x+6Abbfkn6sW5G+V1T9Onjuv1l3UVvuCZPGF5drow5OG7p+GGEaa3pFkRK27GPhGvT5pfEmtuwAAAAAAgGFPmA9sm1X/lSz9/2rdxZat+I9kxZW17qI2ivZkwalJ+1/K9eZ31aafgaJ+UtJ0cq27GPjGvbvWHQAAAAAAADFmH9gWbU+vD4tTdPtB/3wXe5Gk6PaoSmVzT+t20aJzk5GvWv996cPJ0n9JVt9aro0+NBl7fG36GUgmfy5pX5isvi1JW627GVjqJiTNf5uM9YEHAAAAAAAYCIT5wNYp2pIF70g6Fpbrkz6dTPhQvzzy/z1T5JyHN64bK8nTByXbjegW57dcmyx4e5deVyfzT0x2uCepa+qX3gac1bclSz5RrtXPSKZek1QMY0n9pGTGLbXuAgAAAAAA4AVJdoCts+SiZM3Py7UxxyTj/0+/PfKqbtPij568mSA/WT9Cfdy55VrrH5JF52x6tH8oavtLsuCUlCcU1CdTv53UT61VVwAAAAAAAGwDYT7Qc6tuTpb+W7nWsHMyZfb6uff94I+rity1vFw7ffrz3DDpsmTEfuVayzXJiq/3eW8DyoaJCe0LyvVJFyej/6Y2PQEAAAAAALDNhPlAz7Q9kSw4vVuxMZn6nfWjy/vJlc+W11MakyMnP88NdaOSad9JKuPK9efOS9b+uq/bGziWfCJZ87NybfSRyfgP16QdAAAAAAAAekeYD7ywYl0y/+SkY3G5PvnSZNSB/fbY9qLI1d3C/FOmJY11LzAFoHFmMvUb5VqxNpl/YtKxfPP3DGarbkmW/ku5Vr9TMvWqpOJtHgAAAAAAYDCS8gAvbPFHk7W/KNfGvi0Zd16/Pvb2JclTa8u1M2b08Oaxb03GnV+utT2SLDw7KYrN3jIotT2ZLHhnt2JDMu3apP75RhgAAAAAAAAwkAnzgee38nvJss+Waw0zkylXJJUXOCHfS91H7O/TlOzdtBXPnHxJMrLb5ICVc5Ll/9775gaCojWZ//ak47lyfdKnklGvrk1PAAAAAAAA9AlhPrBlrfOShWeWa5WR67+Tvm58vz56WVuR7y4s12ZN38pNKiOSqd9J6iaW6899IFlzT6/6GxAW/0Oy9q5ybcxxyfjza9ENAAAAAAAAfUiYD2xesTaZf1LSsaxcn/z5ZOR+/f747yxI1nRsXDdUklOmbcNGjTsnU67qVmxNFpyUtC/pTYu1tfLGZNml5VrDS5Ip/9HvExMAAAAAAADof8J8YPOe+z/JunvLtbHvSJr/riqPv/Iv5fXRk5MpI7YxpB57dDL+w+Va25/XTx0oim3bs5Za/5wsnNWtOGL9xIT6iZu7AwAAAAAAgEFGmA9squXaZPmXyrXGPZIpX63Kqe8/ripy1/JybatH7Hc36eJk1GvLtVVzk2Wf7eXGVVasWz9VoGNpuT75smTk/jVpCQAAAAAAgL4nzAfK1v0xWfi35VpldDJtTlLXXJUWrnq2vJ7SmBw5uZebVhqTqd9O6rYr1xd/JFlzdy83r6LnLkjW3lOujT0pGfee2vQDAAAAAABAvxDmAxt1rE4WnJgUK8r17b6UjHhFVVpoL4pNwvxTpiWNdX0wEaBhh2TqNUm67tWWzD8paV/U+/37W8t1yfIvlGuNuyVT/l9VJiYAAAAAAABQPcJ8YKPn/j5Z90C51nRG0nxm1Vr4yZLkqbXlWq9H7Hc15rBkwsfKtfankgWnJ0VHHz6oj7U+kix8V7lWGZVMnZPUjatNTwAAAAAAAPQbYT6w3oqrkxX/r1xr3HP9qfwqurLbqfy9m5J9mvv41PnEf0pGvaFcW31LsuxTffucvtKxJpl/YlIsL9cnfzEZuXdtegIAAAAAAKBfCfOBZN1DyaJzyrXK2GTanKRuTNXaWNZW5LsLy7U+PZW/QaU+mfqfSf20cn3xPyarf9YPD+yl585P1v26XGs6LWl+1+auBgAAAAAAYAgQ5sNw17Hyf099ryrXt/tqMuJlVW1lzoJkdZdJ9w2V5JRpW76+VxqmJ1O/lfLbYEey4B1J2/x+eug2aPnPZMVXy7XGlyXbfTmp9PHEAgAAAAAAAAYMYT4MZ0WRLDo3aX2oXG/+u6T51Kq3033E/lGTk6kj+jGwHv2GZOL/Ldfa/5IsPC0p2vvvuT217g/Jwr8r1ypjkmnXJXVNtekJAAAAAACAqhDmw3C24j+SlqvLtRH7JJM/X/VW/rSqyH8vK9f6ZcR+dxP+IRl9eLm2+tZk6cVVePjz6Fj1vxMTVpbr2305GfHy2vQEAAAAAABA1QjzYbha+0Dy3PvKtUpzMm1OUjeq6u10P5W/XWNy5OQqPLhSl0y9OqnfoVxf8n+TVbdWoYEtWPS+pPV35Vrzu5Lm02vTDwAAAAAAAFUlzIfhqGNFsuDEpFhTrk+5ImncterttBdFruoW5p8yLRlRV6XvhK+fkkz7dpL6LsUiWXhq0vZMdXroasXspOUb5dqIVyaTv1j9XgAAAAAAAKgJYT4MN0Wx/nvYW/9Yro97X9J0Yk1a+smS5Km15doZ1Rix39Wo1yaT/rVca1+QLHhHUrRVr491v0sWvadcqzQlU+ckdaOr1wcAAAAAAAA1JcyH4WbFV5KV3y7XRu6fTL60Nv0km5zK37sp2ae5Sqfyuxr/oWTM0eXamjuSJf9Uned3tCTzT0yK1eX6lK8nI3avTg8AAAAAAAAMCMJ8GE7W3pssOr9cq5uQTP1OUhlZi46yvK3I9QvLtVnVPpW/QaUumXJl0vCicn3pvyarbunfZxdFsujdSesfyvVx5yZNJ/fvswEAAAAAABhwhPkwXLQvTeaflGRduT5ldtL4kho0tN6cBcnqjo3rhkpyyrSatZPUT1r/4YY0lusLTkvanuy/5674f0nLf5ZrI/ZLJl3Wf88EAAAAAABgwBLmw3BQFMnCs5K2eeX6+P+TjH1LbXr6X1d2G7F/5ORk6ogajNjvatRfJZM/Xa51LE7mn5wUrX3/vLX3J8+9v1yrG59Mm5PUjer75wEAAAAAADDgCfNhOFj+hWTV98q1ka9OJn2yNv38r0dWFblzWblWsxH73Y17fzLmreXa2ruTxRf27XM6lq2fmFCsLdenfCNp3KVvnwUAAAAAAMCgIcyHoW7NL5LnPlSu1U1Opl2bVBo3f0+VdD+VP7kxOWpybXrZRKWSTP2PpKFboL7sM8nKuX3zjKJIFp6dtD1Sro87Pxl7fN88AwAAAAAAgEFJmA9DWfviZMHJSdrK9alXJw071aSlDTqKIld1C/NPmZaMqKvxiP2uNoy6z4hyfeEZSetjvd9/+ZeSldeVayP/Kpl8Se/3BgAAAAAAYFAT5sNQVXQkC2clbU+U6xMuTMYcUZueuvjJkuTJbpPlzxgoI/a7Grlfst3nyrWOpcmCzYzG3xpr7kme+2C5VjcxmXptUhmx+XsAAAAAAAAYNoT5MFQtuzRZ9f1ybdTfJBP/uTb9dNN9xP4rxyb7NNWmlxfUfE4y9u3l2tr/SZ67YNv2a1+SLDgxSWu5PuWqpHHnbdsTAAAAAACAIUWYD0PRmjuTxf9QrtVPTaZ+K6k01KanLpa3Fbl+Ybk2a0ZSqQygEftdVSrJlK8ljbuX68u/mLTM2bq9imL9mP62x8v18R9Oxh7dqzYBAAAAAAAYOoT5MNS0L0zmn5ykvUuxkkz9z6Rh+1p1VXLdwmR1x8Z1QyU5dVrt+umRuuZk6pykMqpcX/iupPVPPd9n2WXJqhvLtVGvTSZd3PseAQAAAAAAGDKE+TCUFB3JgtOS9mfK9Yn/lIx+Y2162owr/1JeHzk5mTpigJ7K72rkK5PJXyrXihXJ/JOSjtUvfP+au5LFHy3X6rZLpn47qTT2XZ8AAAAAAAAMesJ8GEqW/muy+sfl2uhDkwkfq00/m/HIqiI/X1auzZpem162SfOZSdPp5dq6XyfPnf/897Uv+t+JCW1dipVk6jVJww592yMAAAAAAACDnjAfhorVP0mW/FO5Vj8jmfLNpFJfm54246pny+vJjclRk2vTyzapVJLt/j1pfHm5vuJryYprNn9P0ZEseGfS/lS5PuFjyZjD+qdPAAAAAAAABjVhPgwFbc8mC96RpMsX0adu/fj2hoHzZfQdRbFJmH/KtGRE3SAYsd9V3dhk2nVJZUy5vujdybrfb3r90kuS1T8s10a9Yf3XHwAAAAAAAMBmCPNh0GtfH+S3zy+XJ16cjP6b2rS0BT9dmjyxtlwbVCP2uxrxsmS7r5Zrxcpk/olJx8qNtdU/S5Z0+5qD+mnJ1P8cUBMTAAAAAAAAGFiE+TDITRvx5WTNT8vF0UckEz5Sk36ez5V/Ka9fMTbZt6k2vfSJ5tOS5r8t11ofTBa9b/3rtvlbmJjwraRhsH6KAQAAAAAAgGoQ5sMgNq7h7kxt/Hq5WL9TMvXqpDKw/vVe3lbkuoXl2qzpSaUyyEbsdzf588mIvcu1ltnJ8q8nC09N2rt9gmHi/01Gv6Fq7QEAAAAAADA4Day0D+ixxsr8vHjsRalUii7VhmTatUn95Jr1tSXXLUxWdzmgXl9JTh0Kh9PrRifT5iSV5nJ90d8mq28r10Yfnkz4h+r1BgAAAAAAwKAlzIfBqGjNi0Z9JI11S8v1SZcko15dk5ZeSPcR+0dOSqaNGOSn8jdo3C2Z8vXnv6Z+hwE5MQEAAAAAAICBSaoEg9HyL2ds/f3l2pi3JOM/UJt+XsCjq4v8fFm5NmtGbXrpN00nJePeu4Uf1v/vxIQpVW0JAAAAAACAwUuYD4PRqh+W1w0vTqZ8Ixmg3z9/1bPl9eTG5OiB900AvTf5M8mIV21an/TJZNRrqt8PAAAAAAAAg5YwHwaj0W/sfNlRjEqmfiepn1jDhrasoyg2CfPfMTUZUTcwP3jQK5WRybQ5SV2XTyqMOS4Z/39q1hIAAAAAAACDU0OtGwC2wfgP5oln1qSxeDTLO47LHqMOqHVHW/Szpcnja8q1M4baiP2uGl+S7PjrZPlXk/rpybh3JxWfmwIAAAAAAGDrCPNhMKpUsrTtmLS2tqaxsbHW3TyvK7udyn/F2GTfptr0UjUNOyaT/r9adwEAAAAAAMAg5rgo0G9WtBW5bkG5Nmt6UqkMwRH7AAAAAAAA0IeE+UC/uW5hsqpj47q+kpw6vXb9AAAAAAAAwGAhzAf6zZV/Ka+PmJRMG+FUPgAAAAAAALwQYT7QLx5dXeSOZeXaLKfyAQAAAAAAoEeE+UC/uOrZ8npSQ3L0drXpBQAAAAAAAAYbYT7Q5zqKYpMw/x3TkpF1RuwDAAAAAABATwjzgT53x9Lk8TXl2hkzatIKAAAAAAAADErCfKDPXdntVP5eY5P9mmrTCwAAAAAAAAxGwnygT7W0FbluYbk2a3pSqRixDwAAAAAAAD0lzAf61HULk5XtG9f1leTUabXrBwAAAAAAAAYjYT7Qp7qP2D9iUjJ9pFP5AAAAAAAAsDWE+UCfmbe6yM+WlmuzptekFQAAAAAAABjUhPlAn7mq26n8SQ3J0dvVphcAAAAAAAAYzIT5QJ/oKIpNwvy3T0tG1hmxDwAAAAAAAFtLmA/0iTuWJn9eU66dYcQ+AAAAAAAAbBNhPtAnrux2Kn/PscmrmmvTCwAAAAAAAAx2wnyg11raily3sFybNT2pVIzYBwAAAAAAgG0hzAd67fqFycr2jev6SnLqtNr1AwAAAAAAAIOdMB/ote4j9t88KZkx0ql8AAAAAAAA2FbCfKBXHltd5KdLy7VZ02vSCgAAAAAAAAwZwnygV67qdip/YkNyzHa16QUAAAAAAACGCmE+sM06imKTMP8d05KRdUbsAwAAAAAAQG8I84Ft9vOlyWNryrUzjNgHAAAAAACAXhPmA9vsym6n8l8+JnlVc216AQAAAAAAgKFEmA9sk5a2InMWlmtnzEgqFSP2AQAAAAAAoLeE+cA2uX5hsrJ947ouyanTatYOAAAAAAAADCnCfGCbdB+x/+ZJyYyRTuUDAAAAAABAXxDmA1vtz6uL/HRpuTZrRk1aAQAAAAAAgCFJmA9stau6ncqf2JAcM7k2vQAAAAAAAMBQJMwHtkpHUWwyYv/t05JR9UbsAwAAAAAAQF8R5gNb5c5lyWNryrUzptemFwAAAAAAABiqhPnAVpn9l/L65WOS/Ztr0wsAAAAAAAAMVcJ8oMda2orMWViuzZqRVCpG7AMAAAAAAEBfEuYDPfbdRcnK9o3ruiSnTatZOwAAAAAAADBkCfOBHruy24j9N09KZox0Kh8AAAAAAAD6mjAf6JE/ry7yk6Xl2ukzatIKAAAAAAAADHnCfKBHrnq2vJ7QkBw7uTa9AAAAAAAAwFAnzAdeUFEUm4T5b5+ajKo3Yh8AAAAAAAD6gzAfeEF3LkvmrSnXzjBiHwAAAAAAAPqNMB94QbO7ncp/2ZjkgOba9AIAAAAAAADDgTAfeF4r24vMWVCuzZqeVCpG7AMAAAAAAEB/EeYDz+u7C5OW9o3ruiSnTa9ZOwAAAAAAADAsCPOB53XlX8rrwycl2490Kh8AAAAAAAD6kzAf2KLH1xS5fWm5NmtGTVoBAAAAAACAYUWYD2zRVc+W1xMakmMn16YXAAAAAAAAGE6E+cBmFUWxyYj9k6cmo+qN2AcAAAAAAID+JswHNuvOZcm8NeXaGdNr0wsAAAAAAAAMN8J8YLOu7DZi/6VjkgPH1aYXAAAAAAAAGG6E+cAmVrYXmbOgXJs1PalUjNgHAAAAAACAahDmA5v43sJkRfvGdV2S04zYBwAAAAAAgKoR5gOb6D5i/7BJyQ4jncoHAAAAAACAahHmAyWPryly+5JybZZT+QAAAAAAAFBVwnyg5Opnk6LLenxD8pbtatYOAAAAAAAADEvCfKBTURSbjNh/+9RkVL0R+wAAAAAAAFBNwnyg038vSx5dXa6dYcQ+AAAAAAAAVJ0wH+g0u9up/JeOSQ4cV5teAAAAAAAAYDgT5gNJkpXtReYsKNdOn55UKkbsAwAAAAAAQLUJ84EkyfcWJivaN67rkrzTiH0AAAAAAACoCWE+kCS5qtuI/TdNSnYY6VQ+AAAAAAAA1IIwH8gTa4rctqRcm+VUPgAAAAAAANSMMB/I1c8mRZf1+IbkLdvVrB0AAAAAAAAY9oT5MMwVRZEru43YP3lqMrreiH0AAAAAAACoFWE+DHN3LUseWV2unWHEPgAAAAAAANSUMB+GudndTuXvMSb5q3G16QUAAAAAAABYT5gPw9iq9iLfWVCuzZqeVCpG7AMAAAAAAEAtCfNhGPvewmRF+8Z1XZJ3GrEPAAAAAAAANSfMh2Hsym4j9g+dmOww0ql8AAAAAAAAqDVhPgxTT64pctuScm3WjNr0AgAAAAAAAJQJ82GYuvrZpOiyHlefHLddzdoBAAAAAAAAuhDmwzBUFMUmI/ZPnpaMrjdiHwAAAAAAAAYCYT4MQ3cvT/60ulw7Y3ptegEAAAAAAAA2JcyHYWj2X8rr3Ucnfz2uNr0AAAAAAAAAmxLmwzCzqr3IdxaUa7NmJJWKEfsAAAAAAAAwUAjzYZi5YVGyvH3jupLkndNq1g4AAAAAAACwGcJ8GGau7DZi/00Tkx1HOZUPAAAAAAAAA4kwH4aRJ9cUuXVJuTZrRm16AQAAAAAAALZMmA/DyNXPJkWX9bj65LjtatYOAAAAAAAAsAXCfBgmiqLIlc+WaydNTUbXG7EPAAAAAAAAA40wH4aJu5cnf1pdrp1hxD4AAAAAAAAMSMJ8GCa6n8rfbXTy6nG16QUAAAAAAAB4fsJ8GAZWtxe5dn65Nmt6UqkYsQ8AAAAAAAADkTAfhoEbFiXL2zeuK0neOb1m7QAAAAAAAAAvQJgPw8CVfymvD52Y7DTKqXwAAAAAAAAYqIT5MMQ9tabIfy0p12bNqE0vAAAAAAAAQM8I82GIu3p+UnRZj6tPjtuuZu0AAAAAAAAAPSDMhyGsKIpNRuyfNDUZU2/EPgAAAAAAAAxkwnwYwn6xPPnj6nLtDCP2AQAAAAAAYMAT5sMQNvvZ8nq30cmrx9WmFwAAAAAAAKDnhPkwRK1uL3Lt/HLt9OlJpWLEPgAAAAAAAAx0wnwYouYuSpa3b1xXkrxzes3aAQAAAAAAALaCMB+GqCu7jdh/48TkRaOcygcAAAAAAIDBQJgPQ9DTa4v81+JybZZT+QAAAAAAADBoCPNhCLr62aSjy7q5Pjl+Ss3aAQAAAAAAALaSMB+GmKIoNhmxf9LUZEy9EfsAAAAAAAAwWAjzYYj55fLk4VXl2hlG7AMAAAAAAMCgIsyHIWZ2t1P5u45ODhpfm14AAAAAAACAbSPMhyFkdXuRaxeUa7OmJ5WKEfsAAAAAAAAwmAjzYQiZuyhZ1rZxXUnyTiP2AQAAAAAAYNAR5sMQcmW3EfuHTExeNMqpfAAAAAAAABhshPkwRDy9tsh/LS7XZjmVDwAAAAAAAIOSMB+GiG8+m3R0WTfXJ8dPqVk7AAAAAAAAQC8I82EIKIpikxH7J05NxtYbsQ8AAAAAAACDkTAfhoBfLU/+sKpcO8OIfQAAAAAAABi0hPkwBMzudip/5ujkNeNr0wsAAAAAAADQe8J8GOTWFpV8e0G5Nmt6UqkYsQ8AAAAAAACDlTAfBrmfrhuXZW0b15UkpxuxDwAAAAAAAIOaMB8GuRvXTiytD5mYvGiUU/kAAAAAAAAwmAnzYRBb0NGYu1ubSrVZTuUDAAAAAADAoCfMh0HsltZJ6cjGU/hN9cnxU2rYEAAAAAAAANAnhPkwSBVF8v3WyaXaiVOTsfVG7AMAAAAAAMBg11DrBga7jo6O3HfffXniiSeyaNGijBs3LjNmzMgBBxyQMWPGVL2fBQsW5IEHHsjChQuzdOnSjBo1KtOnT89uu+2WmTNnplIR9A4Vv2sbnT93jCrVzjBiHwAAAAAAAIYEYf42am9vzxVXXJGrr746CxYs2OTnY8aMyVFHHZULLrgg48eP7/d+br311syePTv33ntvOjo6NnvNhAkTcvDBB+fTn/60UH8IuHHtxNJ6l1HJa/v/Vw0AAAAAAACoAmP2t8Hy5ctz2mmn5TOf+cxmg/wkWbVqVebMmZNjjz02Dz30UL/1smzZsrzvfe/Le9/73txzzz1bDPKTZOnSpbnpppvS3t7eb/1QHWvai/xw3YRSbdaM+JAGAAAAAAAADBFO5m+ltra2/P3f/33uu+++ztr222+fY489NjvssEMWL16cW2+9Nb/97W+TJM8++2zOOeeczJkzJ9OmTevTXlasWJF3vetdnc9KkkmTJuX1r399dt1110yYMCGrV6/O448/nt/85jd54IEHUhRFn/ZAbdz4XLKiqC/VTjdiHwAAAAAAAIYMYf5W+sY3vpG77rqrc3300Ufnk5/8ZEaMGNFZO+ecc3LVVVflX//1X1MURebPn5+LLrooX/va1/qsj6Io8r73va8zyG9oaMj73ve+vOtd7yr10tWCBQvyne98J3V1BjIMdjcuKq8PmZDsPMqpfAAAAAAAABgqpLpboaWlJV//+tc71y9/+ctzySWXbDY8P/3003Pqqad2rn/2s5/l3nvv7bNe5syZk1/84hdJkrq6unz605/Oueeeu8UgP0mmTp2a973vfcL8IaC127cpnDGjNn0AAAAAAAAA/UOquxXmzp2bpUuXdq4vuOCCNDRsebjB+eefn9GjR3eur7rqqj7pY+XKlfn0pz/duT7hhBNy5JFH9sneDA4fe3EypdKaJDlsxNKc0rff4AAAAAAAAADUmDB/K9x2222dr3fYYYe8+tWvft7rm5ubc/jhh3euf/7zn2fdunW97uMHP/hBli9fniSpr6/Peeed1+s9GVxe0VTJLRP/kB83PZBPNT+ZuooR+wAAAAAAADCUCPN7aM2aNfnVr37VuT7ooINS6UGAetBBB3W+XrlyZZ+M2r/++us7Xx944IGZOnVqr/dk8GmoJBPr2mrdBgAAAAAAANAPhPk9NG/evLS2tnau99577x7dt++++5bWDz/8cK/6WLVqVR544IHO9QEHHNCr/QAAAAAAAAAYeLb8he+UPProo6X1zjvv3KP7dthhh9TX16e9vT3J+g8F9MaDDz7YuVeS7LHHHkmSpUuX5rvf/W5++MMf5oknnsjKlSszadKk7Lrrrvmbv/mbvO1tb0tTU1Ovng0AAAAAAABAdQjze+ipp54qrWfMmNGj++rr6zNlypQ8++yzSZInn3yyV3384Q9/KK2nTp2aO+64IxdeeGEWLVpU+tmzzz6bZ599NnfeeWe+/OUv5+Mf/3iOPPLIXj0fAAAAAAAAgP5nzH4PtbS0lNbjx4/v8b3jxo3rfL1y5cpe9bFkyZLS+je/+U3OPffcziC/vr4+U6dOzcSJEze574Mf/GCuueaaXj0fAAAAAAAAgP7nZH4PrVq1qrQeOXJkj+8dNWrUFvfZWsuXLy+tL7nkkrS1tWXs2LF5//vfn+OPP77zgwbPPPNMrrzyylx55ZUpiiJFUeRf//Vfs+eee2afffbpVR+99cgjj6SuzmdJeqO1tbXzrw888ECNuwEYWrzHAvQf77EA/cv7LED/8R4L0H+GwntsR0dHn+8pzO+htWvXltaNjY09vnfEiBGdr9esWdOrPlavXl1at7a2ZtSoUZk9e3Ze+cpXln62/fbb58ILL8zMmTNz0UUXJUna2tpy6aWX5pvf/Gav+uit9vb2tLe317SHoWTDGxwAfc97LED/8R4L0L+8zwL0H++xAP3He+xGwvwe6n4Sv7W1tcen89etW9f5uusp/b7oI0nOOeecTYL8rk466aTceuut+dnPfpYkueeee/LHP/4xu+++e6966Y36+non83up6xvZ1ny4BIAX5j0WoP94jwXoX95nAfqP91iA/jMU3mM7Ojr6/DCzML+HxowZU1qvXbu2x2F+19P43ffpbR/19fV5+9vf/oL3nXbaaZ1hfpL84he/qGmYv+uuu6apqalmzx8KHnjggbS2tqaxsfF5P8wBwNbzHgvQf7zHAvQv77MA/cd7LED/GQrvsS0tLXn44Yf7dE9Ho3uoe/C8bNmyHt+7YsWKztdjx47t0z523XXXTJw48QXve9WrXlU6Cf/73/++V30AAAAAAAAA0H+E+T204447ltZ/+ctfenRfe3t7FixY0Lneaaed+rSP7bffvkf3jR07NuPGjetcL1mypFd9AAAAAAAAANB/hPk9tMsuu5TWTzzxRI/ue/rpp0vfjdB9n6216667ltYjRozo8b1dr+36vRMAAAAAAAAADCzC/B7aZZdd0tjY2Ln+9a9/3aP77r///tK6t99Tv8suu5RC+a0Z9798+fLO1+PHj+9VHwAAAAAAAAD0H2F+D40ePToHHHBA5/ruu+9OURQveN9dd93V+XrMmDHZf//9e9XHiBEj8upXv7pz/fDDD/fovscffzxr1qzpXHcf1w8AAAAAAADAwCHM3wqHHnpo5+unnnoqd9999/Nev2LFivzoRz/qXB988MFbNRZ/S970pjd1vl6yZEl+9atfveA9XftIkgMPPLDXfQAAAAAAAADQP4T5W+HYY48tjae/9NJL09bWtsXrP/e5z2X16tWd69NPP32L1x5yyCHZY489sscee+SQQw553j6OOuqoTJkypXN92WWXpaOjY4vXL168OP/xH//RuZ4+fbowHwAAAAAAAGAAE+Zvhebm5px99tmd6wcffDAf/ehH09rausm1V199da655prO9cEHH9zrEfsbjBkzJu95z3s61/fff38+/OEPlz44sMH8+fNz9tlnZ8mSJZ21d7/73X0yIQAAAAAAAACA/tFQ6wYGmzPPPDN33nlnfvnLXyZJbrrpptx333055phjsuOOO2bx4sW59dZb88ADD3TeM2XKlFx88cV92sfb3/723H333fnxj3/c2cevfvWrHHXUUXnJS16S1tbWPPTQQ/nBD36QVatWdd536KGH5h3veEef9gIAAAAAAABA3xLmb6XGxsZ88YtfzLvf/e7cf//9SZKnn346X/nKVzZ7/dSpU/PlL38506dP79M+6urq8ulPfzrr1q3LT3/60yTrT+F3Haff3RFHHJF/+7d/S6VS6dNeAAAAAAAAAOhbxuxvg/Hjx+eaa67JBz7wgdJ313c1ZsyYnHDCCbnpppuy11579Usfo0aNyle/+tVcfPHFefGLX7zF62bOnJnPfOYz+exnP5tRo0b1Sy8AAAAAAAAA9B0n87dRfX19zjnnnPzt3/5t7rvvvjz++ON57rnnMm7cuMyYMSMHHnhgxowZ0+P9br/99m3u5cQTT8yJJ56YBx98MI888kgWLFiQ+vr6TJo0Kfvss8/zBv0AAAAAAAAADDzC/F6qr6/PAQcckAMOOKDWrWTPPffMnnvuWes2AAAAAAAAAOglY/YBAAAAAAAAYIAR5gMAAAAAAADAACPMBwAAAAAAAIABRpgPAAAAAAAAAAOMMB8AAAAAAAAABhhhPgAAAAAAAAAMMMJ8AAAAAAAAABhghPkAAAAAAAAAMMAI8wEAAAAAAABggBHmAwAAAAAAAMAAI8wHAAAAAAAAgAFGmA8AAAAAAAAAA4wwHwAAAAAAAAAGGGE+AAAAAAAAAAwwwnwAAAAAAAAAGGCE+QAAAAAAAAAwwAjzAQAAAAAAAGCAEeYDAAAAAAAAwAAjzAcAAAAAAACAAabqYf69995b7UcCAAAAAAAAwKBS9TD/1FNPzVFHHZVvfOMbWbx4cbUfDwAAAAAAAAADXk3G7M+bNy+f+tSn8rrXvS7nn39+7rzzzlq0AQAAAAAAAAADUkMtH97a2pof/ehH+dGPfpQZM2bkhBNOyNve9rZMmzatlm0BAAAAAAAAQE1V/WT+rFmzMmHChBRF0VkriiLPPPNMvvjFL+aQQw7J3/3d3+XWW29Ne3t7tdsDAAAAAAAAgJqreph/4YUX5o477shll12W17zmNalUKknS+df29vb8/Oc/z3nnnZfXve51+cxnPpPHH3+82m0CAAAAAAAAQM1UPcxPksbGxhx55JG54oorcuutt+bcc8/N9OnTNzmtv2jRonz961/Pm9/85rzzne/MTTfdlHXr1tWiZQAAAAAAAACompqE+V1tv/32+fu///vcfvvt+drXvpY3velNqa+vT7LxtH5RFPmf//mffPjDH87BBx+ciy++OH/4wx9q2TYAAAAAAAAA9Juah/kbVCqV/M3f/E2++MUv5o477siHPvShvPjFL97ktP6yZctyzTXX5Pjjj88JJ5yQ73znO1m5cmUNOwcAAAAAAACAvjVgwvyuJk2alLPPPju33HJLvvnNb+a4447LqFGjOn9eFEWKosjvfve7/NM//VNe+9rX5h//8R9z//3317BrAAAAAAAAAOgbAzLM72r//ffPv/3bv+XnP/95/umf/il77rlnkvII/tWrV+e73/1uTjnllBx99NG55ppr0tLSUsu2AQAAAAAAAGCbDfgwf4OmpqYcd9xxecc73pEZM2akKIpUKpXO/yXrg/1HHnkkF198cQ455JB86Utfytq1a2vcOQAAAAAAAABsnYZaN9ATDzzwQObMmZMf/OAHWbVqVZLyyfyuKpVKiqLI8uXLc/nll+fGG2/MF7/4xey+++5V7xsAAAAAAAAAtsWADfOXLVuWG264Idddd10eeeSRJJsG96NGjcqb3/zmnHzyyWlubs7111+fuXPnZvHixZ2h/uOPP54zzjgjN954Y7bbbrta/FEAAAAAAAAAYKsMuDD/rrvuypw5c3LbbbeltbW1M8DfcBI/SXbbbbecdNJJOe6449Lc3NxZ/8hHPpIPfvCDmTt3bi6//PI8++yzSZIlS5bkiiuuyEc+8pHq/mEAAAAAAAAAYBsMiDB//vz5ue666/Ld7343zzzzTJL1p/ArlUrnCfsRI0Z0nsLfb7/9trhXY2NjTjjhhBx22GE59dRT86c//SlFUeRnP/uZMB8AAAAAAACAQaFmYX57e3tuu+22zJkzJ3fddVc6Ojo2OYVfFEV23XXXzlP448aN6/H+48aNy7nnnpsPfvCDSZKnn3667/8QAAAAAAAAANAPqh7mz5s3L3PmzMmNN96YxYsXJ9n8KfzDDz88J598cl71qldt87P22GOPztfr1q3rde8AAAAAAAAAUA1VD/OPPPLIztA+KZ/CnzlzZucp/PHjx/f6WaNGjer1HgAAAAAAAABQbTUbs9/1FP5hhx2Wk08+Ofvvv3+fPqOhoSHbb799n+4JAAAAAAAAAP2tJmF+URTZZZddctJJJ+X444/vk1P4mzNt2rTcfvvt/bI3AAAAAAAAAPSXqof5Rx99dN7+9rf3+Sl8AAAAAAAAABgqqh7mX3rppdV+JAAAAAAAAAAMKnW1bgAAAAAAAAAAKBPmAwAAAAAAAMAAU/Ux+88++2y+8Y1vdK7f/e53Z9KkSVu1x3PPPZevfe1rneu//du/zXbbbddnPQIAAAAAAABALVU9zP/Wt76VK6+8MpVKJa94xSu2OshPksmTJ+e+++7L7373uyTJuHHj8t73vrevWwUAAAAAAACAmqj6mP0f/vCHna9PPvnkbd7n5JNPTlEUKYoiN998c1+0BgAAAAAAAAADQlXD/GeeeSaPP/54kqRSqeRNb3rTNu/1pje9KXV169t/7LHHMn/+/D7pEQAAAAAAAABqraph/h/+8Ick64P8F7/4xRk3btw27zV+/Pi8+MUv3mRvAAAAAAAAABjsqhrmP/30052vd955517v13WPp556qtf7AQAAAAAAAMBAUNUwf+XKlZ2vm5qaer1f1z267g0AAAAAAAAAg1lVw/zRo0d3vl6xYkWv92tpael83dDQ0Ov9AAAAAAAAAGAgqGqYP2nSpM7XTzzxRK/367pH170BAAAAAAAAYDCrapi/4Tvui6LIY489lqeffnqb93r66afz6KOPdq532GGHXvcHAAAAAAAAAANBVcP8vfbaK83NzalUKkmSr3zlK9u811e/+tXO16NHj86+++7b6/4AAAAAAAAAYCCoaphfV1eXN77xjSmKIkVR5Prrr88PfvCDrd7nBz/4QebMmZNKpZJKpZI3vOENaWho6IeOAQAAAAAAAKD6qhrmJ8l73vOeNDQ0pFKppKOjIx/+8IfzpS99KW1tbS94b3t7e7785S/nwx/+cJL14/rr6urynve8p7/bBgAAAAAAAICqqfpx9he96EU5++yz85WvfCWVSiVtbW25/PLL861vfSvHHXdc9t9//8ycObNzHP/y5cszb968/M///E9uuOGGLFq0KEVRdJ7KP+usszJz5sxq/zEAAAAAAAAAoN/UZDb9+eefn3nz5uXHP/5xKpVKiqLIokWLcsUVV+SKK67Y4n1FUSRJ5z2HH354/s//+T/VahsAAAAAAAAAqqLqY/Y3+NznPpd3v/vdnetKpZJkfWC/uf91vSZJzjnnnHz2s5+tbtMAAAAAAAAAUAU1C/Pr6urygQ98INdee23e+MY3Jtl48n5zNozWP+ywwzJnzpycf/75qaurWfsAAAAAAAAA0G9qMma/q1e+8pX50pe+lMWLF+dXv/pVfvOb32TRokVZunRpkmT8+PGZMmVK9tlnnxxwwAGZNGlSbRsGAAAAAAAAgH5W8zB/g0mTJuXNb35z3vzmN9e6FQAAAAAAAACoKXPqAQAAAAAAAGCAEeYDAAAAAAAAwAAjzAcAAAAAAACAAUaYDwAAAAAAAAADTEOtG9hg8eLFmTdvXpYtW5aWlpYURbFV9x933HH90xgAAAAAAAAAVFlNw/xnn30211xzTX7wgx/kmWee6dVewnwAAAAAAAAAhoqahfnXXnttPvnJT2bt2rVbfQp/g0qlkqIoUqlU+rg7AAAAAAAAAKidmoT53/jGN/KpT31qs0F813X3kL/7z7b1QwAAAAAAAAAAMJBVPcx/6KGHcumllybZeLL+sMMOyyGHHJL6+vpccMEFnT+76qqrsnLlyixatCi//vWvc+utt2bZsmWpVCqZNGlSPvzhD2f77bev9h8BAAAAAAAAAPpV1cP8r3zlK2lvb1//8IaGXHbZZTnssMOSJE8//XTp2gMPPLDz9YknnpiLLrooX//61/OVr3wlS5Ysyac+9alcccUVednLXla9PwAAAAAAAAAA9LO6aj5szZo1uf3221OpVFKpVHLWWWd1Bvk9MWrUqLzvfe/LF7/4xdTX12fx4sX5u7/7uyxZsqQfuwYAAAAAAACA6qpqmP/rX/86bW1tKYoi9fX1mTVr1jbt84Y3vCFnn312kmTRokX50pe+1JdtAgAAAAAAAEBNVTXMf+qpp5IklUolM2fOzOTJk5/3+ra2ti3+7Oyzz05DQ0OKosj3v//9ztH9AAAAAAAAADDYVTXMX7ZsWefrnXfeeZOfNzQ0lNbr1q3b4l5NTU3Ze++9O/e99957+6hLAAAAAAAAAKitqob5XU/Pjxo1apOfjx07trR+7rnnnne/adOmdb5+5plnetkdAAAAAAAAAAwMVQ3zu4b1q1at2uzP6+vrO9cvFNB3/XDAokWL+qBDAAAAAAAAAKi9qob5O+ywQ+frzZ26r1QqpfH7v/nNb553vz/96U+dr7uP6AcAAAAAAACAwaqqYf7MmTOTJEVRlIL4rl7+8pd3vr7pppu2uNe9996befPmda67jtwHAAAAAAAAgMGsqmH+TjvtlKlTpyZJVq5cmT/+8Y+bXHP44Yd3vn7kkUdy6aWXbnLNE088kQ9/+MOpVCpJ1p/o33///fupawAAAAAAAACorqrPpj/ooINyww03JEl+8pOfZPfddy/9/HWve1122GGHPPPMMymKIldccUVuu+22vOY1r8nYsWPz5z//OT/96U+zbt26FEWRSqWS173udZkyZUq1/ygAAAAAAAAA0C+qejI/SY444ogk60ftX3fddZv8fMSIEbnooouSrD9xXxRFHnvssVxzzTX52te+lh//+MdZu3Zt5/VNTU258MILq9M8AAAAAAAAAFRB1U/mv+Y1r8l73vOedHR0JEnmz5+/yffdv/71r8//9//9f/m///f/prW1tXOc/gYbQv4JEybk8ssvz4te9KKq9Q8AAAAAAAAA/a3qYX5DQ0Pe//73v+B1J5xwQg444IB87Wtfy89+9rMsWrSo82c77bRTDj/88Jx11lmZNGlSf7YLAAAAAAAAAFVX9TB/a+y88875l3/5lyTJ6tWrs2LFiowbNy6jRo2qcWcAAAAAAAAA0H8GdJjf1ejRozN69OhatwEAAAAAAAAA/a6qYf6f//zn3HHHHZ3rI488Mtttt101WwAAAAAAAACAAa+qYf4dd9yRT37yk0mSCRMm5JRTTqnm4wEAAAAAAABgUKir5sPWrFmToiiSJC9/+cvT0DBopvwDAAAAAAAAQNVUNcyfNGlS5+uJEydW89EAAAAAAAAAMGhUNcyfNm1a5+tly5ZV89EAAAAAAAAAMGhUNcx/1ateldGjR6coivzud7/rHLkPAAAAAAAAAGxU1TB/zJgxeeMb35gkWbp0aX784x9X8/EAAAAAAAAAMChUNcxPkgsuuCATJkxIkvzLv/xLnnnmmWq3AAAAAAAAAAADWtXD/GnTpuWyyy7L2LFjs2DBgrz97W/PrbfeWu02AAAAAAAAAGDAaqj2A++55540NjbmIx/5SD75yU9mwYIFOe+887LTTjvl9a9/fV72spdl0qRJGTNmzFbte8ABB/RTxwAAAAAAAABQXVUP89/5znemUql0riuVSoqiyBNPPJGrr756m/asVCp56KGH+qpFAAAAAAAAAKipqof5GxRF0Rnqdw33i6KoVUsAAAAAAAAAMCDUJMzfENgL7gEAAAAAAABgU1UP8z/5yU9W+5EAAAAAAAAAMKhUPcw//vjjq/1IAAAAAAAAABhU6mrdAAAAAAAAAABQJswHAAAAAAAAgAFGmA8AAAAAAAAAA4wwHwAAAAAAAAAGGGE+AAAAAAAAAAwwDdV+4A033NAv+x533HH9si8AAAAAAAAAVFvVw/yPfvSjqVQqfb6vMB8AAAAAAACAoaLqYf4GRVH0eo9KpZKiKPrlwwEAAAAAAAAAUCt1tXhob4L8SqXSGd73xQcCAAAAAAAAAGCgqfrJ/Kuuumqrru/o6MiKFSvyyCOP5M4778y9996bJBk/fnw++tGPZocdduiPNgEAAAAAAACgZqoe5h944IHbdN+b3vSmnHvuubn33nvzkY98JE899VQ+/elP5z/+4z/y0pe+tI+7BAAAAAAAAIDaqcmY/d541atelWuuuSYzZszI4sWL83d/93dZvHhxrdsCAAAAAAAAgD4z6ML8JJk2bVouvPDCJMnChQvzhS98ocYdAQAAAAAAAEDfGZRhfrJ+7P6kSZNSFEVuuummrF69utYtAQAAAAAAAECfGLRhfqVSyV577ZUkWbVqVX71q1/VuCMAAAAAAAAA6BuDNsxPknHjxnW+/stf/lLDTgAAAAAAAACg7wzqMH/ZsmWdr5cvX17DTgAAAAAAAACg7wzaMH/t2rW5//77O9cTJkyoXTMAAAAAAAAA0IcGbZj/uc99Li0tLZ3rmTNn1rAbAAAAAAAAAOg7DbVuYGs98cQT+fd///fMnTs3lUolRVFk4sSJ2XfffWvdGgAAAAAAAAD0iaqH+RdeeOFW39Pe3p7ly5fnscceyxNPPJEkKYoiSVKpVHLuueemrm7QDhkAAAAAAAAAgJKqh/nf+973UqlUtunergH+hlP5RxxxRN75znf2ZYsAAAAAAAAAUFODasz+hgC/KIqMGjUq5557bs4+++xatwUAAAAAAAAAfaomYf6GE/Y9VV9fn6ampkycODEvfelL81d/9Vc56qijMm7cuH7qEAAAAAAAAABqp+ph/h/+8IdqPxIAAAAAAAAABpW6WjcAAAAAAAAAAJQJ8wEAAAAAAABggBHmAwAAAAAAAMAAI8wHAAAAAAAAgAGmodoPbGtryyOPPNK53nnnnTN69Oit2mPVqlV54oknOte777576up8LgEAAAAAAACAoaHqYf73v//9XHjhhUmSCRMm5Cc/+clW71GpVHLGGWdk2bJlSZLLLrssRxxxRJ/2CQAAAAAAAAC1UvXj7N/97ndTFEWS5KSTTsqoUaO2eo/Ro0fn5JNPTlEUKYoi1113XV+3CQAAAAAAAAA1U9Uwf+XKlbnvvvs610cfffQ279X13nvuuSdr1qzpVW8AAAAAAAAAMFBUNcz//e9/n7a2tiTJpEmTsttuu23zXrvttlsmTZqUJGltbc1DDz3UJz0CAAAAAAAAQK1VNcx/7LHHkqz/zvs99tij1/t13WPD3gAAAAAAAAAw2FU1zF+6dGnn64kTJ/Z6vw0n85Nk2bJlvd4PAAAAAAAAAAaCqob5XW0Yt98b7e3tna9bW1t7vR8AAAAAAAAADARVDfO7nsZfuHBhr/fruseECRN6vR8AAAAAAAAADARVDfOnTJmSJCmKIg8++GDWrl27zXutWbMmv/3tbzvXkydP7nV/AAAAAAAAADAQVDXM32+//VJfX59KpZJ169Zl7ty527zXjTfemHXr1iVJKpVK9ttvv75qEwAAAAAAAABqqqphfnNzc17xilekKIoURZEvfOELmT9//lbvM3/+/HzhC19IpVJJpVLJy1/+8kyaNKkfOgYAAAAAAACA6qtqmJ8kZ511VpL1p+kXLVqUs846K4899liP73/88cfzrne9K4sWLUpRFEmSM888s196BQAAAAAAAIBaqHqYf9hhh2WfffZJURSpVCp59NFH89a3vjWXXHJJHn300S3eN2/evFxyySU57rjj8uijj3aeyt9rr71y1FFHVfFPAAAAAAAAAAD9q6EWD/385z+fE044IYsWLUqlUsnq1asze/bszJ49OxMmTMguu+yS5ubmVCqVrFixIvPmzcuSJUuSpPNDAEVRZNq0abn88str8UcAAAAAAAAAgH5TkzB/2rRpmT17dt773vfmz3/+cyqVSpL1Qf2SJUty3333la7fME5/w2n8oijykpe8JJdffnmmTZtW9f4BAAAAAAAAoD9Vfcz+BjNnzsz111+fU045JSNGjCgF9t11DftHjBiR0047Lddff31mzpxZ1Z4BAAAAAAAAoBpqcjJ/g7Fjx+bjH/943vve92bu3Ln55S9/md/85jdZunRp6brx48dn3333zV/91V/lLW95SyZNmlSbhgEAAAAAAACgCmoa5m8wefLknHXWWTnrrLOSJG1tbVm2bFmS9UF+Q8OAaBMAAAAAAAAAqmJApuQNDQ2ZPHlyrdsAAAAAAAAAgJqoq3UDAAAAAAAAAECZMB8AAAAAAAAABpiqj9lva2vLI4880rneeeedM3r06K3aY9WqVXniiSc617vvvnvq6nwuAQAAAAAAAIChoeph/ve///1ceOGFSZIJEybkJz/5yVbvUalUcsYZZ2TZsmVJkssuuyxHHHFEn/YJAAAAAAAAALVS9ePs3/3ud1MURZLkpJNOyqhRo7Z6j9GjR+fkk09OURQpiiLXXXddX7cJAAAAAAAAADVT1TB/5cqVue+++zrXRx999Dbv1fXee+65J2vWrOlVbwAAAAAAAAAwUFQ1zP/973+ftra2JMmkSZOy2267bfNeu+22WyZNmpQkaW1tzUMPPdQnPQIAAAAAAABArVU1zH/ssceSrP/O+z322KPX+3XdY8PeAAAAAAAAADDYVTXMX7p0aefriRMn9nq/DSfzk2TZsmW93g8AAAAAAAAABoKqhvldbRi33xvt7e2dr1tbW3u9HwAAAAAAAAAMBFUN87uexl+4cGGv9+u6x4QJE3q9HwAAAAAAAAAMBFUN86dMmZIkKYoiDz74YNauXbvNe61Zsya//e1vO9eTJ0/udX8AAAAAAAAAMBBUNczfb7/9Ul9fn0qlknXr1mXu3LnbvNeNN96YdevWJUkqlUr222+/vmoTAAAAAAAAAGqqqmF+c3NzXvGKV6QoihRFkS984QuZP3/+Vu8zf/78fOELX0ilUkmlUsnLX/7yTJo0qR86BgAAAAAAAIDqq2qYnyRnnXVWkvWn6RctWpSzzjorjz32WI/vf/zxx/Oud70rixYtSlEUSZIzzzyzX3oFAAAAAAAAgFqoeph/2GGHZZ999klRFKlUKnn00Ufz1re+NZdcckkeffTRLd43b968XHLJJTnuuOPy6KOPdp7K32uvvXLUUUdV8U8AAAAAAAAAAP2roRYP/fznP58TTjghixYtSqVSyerVqzN79uzMnj07EyZMyC677JLm5uZUKpWsWLEi8+bNy5IlS5Kk80MARVFk2rRpufzyy2vxRwAAAAAAAACAflOTMH/atGmZPXt23vve9+bPf/5zKpVKkvVB/ZIlS3LfffeVrt8wTn/DafyiKPKSl7wkl19+eaZNm1b1/gEAAAAAAACgP1V9zP4GM2fOzPXXX59TTjklI0aMKAX23XUN+0eMGJHTTjst119/fWbOnFnVngEAAAAAAACgGmpyMn+DsWPH5uMf/3je+973Zu7cufnlL3+Z3/zmN1m6dGnpuvHjx2fffffNX/3VX+Utb3lLJk2aVJuGAQAAAAAAAKAKahrmbzB58uScddZZOeuss5IkbW1tWbZsWZL1QX5Dw4BoEwAAAAAAAACqomZj9p9PQ0NDJk+enMmTJz9vkD9//vx87Wtfy5FHHlnF7gAAAAAAAACgfw26I+9r1qzJj3/848ydOze/+MUv0tHRUeuWAAAAAAAAAKBPDZow/5577sn3vve9/OhHP8qqVauSJEVRJEkqlUotWwMAAAAAAACAPjWgw/wnnngiN9xwQ2688cY8/fTTScoBfqVS6VwDAAAAAAAAwFAx4ML8lpaW3HLLLfne976X+++/P8nmA/yiKDJlypQcfvjhOfLII2vZMgAAAAAAAAD0qQER5hdFkZ///Oe54YYbcvvtt2ft2rWd9SSlAH+77bbLYYcdliOOOCL777+/EfsAAAAAAAAADDk1DfP/9Kc/5Xvf+15uuummLFq0KMmWx+gff/zxectb3pIDDzwwdXV1NesZAAAAAAAAAPpb1cP8xYsX5/vf/35uuOGG/P73v0+y5TH6XU/dn3feedl+++2r3S4AAAAAAAAAVF1Vwvy2trb85Cc/yfe+973ccccdaW9v32KAv/POO+eYY47Jsccem8MOO6wa7QEAAAAAAADAgNKvYf4DDzyQG264ITfffHOWL1+epHwKf0OAP3HixBx55JE59thjs/fee/dnSwAAAAAAAAAw4PV5mD9//vzMnTs3N9xwQx577LEk5QB/gxEjRuSQQw7Jsccem4MPPjgNDVWf+A8AAAAAAAAAA1KfJ+hveMMbOk/cb7DhFH6SHHjggXnLW96Sww8/PE1NTX39eAAAAAAAAAAY9Po8zO/o6EilUuk8hV8URXbdddcce+yxOeaYYzJ9+vS+fiQAAAAAAAAADCn9Ntu+KIpUKpW87nWvywUXXJBdd921vx4FAAAAAAAAAENKXX9tvOFk/h133JFjjjkmxx9/fGbPnp2FCxf21yMBAAAAAAAAYEjo8zD/r//6r1OpVFIURWetKIr8/ve/zyWXXJLXv/71Oeuss3LDDTdk1apVff14AAAAAAAAABj0+jzMnz17dm6//facf/752XnnnTtD/Q0n9dvb23P33XfnwgsvzGte85p88IMfzE9/+tO0t7f3dSsAAAAAAAAAMCj1y5j96dOn55xzzskPf/jDXHvttTn55JMzbty4TU7rr169OrfcckvOPffcHHzwwbn44ovzm9/8pj9aAgAAAAAAAIBBo6G/H7D33ntn7733zj/+4z/mtttuy9y5c3PnnXemra2t87R+URRZvHhxrrnmmlxzzTV50YtelGOOOaa/WwMAAAAAAACAAanfw/wNRowYkSOOOCJHHHFEnnvuudx444254YYb8vDDDydJKdh//PHH86UvfSmVSqXzNL8x/AAAAAAAAAAMF/0yZv+FTJ48OWeeeWbmzp2bG264IaeffnomTZrUGdxvCPY3vC6KIm95y1vywQ9+MLfeemvWrVtXi7YBAAAAAAAAoCpqEuZ39dKXvjT/8A//kDvuuCP//u//nsMOOywNDQ0piqIU7q9atSq33HJLzjvvvLz61a/Ohz70odx+++1pbW2t8Z8AAAAAAAAAAPpW1cbsv5D6+voccsghOeSQQ7Js2bJ8//vfzw033JDf/va3Scpj+FeuXJmbb745N998c5qamvLGN74x//Zv/1bL9gEAAAAAAACgz9T8ZP7mjB8/PqeeemrmzJmTm2++OWeffXamTp26yRj+oiiyYsWKzJ07t5btAgAAAAAAAECfGpBhflczZ87Mhz70ofz0pz/NFVdckaOOOiojR45MURSdoT4AAAAAAAAADCUDZsz+C6lUKnnNa16T17zmNWlpacktt9ySuXPn5t577611awAAAAAAAADQpwZNmN9VU1NTTjzxxJx44ol58sknjdkHAAAAAAAAYEgZ8GP2X8hOO+2U973vfbVuAwAAAAAAAAD6zKAP8wEAAAAAAABgqBHmAwAAAAAAAMAAI8wHAAAAAAAAgAFGmA8AAAAAAAAAA4wwHwAAAAAAAAAGGGE+AAAAAAAAAAwwwnwAAAAAAAAAGGCE+QAAAAAAAAAwwAjzAQAAAAAAAGCAEeYDAAAAAAAAwAAjzAcAAAAAAACAAUaYDwAAAAAAAAADjDAfAAAAAAAAAAYYYT4AAAAAAAAADDDCfAAAAAAAAAAYYIT5AAAAAAAAADDANNS6gcGuo6Mj9913X5544oksWrQo48aNy4wZM3LAAQdkzJgxtW4PAAAAAAAAgEFImL+N2tvbc8UVV+Tqq6/OggULNvn5mDFjctRRR+WCCy7I+PHjq97fZz/72XzlK18p1T75yU/mrW99a9V7AQAAAAAAAGDrGLO/DZYvX57TTjstn/nMZzYb5CfJqlWrMmfOnBx77LF56KGHqtrfn/70p1xxxRVVfSYAAAAAAAAAfcfJ/K3U1taWv//7v899993XWdt+++1z7LHHZocddsjixYtz66235re//W2S5Nlnn80555yTOXPmZNq0af3eX1EUueiii9La2trvzwIAAAAAAACgfziZv5W+8Y1v5K677upcH3300fnRj36UD3zgAznppJNyzjnn5Lrrrss//uM/plKpJEnmz5+fiy66qCr9ffvb387999+fJNlll12q8kwAAAAAAAAA+pYwfyu0tLTk61//euf65S9/eS655JKMGDFik2tPP/30nHrqqZ3rn/3sZ7n33nv7tb8FCxbkM5/5TJJkwoQJOf/88/v1eQAAAAAAAAD0D2H+Vpg7d26WLl3aub7gggvS0LDlbyo4//zzM3r06M71VVdd1Z/t5eKLL86KFSs6e5swYUK/Pg8AAAAAAACA/iHM3wq33XZb5+sddtghr371q5/3+ubm5hx++OGd65///OdZt25dv/T2k5/8JD/60Y+SJPvtt1/e9ra39ctzAAAAAAAAAOh/wvweWrNmTX71q191rg866KBUKpUXvO+ggw7qfL1y5cp+GbW/atWq/PM//3OSpKGhIZ/4xCd61BsAAAAAAAAAA5Mwv4fmzZuX1tbWzvXee+/do/v23Xff0vrhhx/u076S5POf/3yeeeaZJMnpp5+ePfbYo8+fAQAAAAAAAED1CPN76NFHHy2td9555x7dt8MOO6S+vr5zPW/evD7t63e/+12uvvrqJMmMGTNy3nnn9en+AAAAAAAAAFSfML+HnnrqqdJ6xowZPbqvvr4+U6ZM6Vw/+eSTfdZTe3t7Pv7xj6e9vT1J8rGPfSxjxozps/0BAAAAAAAAqA1hfg+1tLSU1uPHj+/xvePGjet8vXLlyj7r6aqrrsqDDz6YJHnDG96QQw89tM/2BgAAAAAAAKB2GmrdwGCxatWq0nrkyJE9vnfUqFFb3GdbPf300/nCF77Quf/HPvaxPtm3Wh555JHU1fksSW+0trZ2/vWBBx6ocTcAQ4v3WID+4z0WoH95nwXoP95jAfrPUHiP7ejo6PM9hfk9tHbt2tK6sbGxx/eOGDGi8/WaNWv6pJ9//ud/7vxgwHve857suOOOfbJvtbS3t3d+PQC9t+ENDoC+5z0WoP94jwXoX95nAfqP91iA/uM9diNhfg91P4nf2tra49P569at63zd9ZT+tvrBD36Qn/70p0mSXXfdNWf9/+3de5hWdbk//huGGWCAGQKGEQfF8ECeEFSkLLTUnTtNNLVs65YST1h4SlEr3R0vjMJLt4etaZ4gMtNStzv6mmiRB0IRFDIFz5wE5HyeGWbm94c/VjwwA8/ADHyA1+u6vHru5/msz7pBr7uB97PWGjx4m/fc3goKClyZv402HGSN+XIJAFtmxgI0HzMWoHmZswDNx4wFaD67woytra1t8ouZhfl5Ki4uzqkrKyvzDvM3vBp/430aa/ny5TF8+PCs/sEPfrBT/ge93377Rfv27Xd0Gzu1qVOnRnV1dRQWFkbv3r13dDsAuxQzFqD5mLEAzcucBWg+ZixA89kVZuzKlStj+vTpTbqnS6PztHHwvGzZsryPXbFiRfa6Xbt229THyJEj46OPPoqIiNNOOy2OOuqobdoPAAAAAAAAgPQI8/O08TPpP/zww7yOq6mpiQULFmT1XnvttdU9vPHGG/G73/0uIiJKS0vjmmuu2eq9AAAAAAAAAEiX2+znqWfPnjn1zJkz87oqfs6cOTnPRth4n8aYM2dO1NXVRcTHz434+te/vtn1G97eP+Ljq/rvvPPOrP71r38d5eXlW90PAAAAAAAAAM1DmJ+nnj17RmFhYVRXV0dExKuvvhpnnnnmFo+bMmVKTn3AAQc0ST+rV6+OmTNnNuqYRYsWxaJFi7J6/a8FAAAAAAAAgLS4zX6e2rZtG/369cvqCRMmZFfJb86LL76YvS4uLo4jjzyyWfoDAAAAAAAAYNfhyvxGOOGEE7Jwfvbs2TFhwoQ4+uijG1y/YsWKeOqpp7J6wIABUVRUtE3nnz59et7rJ06cGIMGDcrqG2+8MU4//fStPj8AAAAAAAAA24cr8xth4MCBUVpamtUjR46MdevWNbj+lltuiTVr1mT1hsH6xo477rjo1atX9OrVK4477rimaRgAAAAAAACAnZIwvxE6dOgQF1xwQVa//vrrcd1119X77PnRo0fHmDFjsnrAgAFusQ8AAAAAAABAXtxmv5HOO++8eP7552PixIkREfHkk0/G5MmT45RTTonu3bvH4sWLY9y4cTF16tTsmLKysvjpT3+6o1oGAAAAAAAAYCcjzG+kwsLCuO222+Liiy+OKVOmRETEnDlz4q677qp3fdeuXePOO++MPfbYY3u2CQAAAAAAAMBOzG32t0JpaWmMGTMmrrzyyigrK6t3TXFxcZx55pnx5JNPxiGHHLKdOwQAAAAAAABgZ+bK/K1UUFAQQ4YMiQsvvDAmT54cH3zwQSxatChKSkqiW7ducdRRR0VxcXHe+z377LNN3mP//v1j+vTpTb4vAAAAAAAAAM1LmL+NCgoKol+/ftGvX78d3QoAAAAAAAAAuwi32QcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABLTakc3sLOrra2NyZMnx8yZM2PhwoVRUlIS3bp1i379+kVxcXGzn3/t2rUxY8aMeOedd2Lx4sVRXV0dJSUlUVFREX379o2SkpJm7wEAAAAAAACApiXM30o1NTVx7733xujRo2PBggWbfF5cXBwnn3xyDBs2LEpLS5v03B9++GGMHTs2xo8fH5MnT47q6up617Vo0SIGDBgQF110UfTr169JewAAAAAAAACg+Qjzt8Ly5cvj4osvjsmTJze4ZvXq1fHII4/Ec889F3feeWccdNBBTXLu559/Pi644IKoq6vb4tq6urr429/+Fs8991wMGjQorrvuumjZ0pMVAAAAAAAAAFInzG+kdevWxeWXX54T5O+5554xcODAqKioiMWLF8e4ceNi2rRpERExb968GDJkSDzyyCNRXl6+zedfu3ZtTpBfWFgYhxxySBxxxBGxxx57RNu2bWP+/PnxwgsvxCuvvBIRH4f6Dz74YKxduzZ+/OMfb3MPAAAAAAAAADQvYX4j3X///fHiiy9m9Ze//OW48cYbo6ioKHtvyJAhMWrUqBg+fHjU1dXF/Pnz44Ybboi77767yfrYZ5994uyzz45TTz01OnbsuMnn3/72t+Nvf/tbXH311bFs2bKIiHj44YfjhBNOiGOOOabJ+gAAAAAAAACg6bnneiOsXLkyfvWrX2X1QQcdFCNGjMgJ8tcbNGhQnHPOOVk9fvz47Er5bdGpU6f46U9/GmPHjo1vfOMb9Qb56x1zzDFx2223RYsWLbL3mvILBQAAAAAAAAA0D2F+IzzxxBOxdOnSrB42bFi0atXwzQ2uuOKKaNu2bVaPGjVqm3s4/PDD46tf/WoUFBTktb5///4xYMCArJ48eXKsWLFim/sAAAAAAAAAoPkI8xvhmWeeyV5XVFTEZz7zmc2u79ChQ5x44olZ/dxzz0VVVVWz9deQ/v37Z69rampi7ty5270HAAAAAAAAAPInzM/T2rVr46WXXsrqo48+Ouf29Q05+uijs9erVq1qklvtN1a7du1y6jVr1mz3HgAAAAAAAADInzA/T++++25UV1dn9WGHHZbXcX379s2pp0+f3qR95WP27Nk5defOnbd7DwAAAAAAAADkT5ifp3feeSen7tGjR17HVVRU5Dzf/t13323SvvIxbty47HVZWVl07959u/cAAAAAAAAAQP6E+Xna+Or2bt265XVcQUFBlJWVZfWsWbOatK8t+ctf/hLvv/9+Vp944ol5PR4AAAAAAAAAgB1HmJ+nlStX5tSlpaV5H1tSUpK9XrVqVZP1tCUrV66Mn/zkJ1ndunXruOiii7bb+QEAAAAAAADYOq12dAM7i9WrV+fUrVu3zvvYNm3aNLhPc6mrq4vvfe97MWfOnOy9oUOHRnl5+XY5/5a8/fbb0bKl75Jsi+rq6ux/p06duoO7Adi1mLEAzceMBWhe5ixA8zFjAZrPrjBja2trm3xPYX6eKisrc+rCwsK8jy0qKsper127tsl62pzbb789nnrqqaw+6qij4oILLtgu585HTU1N1NTU7Og2dhnrBxwATc+MBWg+ZixA8zJnAZqPGQvQfMzYfxHm52njK/Grq6vzvjq/qqoqe73hVfrN5eGHH47bb789q/fee++4+eabk7oSvqCgIKl+dkYbDrLGfLkEgC0zYwGajxkL0LzMWYDmY8YCNJ9dYcbW1tY2+cXMwvw8FRcX59SVlZV5h/kbXo2/8T5NbezYsfHDH/4wq8vKyuK+++6LLl26NOt5G2u//faL9u3b7+g2dmpTp06N6urqKCwsjN69e+/odgB2KWYsQPMxYwGalzkL0HzMWIDmsyvM2JUrV8b06dObdE+XRudp4+B52bJleR+7YsWK7HW7du2arKeNjR8/Pq655prseQwdO3aM+++/P/baa69mOycAAAAAAAAATU+Yn6fu3bvn1B9++GFex9XU1MSCBQuyurmC9b///e9x6aWXZregaN++ffzqV7+K/fffv1nOBwAAAAAAAEDzEebnqWfPnjn1zJkz8zpuzpw5Oc9G2HifpjBlypS45JJLorKyMiIi2rZtG7/85S/j0EMPbfJzAQAAAAAAAND8hPl56tmzZxQWFmb1q6++mtdxU6ZMyakPOOCApmwr/vnPf8ZFF10Uq1evjoiIwsLCuP322+PII49s0vMAAAAAAAAAsP0I8/PUtm3b6NevX1ZPmDAh6urqtnjciy++mL0uLi5u0pD9nXfeifPPPz+WL18eERGtWrWKW265JT73uc812TkAAAAAAAAA2P6E+Y1wwgknZK9nz54dEyZM2Oz6FStWxFNPPZXVAwYMiKKioibpZdasWXHeeefF4sWLIyKiZcuWceONN+b0CAAAAAAAAMDOSZjfCAMHDozS0tKsHjlyZKxbt67B9bfcckusWbMmqwcNGtTg2uOOOy569eoVvXr1iuOOO26zfcyfPz/OO++8mD9/fvbej370oxg4cGA+vwwAAAAAAAAAEifMb4QOHTrEBRdckNWvv/56XHfddVFdXb3J2tGjR8eYMWOyesCAAU1yi/2lS5fG+eefH7Nmzcre++53vxtf+9rXtnlvAAAAAAAAANLQakc3sLM577zz4vnnn4+JEydGRMSTTz4ZkydPjlNOOSW6d+8eixcvjnHjxsXUqVOzY8rKyuKnP/1pk5x/zJgx8dZbb2V1QUFBjBkzJueLA1ty7rnnbvYuAQAAAAAAAADsWML8RiosLIzbbrstLr744pgyZUpERMyZMyfuuuuuetd37do17rzzzthjjz2a5Py1tbU5dU1NTcycObNReyxbtqxJegEAAAAAAACgebjN/lYoLS2NMWPGxJVXXhllZWX1rikuLo4zzzwznnzyyTjkkEO2c4cAAAAAAAAA7Mxcmb+VCgoKYsiQIXHhhRfG5MmT44MPPohFixZFSUlJdOvWLY466qgoLi7Oe79nn302r3WXXnppXHrppVvbNgAAAAAAAAA7AWH+NiooKIh+/fpFv379dnQrAAAAAAAAAOwi3GYfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAA9iUzPQAAMVVJREFUEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABIjzAcAAAAAAACAxAjzAQAAAAAAACAxwnwAAAAAAAAASIwwHwAAAAAAAAASI8wHAAAAAAAAgMQI8wEAAAAAAAAgMcJ8AAAAAAAAAEiMMB8AAAAAAAAAEiPMBwAAAAAAAIDECPMBAAAAAAAAIDHCfAAAAAAAAABIjDAfAAAAAAAAABLTakc3sLOrra2NyZMnx8yZM2PhwoVRUlIS3bp1i379+kVxcfF266OqqiomTZoUc+bMicWLF0enTp2ioqIijjzyyCgqKtpufQAAAAAAAACw7YT5W6mmpibuvffeGD16dCxYsGCTz4uLi+Pkk0+OYcOGRWlpabP1sXbt2rj11lvj97//fSxdunSTzzt27BhnnHFGXHbZZdGmTZtm6wMAAAAAAACApuM2+1th+fLl8Z//+Z9x00031RvkR0SsXr06HnnkkRg4cGD885//bJY+5syZE2eccUbce++99Qb5ERFLly6Ne++9N84444yYM2dOs/QBAAAAAAAAQNNyZX4jrVu3Li6//PKYPHly9t6ee+4ZAwcOjIqKili8eHGMGzcupk2bFhER8+bNiyFDhsQjjzwS5eXlTdbHypUrY8iQIfH2229n7+27775x0kknRXl5ecybNy/Gjh0b7777bkREvP322zFkyJB46KGHon379k3WBwAAAAAAAABNT5jfSPfff3+8+OKLWf3lL385brzxxpzn0g8ZMiRGjRoVw4cPj7q6upg/f37ccMMNcffddzdZHyNHjowZM2Zk9fnnnx/Dhg2LFi1aZO8NHTo0fv7zn8d9990XEREzZsyIm266KX7wgx80WR8AAAAAAAAAND232W+ElStXxq9+9ausPuigg2LEiBE5Qf56gwYNinPOOSerx48fH6+88kqT9DFr1qx49NFHs/oLX/hCXHPNNTlBfkREixYt4tprr40vfOEL2XuPPPJIzJo1q0n6AAAAAAAAAKB5CPMb4Yknnsh5Nv2wYcOiVauGb25wxRVXRNu2bbN61KhRTdLHQw89FNXV1RHxcWB/3XXXbXb9hp9XV1fHQw891CR9AAAAAAAAANA8hPmN8Mwzz2SvKyoq4jOf+cxm13fo0CFOPPHErH7uueeiqqqqSfvo169f7LPPPptdv88++0S/fv3qPR4AAAAAAACA9Ajz87R27dp46aWXsvroo4/e5Lb29Tn66KOz16tWrdrmW+1/8MEH8f7779e7f759vP/++zFz5sxt6gMAAAAAAACA5iPMz9O7776b3do+IuKwww7L67i+ffvm1NOnT9+mPmbMmJFT9+nTZ6v62HgfAAAAAAAAANIhzM/TO++8k1P36NEjr+MqKiqioKAgq999990m7WPvvffO67i99tprs/sAAAAAAAAAkA5hfp5mz56dU3fr1i2v4woKCqKsrCyrZ82a1WR9tGzZMsrLy/M6rry8PFq2/Ne/7m3tAwAAAAAAAIDm02pHN7CzWLlyZU5dWlqa97ElJSUxb968iIhYtWpVk/XRrl27aNUqv3+FhYWF0bZt2+z829pHY9XU1OTUq1ev3q7n3xXV1tZm/7vxf58AbBszFqD5mLEAzcucBWg+ZixA89kVZuzG+efG+ejWEObnaePf/NatW+d9bJs2bRrcZ1v6aEwP6/tYH+Jv7zC9srIyp3ZngKZTU1MT06dP39FtAOySzFiA5mPGAjQvcxag+ZixAM1nV5qxG+ejW8Nt9vO08W92YWFh3scWFRVlr9euXdtkfTSmh6buAwAAAAAAAIDmI8zP08ZXwVdXV+d9bFVVVfZ6w6v0t7WPxvTQ1H0AAAAAAAAA0HzcZj9PxcXFOXVlZWXet7nf8Cr4jffZlj4ae2uGpuyjsTp27JhTt27dOgoKCrZrDwAAAAAAAADNoaamJie/3Tgf3RrC/Dy1b98+p162bFmUlJTkdeyKFSuy1+3atWuyPlavXh3r1q2LVq22/K9x3bp1sWbNmibro7GKioqia9eu2/WcAAAAAAAAADsrt9nPU/fu3XPqDz/8MK/jampqYsGCBVm91157NVkfNTU1MX/+/LyOmzdvXtTW1jZZHwAAAAAAAAA0H2F+nnr27JlTz5w5M6/j5syZEzU1NQ3us736mDVr1mb3AQAAAAAAACAdwvw89ezZMwoLC7P61Vdfzeu4KVOm5NQHHHDANvXRq1evnHpH9QEAAAAAAABA8xHm56lt27bRr1+/rJ4wYULU1dVt8bgXX3wxe11cXBxHHnnkNvXRo0eP6NGjR73759vHPvvsk7MHAAAAAAAAAGkR5jfCCSeckL2ePXt2TJgwYbPrV6xYEU899VRWDxgwIIqKira5j+OPPz57/fLLL8f777+/2fXvv/9+vPzyy1l93HHHbXMPAAAAAAAAADQfYX4jDBw4MEpLS7N65MiRsW7dugbX33LLLbFmzZqsHjRoUINrjzvuuOjVq1f06tVri2H7f/zHf2S3/K+rq4sRI0Zsdv3Pfvaz7HVhYWGcffbZm10PAAAAAAAAwI4lzG+EDh06xAUXXJDVr7/+elx33XVRXV29ydrRo0fHmDFjsnrAgAHbfIv99fbee+84/fTTs/rZZ5+NX/ziF5vc9r+uri5+/vOfx1/+8pfsvTPOOCP22muvJukDAAAAAAAAgObRoi6fB7+Tqa6ujvPPPz8mTpyYvVdRURGnnHJKdO/ePRYvXhzjxo2LqVOnZp+XlZXFo48+GnvssUeD+x533HExZ86cbL9nn312s32sXLkyzjrrrHj77bez9/bbb7/40pe+FOXl5TF//vz44x//GO+++272+f777x+//e1vo3379o3+dQMAAAAAAACw/Qjzt8KyZcvi4osvjilTpmxxbdeuXePOO++MQw45ZLPrGhvmR0TMnj07LrzwwpzAviE9e/aMe+65J7p3777FtQAAAAAAAADsWG6zvxVKS0tjzJgxceWVV0ZZWVm9a4qLi+PMM8+MJ598cotB/tbq3r17PPbYYzF48OAoLS1tsNfBgwfHY489JsgHAAAAAAAA2Em4Mn8b1dTUxOTJk+ODDz6IRYsWRUlJSXTr1i2OOuqoKC4u3m59VFVVxcsvvxxz5syJJUuWxCc+8YmoqKiIfv36RVFR0XbrAwAAAAAAAIBtJ8wHAAAAAAAAgMS4zT4AAAAAAAAAJEaYDwAAAAAAAACJEeYDAAAAAAAAQGKE+QAAAAAAAACQGGE+AAAAAAAAACRGmA8AAAAAAAAAiRHmAwAAAAAAAEBihPkAAAAAAAAAkBhhPgAAAAAAAAAkRpgPAAAAAAAAAIkR5gMAAAAAAABAYlrt6AaAxqmtrY3JkyfHzJkzY+HChVFSUhLdunWLfv36RXFx8Y5uD2C3MmPGjJg+fXrMnz8/ioqKory8PPr27Rtdu3bd0a0BNKuqqqp455134q233opFixZFZWVldOjQIcrLy6NPnz7RpUuXbT6HGQvsrpYtWxZvvfVWzJ07NxYvXhyrV6+OoqKiKC0tjX333TcOPPDAaNu27Tadw4wFaD5mLEDzmTVrVkybNi3mz58fERHl5eVx6KGHxl577bWDO2s+wnzYSdTU1MS9994bo0ePjgULFmzyeXFxcZx88skxbNiwKC0t3QEdAqShqqoqpk+fHv/4xz9i2rRpMW3atHjnnXeipqYmWzN9+vRtOse4cePitttuizfffHOTzwoKCuIzn/lMXHfddbH//vtv03kAUrJ48eL4f//v/8Vf/vKXmDRpUqxevbrBtYcffnicf/75ccIJJzT6PGYssDuaNm1aPPjggzF58uSYM2fOZte2adMmvvjFL8aQIUNi3333bdR5zFiA+v3ud7+LG264Iee9oUOHxqWXXpr3HmYssLvq1avXVh03duzYvH+enTRpUowcOTKmTJlS7+d9+/aNq6++Oo488sit6iVlLerq6up2dBPA5i1fvjwuvvjimDx58hbX7rHHHnHnnXfGQQcdtB06A0jLmWeeGW+++WZUV1dvdt22hPk//vGPY8yYMVtc17p16/jxj38cp5122lafCyAV77zzTgwcODDWrVvXqONOPvnkGD58eLRp0yav9WYssLt64IEH4sYbb2zUMYWFhTFs2LD4xje+kdd6MxagfgsXLoyTTjopli1blvN+Y8J8MxbYnTV3mH/33XfHzTffHLW1tZtdV1BQEFdccUVcdNFFW9VPqlyZD4lbt25dXH755TlB/p577hkDBw6MioqKWLx4cYwbNy6mTZsWERHz5s2LIUOGxCOPPBLl5eU7qm2AHWL9LGwut912W84fzouLi2PgwIHRq1evqKysjEmTJsWzzz4btbW1UVlZGd///vejvLw8PvOZzzRrXwDNraqqKifIb9myZRx44IFx5JFHxp577hkdOnSIRYsWxUsvvRTPP/98rP/O+B//+MdYuXJl3HnnnVFQULDZc5ixAB+rqKiI3r17xyc/+cno0qVLFBcXx6pVq+K9996Lv/71rzF79uyIiKiuro7hw4dHYWFhnH322Zvd04wFaNjw4cM3CfIbw4wF+JeuXbvm/YX+oqKiLa75wx/+EDfddFNWFxYWxsknnxyHHnpo1NbWxrRp0+JPf/pTVFdXR01NTdx0001RVlYWX/nKV7b615AaV+ZD4u65554YOXJkVn/5y1+OG2+8cZMhN2rUqBg+fHj2F6fHHnts3H333du1V4AdbcNvgbZv3z4OOuigOPTQQ2Py5Mk5t2DamivzX3vttfja176Wc6577rlnky9OTZo0KS655JJYvnx5RER07tw5nn766WjXrl2jzwmQijfeeCNOO+20KC8vj69//etxxhlnNPjF0alTp8bll18ec+fOzd77wQ9+sNmgyYwFdnd/+9vf4oMPPojjjjsuKioqGlxXV1cXY8aMieHDh2ePkSouLo6nnnqqwWcxm7EADfvb3/4WF154YURE9OzZM959993ss3yuzDdjAXL/TnbUqFHRv3//Jtl37ty5ceKJJ0ZVVVVERHTr1i3uvffeTa7mf/vtt+OCCy6IDz/8MCI+/pLAn//85+jWrVuT9LGjtdzRDQANW7lyZfzqV7/K6oMOOihGjBhR77eVBg0aFOecc05Wjx8/Pl555ZXt0idAKs4999wYMWJEjB07NiZNmhSjR4+Oa665JvbZZ59t3vvmm2/OXhcXF8ddd91Vb5B15JFHxk9/+tOsXrRoUYwaNWqbzw+wIxUXF8e1114bTz/9dHzrW9/a7B2gevfuHffee2+0bt06e++ee+7Z7P5mLLC7O+aYY+Lcc8/dbJAfEdGiRYv4z//8z7jsssuy91avXh1jx45t8BgzFqB+a9asiR/+8IcR8fGVnt/73vcavYcZC9B87rjjjizILygoiFtvvbXe2/Lvt99+ceutt2Z3BKyqqoo77rhju/banIT5kLAnnngili5dmtXDhg2LVq0afjrGFVdcEW3bts1qPxACu5vrr78+TjvttNh3332jRYsWTbbv22+/HRMmTMjqQYMGxZ577tng+hNPPDEOP/zwrP71r3+9xWc6AaSsR48eMXjw4JyAfnN69uwZp59+elbPnTs33nrrrXrXmrEAjXf22WfnPL6kocdNmbEADbv11ltjzpw5ERFx4YUXxic/+clGHW/GAjSf5cuXxxNPPJHVJ510UvTu3bvB9b17946TTjopqx9//PFYsWJFs/a4vQjzIWHPPPNM9rqiomKLz1Hq0KFDnHjiiVn93HPPZd9aAmDrjRs3Lqf+6le/usVjzjzzzOz1woUL47XXXmvyvgBStvFt9WbNmlXvOjMWoPFKSkqiU6dOWb1kyZJ615mxAPV74403sguh9t577xgyZEij9zBjAZrP+PHjo7q6OqsbO2Orq6tj/PjxzdLb9ibMh0StXbs2Xnrppaw++uij87rK9Oijj85er1q1yq32AZrAhj/49ejRI7p3777FYz772c82uAfA7mDj53+uWbOm3nVmLEDj1dXVxerVq7O6Y8eO9a4zYwE2VVtbGzfccEOsW7cuIiJuuOGGvO9AtSEzFqD5bDgf27RpE0ccccQWjzniiCOiTZs29e6xMxPmQ6LefffdnG8dHXbYYXkd17dv35x6+vTpTdoXwO5oxowZ2et85/Eee+wRe+yxR717AOwOZs+enVN37ty53nVmLEDjvfLKK7Fq1aqs3vC2zRsyYwE29etf/zp7PMmJJ54YxxxzzFbtY8YCNJ8N5+PBBx+82UdQr1dYWBgHH3xwvXvszIT5kKh33nknp+7Ro0dex1VUVOQ8N+/dd99t0r4Adjfz58+PlStXZnW+8zji41v1rbfxXAfY1W34yKiN/0C9nhkL0HiLFy+OH/3oR1ndqVOnOPXUUzdZZ8YCbGrevHlxyy23RMTHd5L6/ve/v1X7mLEA9XvwwQfjjDPOiP79+8chhxwSn/70p+OUU06JG264IZ5++umora3d4h61tbXx/vvvZ/XWztj33nsvr/OlbstfYwB2iI2vZOrWrVtexxUUFERZWVnMmzcvIhp+NikA+dnaeRwROd+2nzNnTpP1BJC6N998M1588cWs/tznPhcdOnTYZJ0ZC5CfVatWxaxZs+K5556LBx54IBYuXBgREUVFRTFy5EgzFiBPP/rRj7I7m1x22WVRXl6+VfuYsQD12/CL/RERS5YsiSVLlsSMGTPid7/7Xeyzzz5xww03xOc+97kG9/joo4+isrIyq7d2xlZWVsZHH3201bM+FcJ8SNSG3+yMiCgtLc372JKSkizM3/C2ewA03rbM4w3XVldXR2Vl5VY9hw9gZ7Ju3bq4/vrrc779/u1vf7vetWYsQP2uu+66eOyxxza75uCDD44f/vCH0bt373o/N2MBcv35z3+OZ599NiIiDjzwwDj33HO3ei8zFqBh7dq1i9LS0qisrIylS5dGTU1N9tn7778fF154YQwbNiwGDx5c7/Ebz9iSkpK8z73xPF65cqUwH2geq1evzqkb8wNdmzZtGtwHgMbZeI4WFRXlfezGs3vVqlX+gA7s8kaOHJk9gzQi4qyzzopDDz203rVmLEDjtWjRIs4444y4+uqr4xOf+ESD68xYgH9ZuXJl/OQnP4mIj+foD3/4w5xHlTaWGQvwL0VFRfHFL34xjj/++DjiiCNywvPVq1fHyy+/HA888EB2B7/a2toYMWJElJeXx8knn7zJfhtfpNqYGbnx2l0hIxPmQ6I2vIVIxMfPGc3Xhj88rl27tsl6AtgdNdU8rm8vgF3N73//+7j//vuz+pOf/GR897vfbXC9GQtQv86dO2fP+6ytrY2VK1fG0qVLIyKirq4uHn300Rg7dmxcdNFFcfHFF0fLli032cOMBfiXm266KRYsWBAREV/72teiT58+27SfGQvwL+PHj49OnTrV+1lxcXEce+yxceyxx8YDDzwQN954Y/bZj3/84zj22GOjffv2OcdUVVXl1Lv7jN30J30gCRt/e6i6ujrvYzccdBtepQ9A4zXVPK5vL4Bdyfjx4+O//uu/srpjx45xxx13RNu2bRs8xowFqN+wYcPi6aefjqeffjqeeeaZmDhxYkyYMCF+9rOfxb777hsRH19ldMstt8SwYcOirq5ukz3MWICPvfrqq/Hb3/42IiI6deoUV1111TbvacYC/EtDQf7GvvnNb8agQYOyeunSpfHQQw9tsm7jQH53n7HCfEhUcXFxTt2Ybw9teDX+xvsA0Dgbz9GNfyDcnI1nd7t27ZqkJ4DUTJo0KS677LJYt25dRHw87+65554scGqIGQuQv06dOsVXvvKVePzxx+PEE0/M3v+///u/LKTakBkLELFu3bq44YYbora2NiIirr322kY9374hZizA1hk6dGjODP3rX/+6yZqN52Jj8rGN1+4KGZkwHxK18W1Fli1blvexK1asyF77YRBg22zLPF6+fHn2urCwcJf4JijAxv7xj3/ExRdfnH2htHXr1nHnnXdG7969t3isGQvQeEVFRfHzn/88KioqsvfuuuuuLKhaz4wFiLjvvvtixowZERFx1FFHxWmnndYk+5qxAFuntLQ0+vXrl9WvvfbaJms2nrEbzs0t2XjtxnvtjIT5kKju3bvn1B9++GFex9XU1GTPf4qI2GuvvZq0L4DdzdbO443XbviXrQC7ihkzZsT5558fK1eujIiP/zLy1ltvjf79++d1vBkLsHXatGkTp59+elbPmzcvpk+fnrPGjAV2dx999FHccccdEfHxz6k/+MEPmmxvMxZg6/Xo0SN7XV1dvUkAX1ZWlvNFp62dsa1bt46ysrJt6DQNrXZ0A0D9evbsmVPPnDkzjjrqqC0eN2fOnKipqWlwHwAap7y8PNq3b58FVTNnzsz72A3XmsfArub999+PwYMHx9KlSyMioqCgIH7+85/H5z//+bz3MGMBtt6nPvWpnHrmzJlx4IEHZrUZC+zuFi5cmN09qkWLFnHJJZdsdv2Gf6caETF69Oj43//936weOXJkHHbYYRFhxgJsi7Zt2+bUa9eujZKSkqxu2bJl9OjRI7uzytbO2H322Sdattz5r2vf+X8FsIvq2bNnFBYWZvWrr76a13FTpkzJqQ844ICmbAtgt7ThLM13Hs+bNy/mzZtX7x4AO7u5c+fGeeedFx999FFEfPyXoz/5yU/ipJNOavReZizA1ikqKsqpNw6hIsxYgPWqqqpi5syZm/1nzpw5OccsW7Ys5/P1XwxYz4wF2DoLFy7MqTt27LjJml69emWvX3/99Vi3bt0W962uro7XX389q3eVGSvMh0S1bds257khEyZMiLq6ui0e9+KLL2avi4uL48gjj2yW/gB2J8ccc0z2+oMPPojZs2dv8ZgXXnghpz722GObvC+AHeGjjz6Kb37zmzF37tzsve9///txxhlnbNV+ZizA1tl4Xnbp0mWTNWYsQPMxYwG2zuTJk7PXXbt23eRLqhG5M3bNmjXxyiuvbHHfV155JeeLV7vKjBXmQ8JOOOGE7PXs2bNjwoQJm12/YsWKeOqpp7J6wIAB9Q5BABpnw3kcEfHII49s8ZhHH300e925c+fo06dPU7cFsN0tXbo0Bg8eHB988EH23lVXXRXnnnvuVu9pxgJsnaeffjp73apVq5yrl9YzY4Hd2YEHHhjTp0/P+59nnnkm5/ihQ4fmfN6/f/+cz81YgMabMGFCvPfee1l99NFH17vu85//fLRq9a+nxTd2xhYWFgrzgeY3cODAKC0tzeqRI0du9lYit9xyS6xZsyarBw0a1Kz9Aewu9t9//5w/tI8aNSrnitSNPfXUUznfMD3nnHN2ieczAbu3lStXxgUXXJA9sy4iYsiQIXHRRRdt075mLLC7W7t2bdTW1jbqmLFjx+bcma9///45f3+wnhkL0HzMWGB3V11dndft79dbvHhxXH/99TnvnXrqqfWuLSkpiYEDB2b12LFjY+rUqQ3uPXXq1Bg7dmxWDxw4MEpKSvLuLWX+nwIS1qFDh7jggguy+vXXX4/rrrsuqqurN1k7evToGDNmTFYPGDDALfYBmtB3vvOd7PXq1avjkksuiQULFmyybtKkSTk/lHbq1Cm++c1vbo8WAZpNZWVlXHLJJTFt2rTsvUGDBsWVV17ZJPubscDu7LXXXouBAwfG448/HqtWrdrs2srKyvjlL38Z11xzTfZey5YtNzuPzViA5mPGAruz+fPnx5e+9KV45JFHYsWKFZtd+8orr8RZZ52V80iSz372sw1emR/x8R1SCgsLIyKipqYmLr/88njnnXc2Wff222/HZZddFjU1NRHx8VX5Q4cO3ZpfUpJa1OXzEG5gh6muro7zzz8/Jk6cmL1XUVERp5xySnTv3j0WL14c48aNy/lGUllZWTz66KOxxx577IiWAXaYUaNGxejRozd5f9GiRTl/Mbr33ntvsmaPPfao99gN3XzzzXHXXXdldbt27eLUU0+NAw44ICorK2PSpEnxzDPPZFdWFRQUxC9/+csYMGDA1v6SAJLw+OOPx7XXXpvz3l577RUtWrTIe48vfvGLMWzYsAY/N2OB3dXEiROzO+u1adMm+vTpEwcddFCUl5dHhw4doqamJhYvXhxvvvlmPP/885v8Rel3v/vdLQZCZizAls2ePTuOP/74rB46dGhceumlWzzOjAV2VxvOzaKiojj88MPjwAMPjG7dukX79u2jqqoqPvzww5gwYcImV9Xvvffe8fDDD0enTp02e45HHnkk58tQRUVFcfLJJ8chhxwSERHTpk2LP/7xjzkXwf70pz+Nr371q031y9zhWm15CbAjFRYWxm233RYXX3xxTJkyJSIi5syZk/MD4oa6du0ad955pyAf2C0tW7YsZs6cucV19a1Z/83Nzbniiiti6dKl8dvf/jYiIlatWhW/+c1v6l1bVFQUP/rRj/zhHNgl1Hf751mzZjVqj0WLFm32czMW4ONb7v/973+Pv//971tc26FDh/jud78bZ5xxxhbXmrEAzceMBYioqqrK++fY/v37xy9+8YstBvkREV/96ldj4cKFceutt0ZtbW1UVVXFY489Fo899tgma1u2bBmXX375LhXkR7jNPuwUSktLY8yYMXHllVdGWVlZvWuKi4vjzDPPjCeffDL7RhIATatFixbxox/9KG6//fY44IAD6l3TsmXL+OxnPxu///3v4/TTT9/OHQLsvMxYYHfVq1evuOqqq6Jfv37RunXrLa7v1q1bDBkyJP70pz/lFeRHmLEAzcmMBXZXHTt2jLPPPjv23XffLd65r0WLFnH44YfHzTffHA888ECUl5fnfZ5LLrkkRo0aFX369GlwTd++fWPUqFExZMiQvPfdWbjNPuxkampqYvLkyfHBBx/EokWLoqSkJLp16xZHHXVUFBcX7+j2AHYr06dPj+nTp8eCBQuisLAwysvLo2/fvo36YRSA+pmxwO6ouro63n777Xj//fdjwYIFsXr16igoKIgOHTpEWVlZHHjggVFRUbHN5zFjAZqPGQvsjlauXBkzZsyI2bNnx6JFi2LNmjVRWFgYJSUlseeee8Zhhx0WJSUl23yemTNnxrRp02L+/PkREVFeXh6HHnpovY9V3VUI8wEAAAAAAAAgMW6zDwAAAAAAAACJEeYDAAAAAAAAQGKE+QAAAAAAAACQGGE+AAAAAAAAACRGmA8AAAAAAAAAiRHmAwAAAAAAAEBihPkAAAAAAAAAkBhhPgAAAAAAAAAkRpgPAAAAAAAAAIkR5gMAAAAAAABAYoT5AAAAAAAAAJAYYT4AAAAAAAAAJEaYDwAAAAAAAACJEeYDAAAAAAAAQGKE+QAAAAAAAACQGGE+AAAAAAAAACRGmA8AAAAAAAAAiRHmAwAAAGxns2fPjl69emX/3HbbbTu6JQAAABLTakc3AAAAAGx/s2fPjuOPP75J9rrjjjvihBNOaJK9AAAAgI+5Mh8AAAAAAAAAEiPMBwAAAAAAAIDEuM0+AAAAEOXl5fGb3/xmq47t3LlzE3cDAAAACPMBAACAaNWqVXTv3n1HtwEAAAD8/9xmHwAAAAAAAAASI8wHAAAAAAAAgMS4zT4AAACw3VVVVcWkSZNizpw5sWTJkujYsWPss88+ccQRR0RBQcE27V1bWxvTpk2L9957LxYtWhR1dXXRuXPn2GeffeKwww6Lli2b5tqG9957L954441YsmRJLF++PNq2bRtlZWWx//77x3777bdN56mtrY0pU6bEzJkz46OPPori4uKoqKiIfv36Rfv27ZukfwAAANImzAcAAACa3OzZs+P444/P6qFDh8all14aK1eujDvuuCP+8Ic/xNKlSzc5rnPnznHeeefF4MGDGx3qL1++PO6888547LHHYsmSJfWu6dixY5x66qnxrW99Kzp27Nio/def47777ovHH388PvzwwwbXfeITn4gvfOEL8R//8R/Ru3fvvPevq6uLBx98MB588MGYO3fuJp8XFhbGV7/61bj88su3qn8AAAB2HsJ8AAAAYLv48MMP47zzzov33nuvwTWLFi2KkSNHxrhx4+JXv/pVdOjQIa+9X3755Rg6dGi9XxDY0NKlS+PBBx+Mxx9/PP77v/87PvOZz+Td/9NPPx3f+973Yvny5Vtcu2TJkvjDH/4Q//znP+OJJ57Ia/8VK1bEFVdcEc8//3yDa6qrq+M3v/lNTJw4Me6///4oLy/Pu38AAAB2LsJ8AAAAoNlVVlbGRRddlAX5RUVF0adPnygrK4tly5bFtGnTYtmyZdn6V199NS644IIYNWpUtG7derN7v/DCC3HJJZdEZWVlzvv77rtv9OzZM1q0aBHvvfdevPXWW9lny5YtiwsvvDBuv/32+PznP7/F/h944IH42c9+FnV1dTnvl5WVRa9evaJjx46xdu3amDdvXsyYMSOqqqq2uOeGampqcoL8Nm3aRO/evaOsrCzWrl0b//jHP2L+/PnZ+nfeeSeuu+66uP/++xt1HgAAAHYewnwAAACg2T388MOxfPnyaNGiRZx77rlx2WWX5Vx1X1VVFb/73e9i5MiRsWbNmoj4ONC//fbb46qrrmpw30WLFsWwYcNygvyDDz44fvzjH8chhxySs/bNN9+M66+/PqZNmxYRH1/lfu2118b//u//bvYK9+eeey5GjBiRE+T369cvvvOd70Tfvn2jRYsWOeurqqri+eefj8ceeyzmzJmTx+9OxEMPPRRLly6N1q1bx+WXXx7nnHNOtGnTJvu8rq4u/vCHP8QPfvCDqK6ujoiIF198McaPHx/HHntsXucAAABg59KibuOvlAMAAAC7vI2faV9eXh6/+c1vGr1P27Zto3Pnzlvcf71rrrkmzj///Ab3e/7552PIkCFZYN2qVav405/+FHvvvXe967///e/Ho48+mtV9+/aN+++/P9q2bVvv+rVr18bgwYPjlVdeyd778pe/HDfddFO969esWRPHH398LFq0KHvvnHPOieuvvz5atmzZ4K9jvYULF0aXLl02eb++35+ioqK4//7748gjj2xwv4cffjj+67/+K6v//d//Pf77v/97i30AAACw8xHmAwAAwG6oobC9sY4//vj4n//5n7z2P+qoo2L06NFb3HPEiBFx3333ZfX5558f11xzzSbrlixZEscee2x2VX6bNm3ij3/8Y3Tv3n2z+8+dOzdOOumk7A4AhYWF8eyzz0bXrl03Wfvggw/G8OHDs7p///7x4IMPbnI1fmPV9/vzne98Jy6++OLNHldbWxuf//zns1vud+nSJV544YVt6gUAAIA0bfkr5AAAAABN4Fvf+lZe6y666KIoLCzM6ieffLLedX/+859zbq//la98ZYtBfkTEnnvuGV/72teyurq6OsaOHVvv2kceeSSn/t73vrfNQX59iouL45xzztniupYtW8aAAQOyeuHChfHRRx81eT8AAADseMJ8AAAAoNl16tQp+vfvn9faT3ziE/HpT386qxcsWBBz587dZN2UKVNy6i9/+ct597Px2o33iohYvHhxvPXWW1l96KGHxqc+9am8z9EYffv2jfbt2+e1tmfPnjn14sWLm6MlAAAAdrBWO7oBAAAAYMerqKiIZ599ttn2P+igg/J6xvx6hx56aDz33HNZ/frrr8eee+6Zs+b111/PXhcUFMQhhxzSqH6Kioqiqqpqk73We+2113LqzT3LflttHNBvTocOHXLqlStXNnU7AAAAJMCV+QAAAECz23vvvRu1vkePHjn1okWLNlmz4RXp5eXl0aZNm7z3b9WqVey111717rXewoULc+p999037/0ba+OAfnNatcq9NmPdunVN3Q4AAAAJEOYDAAAAzS7fW8g3tH758uWbrNnwvcbuH5EboK9atWqTUHzJkiUNrm9qjblrAQAAALsHf1IEAAAAyEOLFi12dAsAAADsRoT5AAAAQLNr7HPdN15fUlKyyZoN39ua58avWLEie92uXbtNbl/fsWPHnLq+uwMAAABAcxHmAwAAAM1u5syZjVr/wQcf5NSdO3feZE2nTp2y1/Pnz4+1a9fmvf+6deti9uzZ9e61XpcuXXLqd999N+/9AQAAYFsJ8wEAAIBm9/rrr0dtbW3e66dNm5ZTH3zwwZus2fC9mpqa+Mc//pH3/m+88UZUVlZudv8+ffrk1JMmTcp7fwAAANhWwnwAAACg2S1ZsiQmTpyY99q///3vWd21a9fYc889N1nXt2/fnPpPf/pT3v383//932b3ivj4av0DDjggq6dOnRrTp0/P+xwAAACwLYT5AAAAwHbxP//zP3mtu/vuu6O6ujqrTznllHrX/du//Vu0bt06q//whz/EvHnztrj//Pnz43e/+11Wt2rVKr70pS/Vu/ZrX/taTv2zn/0s6urqtngOAAAA2FbCfAAAAGC7eOmll+Lee+/d7JoXXnghRo8endWtWrWKs846q961nTp1ipNPPjmrV69eHVdffXXO7fM3VllZGVdffXWsXr06e+/EE0+M8vLyetefeeaZ0aVLl6x+8cUXY/jw4XkH+gsXLsxrHQAAAGxMmA8AAADEunXrYvbs2Vv1z6JFi7a4f0lJSURE/OIXv4jhw4fHihUrcj6vqqqKMWPGxLe//e2cq/IHDx4cPXr0aHDfq666Kjp16pTVL7/8cpx77rnxxhtvbLL2zTffjHPPPTdeeuml7L3S0tK49tprG9y/bdu2MWLEiGjZ8l9/hTJq1Kj4xje+EVOmTKn3mKqqqvjLX/4Sl156aVx00UUN7g0AAACb02pHNwAAAADsePPnz4/jjz9+q449/vjjt3gL/bPOOiv++te/xltvvRUPPvhgPPTQQ9G3b98oKyuLZcuWxdSpU2PZsmU5x/Tp0yeGDh262X27dOkSI0aMiG9/+9tRVVUVERGvvfZanHbaabH//vvHJz/5yWjRokW89957MWPGjJxjCwsL48Ybb2zwqvz1Pve5z8W1116bc4v9iRMnxte//vUoKyuLXr16RceOHaOysjLmzZsX06dPz3r51Kc+tdm9AQAAoCHCfAAAAKDZtW7dOn75y1/GeeedFx988EFUVVXFxIkTG1zfp0+fuOeee6J169Zb3PuYY46Je+65Jy6//PJYunRp9v5bb70Vb731Vr3HlJSUxC233BKf/exn8+r/m9/8ZnTt2jWuv/76WLVqVfb+Rx99FB999FFeewAAAEBjuM0+AAAAsF1UVFTE73//+/jGN74RpaWl9a7p3LlzXHXVVTFmzJjs1vz5+PSnPx1PPfVUnHfeedGxY8cG13Xs2DHOPffceOqpp/IO8tc76aSTYty4cTF48ODo0qXLZtd26dIlzjrrrBgxYkSjzgEAAADrtahbf384AAAAgCYye/bsnNv2Dx06NC699NKsrqqqipdffjnmzp0bixcvjo4dO0aPHj2iX79+UVBQsE3nrq2tjddeey3ee++9WLx4cUREdOrUKfbZZ5847LDDtnn/iIi6urp4880346233orFixfH6tWro7i4OMrLy2P//fePfffdN1q0aLHN5wEAAGD35Tb7AAAAwHZXVFTU6Cvj89WyZcvo27dv9O3bt1n2j4ho0aJFHHjggXHggQc22zkAAADYvbnNPgAAAAAAAAAkRpgPAAAAAAAAAIkR5gMAAAAAAABAYoT5AAAAAAAAAJAYYT4AAAAAAAAAJEaYDwAAAAAAAACJEeYDAAAAAAAAQGJa1NXV1e3oJgAAAAAAAACAf3FlPgAAAAAAAAAkRpgPAAAAAAAAAIkR5gMAAAAAAABAYoT5AAAAAAAAAJAYYT4AAAAAAAAAJEaYDwAAAAAAAACJEeYDAAAAAAAAQGKE+QAAAAAAAACQGGE+AAAAAAAAACRGmA8AAAAAAAAAiRHmAwAAAAAAAEBihPkAAAAAAAAAkBhhPgAAAAAAAAAkRpgPAAAAAAAAAIkR5gMAAAAAAABAYoT5AAAAAAAAAJAYYT4AAAAAAAAAJEaYDwAAAAAAAACJEeYDAAAAAAAAQGL+P6NGi8F+953kAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "image/png": {
              "width": 1017,
              "height": 717
            }
          }
        }
      ],
      "source": [
        "plt.plot(A2, label='train accuracy')\n",
        "plt.plot(B2, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd9c9eGmecP"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jFRntT6mgoW",
        "outputId": "bdf075e4-19d3-4477-a079-559c62a48abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6470588235294118"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBG2MIlgmkHv"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXTHRhD5mqnn",
        "outputId": "edaa1edb-b515-4c22-f44d-c4982ef448f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTdojX6Jmqqm",
        "outputId": "c71c902f-808f-422e-9540-14497891f5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Faixa 1       0.70      0.84      0.76        19\n",
            "     Faixa 2       0.17      0.12      0.14         8\n",
            "     Faixa 3       1.00      0.71      0.83         7\n",
            "\n",
            "    accuracy                           0.65        34\n",
            "   macro avg       0.62      0.56      0.58        34\n",
            "weighted avg       0.63      0.65      0.63        34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "HQ54xluu8RL3",
        "outputId": "70e61c2f-fcf3-4791-8edb-0438fb378209"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 981,
              "height": 730
            }
          }
        }
      ],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('Faixa TRL verdadeira')\n",
        "  plt.xlabel('Faixa TRL predita');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZTajeJuiP_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad2dbb2d7f984ad1ab4993ea6844ae5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f70847c897e49b1a1fc78b010a97900",
              "IPY_MODEL_70b1061c935743498963dc8472eaf404",
              "IPY_MODEL_763801631c884164a4b7b097eab11cdf"
            ],
            "layout": "IPY_MODEL_e9d958c8283f493ba63e370660f58b34"
          }
        },
        "1f70847c897e49b1a1fc78b010a97900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa47fc762c9d48ee961a205e84b1e4e9",
            "placeholder": "​",
            "style": "IPY_MODEL_bc864cc9def64dc4bf2ae07445688f79",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "70b1061c935743498963dc8472eaf404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8a0d6d755cb45f7bbe4f516d0f24f0c",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d5083a0b2f4402bb411d8052b9a748",
            "value": 43
          }
        },
        "763801631c884164a4b7b097eab11cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9a063a1d5a94c359da0c5fcac875416",
            "placeholder": "​",
            "style": "IPY_MODEL_28f3ae7a04b548ce8654ae88209c028a",
            "value": " 43.0/43.0 [00:00&lt;00:00, 1.05kB/s]"
          }
        },
        "e9d958c8283f493ba63e370660f58b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa47fc762c9d48ee961a205e84b1e4e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc864cc9def64dc4bf2ae07445688f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8a0d6d755cb45f7bbe4f516d0f24f0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d5083a0b2f4402bb411d8052b9a748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9a063a1d5a94c359da0c5fcac875416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28f3ae7a04b548ce8654ae88209c028a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e127530f156b4b42b95b6d2ff7647b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_898e3ddfe6cd4af2a26862b1d05ee3ef",
              "IPY_MODEL_6e7d655a8b9d41cf89f378beb69dc201",
              "IPY_MODEL_bce9ec6645144acfbec6ee21268bf24e"
            ],
            "layout": "IPY_MODEL_624d5e5110fa4f6185fde2e9948ba8e6"
          }
        },
        "898e3ddfe6cd4af2a26862b1d05ee3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8113b327f30e43118513c9ff0dc43f39",
            "placeholder": "​",
            "style": "IPY_MODEL_efcaaafab4374aa7b67abdf81a9bbdd7",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "6e7d655a8b9d41cf89f378beb69dc201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8f7ea0b6b7e458f88395aaca0f5c391",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e304d0ac7160401ea1900a297c44acc2",
            "value": 209528
          }
        },
        "bce9ec6645144acfbec6ee21268bf24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8069b70ce048b5b33816b854e7f310",
            "placeholder": "​",
            "style": "IPY_MODEL_c996f18cc73d4688836cb7aa44ed92a4",
            "value": " 210k/210k [00:00&lt;00:00, 537kB/s]"
          }
        },
        "624d5e5110fa4f6185fde2e9948ba8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8113b327f30e43118513c9ff0dc43f39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efcaaafab4374aa7b67abdf81a9bbdd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f7ea0b6b7e458f88395aaca0f5c391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e304d0ac7160401ea1900a297c44acc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a8069b70ce048b5b33816b854e7f310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c996f18cc73d4688836cb7aa44ed92a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a2e38b141544a44929fde16e91fbd20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63986f683aea4f1082c41d5b6a27bb83",
              "IPY_MODEL_6fd88f5a0fa34cf9b3e608ef5f721d4f",
              "IPY_MODEL_83ae1551c18345cfbe0d24eb49ce1ec7"
            ],
            "layout": "IPY_MODEL_741f0469f6cc487fb560e10a516d69bd"
          }
        },
        "63986f683aea4f1082c41d5b6a27bb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b593f114c01f46179b0708ef2efbbec4",
            "placeholder": "​",
            "style": "IPY_MODEL_57612c0d17754e2c897fd70814081dd3",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "6fd88f5a0fa34cf9b3e608ef5f721d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b8515962a734bb0aadfac54868d71e6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d811322334d3488e94f71ed86671eef6",
            "value": 2
          }
        },
        "83ae1551c18345cfbe0d24eb49ce1ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec67d8fb6f094967b522b0abd5e904ee",
            "placeholder": "​",
            "style": "IPY_MODEL_a05f71a528ee4e36a5e258a37312d806",
            "value": " 2.00/2.00 [00:00&lt;00:00, 73.8B/s]"
          }
        },
        "741f0469f6cc487fb560e10a516d69bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b593f114c01f46179b0708ef2efbbec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57612c0d17754e2c897fd70814081dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b8515962a734bb0aadfac54868d71e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d811322334d3488e94f71ed86671eef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec67d8fb6f094967b522b0abd5e904ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a05f71a528ee4e36a5e258a37312d806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2226a154f0f84f75b353fb171616cabc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3c6644881b94682b734f7d47bd4b008",
              "IPY_MODEL_297fbd7058744a3eba01fd6a37a122d5",
              "IPY_MODEL_5fd1501c1a944a27b8370795299aefb0"
            ],
            "layout": "IPY_MODEL_b376d7c0a93a4948a381cb07f4b1b412"
          }
        },
        "f3c6644881b94682b734f7d47bd4b008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bb9d2c8d5f348089d4c1c2060c306f9",
            "placeholder": "​",
            "style": "IPY_MODEL_f19a4a21b6c34902af9985a3f21731ff",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "297fbd7058744a3eba01fd6a37a122d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2304a5c72064f998ce6081119d5d742",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_792e3d50c86349a7b4b0cdef02262e7a",
            "value": 112
          }
        },
        "5fd1501c1a944a27b8370795299aefb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8e6288bc9284089a763025fd21eafea",
            "placeholder": "​",
            "style": "IPY_MODEL_4775e597223842928f571508938c5bf7",
            "value": " 112/112 [00:00&lt;00:00, 5.68kB/s]"
          }
        },
        "b376d7c0a93a4948a381cb07f4b1b412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb9d2c8d5f348089d4c1c2060c306f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f19a4a21b6c34902af9985a3f21731ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2304a5c72064f998ce6081119d5d742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792e3d50c86349a7b4b0cdef02262e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8e6288bc9284089a763025fd21eafea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4775e597223842928f571508938c5bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a064b9451e64da1b66c64ec686febe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6b556126c45491cb291664ab66ac18e",
              "IPY_MODEL_5fe39a7da7324652bdd748e9cde6a76b",
              "IPY_MODEL_aada7691529d4dbbba5e2b228cd0c0b7"
            ],
            "layout": "IPY_MODEL_11c4bd4b875f405c82d1b85094eb1598"
          }
        },
        "f6b556126c45491cb291664ab66ac18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d4e4c8d6fc4b668bc81adcef0ba8ec",
            "placeholder": "​",
            "style": "IPY_MODEL_853850129a654046b2e21012ff7cec98",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "5fe39a7da7324652bdd748e9cde6a76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8892c5441ec42faa405687b8439ee06",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12d45457cadd4ad8a054ba204df24af3",
            "value": 647
          }
        },
        "aada7691529d4dbbba5e2b228cd0c0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d7ce11ae9ff4011bf4b963754b1e4de",
            "placeholder": "​",
            "style": "IPY_MODEL_a3ab5da0ece54a8da02242fa438072db",
            "value": " 647/647 [00:00&lt;00:00, 42.4kB/s]"
          }
        },
        "11c4bd4b875f405c82d1b85094eb1598": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84d4e4c8d6fc4b668bc81adcef0ba8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "853850129a654046b2e21012ff7cec98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8892c5441ec42faa405687b8439ee06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12d45457cadd4ad8a054ba204df24af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d7ce11ae9ff4011bf4b963754b1e4de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ab5da0ece54a8da02242fa438072db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9f69fe64b6c41deae7713335a6e711a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c27c43d8d14497cb87b909c11c135cd",
              "IPY_MODEL_e88d31cb26d0403599867198ff8753f8",
              "IPY_MODEL_75baee6367594123b139de9177e6366b"
            ],
            "layout": "IPY_MODEL_b8c3ff897ac5425d92a3c0302310a332"
          }
        },
        "0c27c43d8d14497cb87b909c11c135cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3285d5b705c4ddebab7a1b2fb3d3446",
            "placeholder": "​",
            "style": "IPY_MODEL_fbd276fdda614883a0d21998c65ea9b8",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "e88d31cb26d0403599867198ff8753f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9216a425b3284ed3b6155df8610d8ef8",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43e3dda1a733457ea467ae456946d8c1",
            "value": 438235074
          }
        },
        "75baee6367594123b139de9177e6366b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da599c47048043b7a1a22fd664d73371",
            "placeholder": "​",
            "style": "IPY_MODEL_0b812457bd094dd2a93caae70f18449f",
            "value": " 438M/438M [00:04&lt;00:00, 81.3MB/s]"
          }
        },
        "b8c3ff897ac5425d92a3c0302310a332": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3285d5b705c4ddebab7a1b2fb3d3446": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbd276fdda614883a0d21998c65ea9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9216a425b3284ed3b6155df8610d8ef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43e3dda1a733457ea467ae456946d8c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da599c47048043b7a1a22fd664d73371": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b812457bd094dd2a93caae70f18449f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}