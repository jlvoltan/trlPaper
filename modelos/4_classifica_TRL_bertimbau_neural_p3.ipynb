{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jKuooZfNFU"
      },
      "source": [
        "# Classificação faixa TRL - Bertimbau + Rede Neural [kfold][P3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9-gf3IMD5C"
      },
      "source": [
        "- BERTimbau Base (aka \"bert-base-portuguese-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i41YvsyfSUR"
      },
      "source": [
        "## Código Versão 1.0 - 21 OUT 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfR22FfgbJ"
      },
      "source": [
        "- Uso dos classificadores: *Rede Neural*\n",
        "- **k-Fold cross-validation: Esse código executou a rodada 3**.\n",
        "- Matriz de confusão;\n",
        "- Semente 42\n",
        "- 50 épocas\n",
        "- BATCH_SIZE = 8\n",
        "- Limite 242 tokens\n",
        "- optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.csv   ou  dataset_pre_processado_1.csv  ou  dataset_pre_processado_stem_2.csv\n",
        "#     CSV1                  CSV2                                   CSV3\n",
        "dataset = \"dataset.csv\""
      ],
      "metadata": {
        "id": "vXMRttjhRQ2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lembre-se estamos usando o dataset: \" + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJkf84RR64K",
        "outputId": "32ea2248-c614-422b-ce1b-98f96a6ffb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lembre-se estamos usando o dataset: dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "particao=3  # O KFold apresenta 5 partições (rodadas)\n",
        "particao-=1  #Funcionará com o índice de kfold_train e kfold_test"
      ],
      "metadata": {
        "id": "k2TIOYIqQNps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melhor_modelo = 'best_model_TRL_bertimbau_base_neural_' + str(particao+1) + '.bin'\n",
        "melhor_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lFCPPlujqL5L",
        "outputId": "bd863317-ce8f-482e-c80f-3d88ddde316b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_model_TRL_bertimbau_base_neural_3.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 242"
      ],
      "metadata": {
        "id": "_6clpQcISMQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jlIjUzOgRY"
      },
      "source": [
        "- 63,7% treinamento - 16% validação...............20% teste\n",
        "- EPOCHS = 50\n",
        "- lr=2e-5\n",
        "- BATCH_SIZE = 8\n",
        "- Quantidade de tokens máxima = 242"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Msvyn0MfsOS"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC81VL_gQ-5"
      },
      "source": [
        "### Bibliotecas e ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZxPMZOfICS",
        "outputId": "7ff1a65a-7a47-4da6-f399-cc534f4bbab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5RDBcpVf0TS",
        "outputId": "0f60a31a-11e6-4bff-a2c7-ae7bddfb419d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmcqgWcf0WZ",
        "outputId": "99545d4c-55f5-4282-8264-7c8312f191e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 21 18:04:54 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoneZ3CngEDZ",
        "outputId": "2443cf08-a47a-48e2-fd61-08984755c20f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWHFe7ef0Zp",
        "outputId": "72229de6-15d3-4412-9760-79d58b0ece83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy       : 1.23.5\n",
            "pandas      : 1.5.3\n",
            "torch       : 2.1.0+cu118\n",
            "transformers: 4.34.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particoes KFOLD"
      ],
      "metadata": {
        "id": "oENoyHsUPjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para mais detalhes, consulte o código *1-kfold.ipynb* em que as partições foram sorteadas. As células a seguir recriam o resultado obtido."
      ],
      "metadata": {
        "id": "qL3FKCGlPtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = np.array([  0,   1,   2,   3,   5,   7,   9,  10,  11,  12,  13,  14,  15,\n",
        "         16,  17,  19,  20,  21,  22,  23,  25,  26,  27,  28,  30,  32,\n",
        "         33,  34,  35,  36,  37,  39,  40,  44,  45,  46,  47,  48,  49,\n",
        "         50,  52,  53,  54,  55,  56,  57,  59,  60,  61,  63,  64,  66,\n",
        "         68,  69,  70,  71,  72,  75,  76,  77,  78,  79,  80,  81,  82,\n",
        "         83,  84,  85,  87,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
        "        102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
        "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130,\n",
        "        131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145,\n",
        "        146, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
        "        164, 165, 166, 167])\n",
        "\n",
        "array2 = np.array([  0,   1,   3,   4,   5,   6,   7,   8,  10,  13,  14,  15,  16,\n",
        "         17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "         34,  36,  37,  38,  40,  41,  42,  43,  44,  45,  47,  49,  51,\n",
        "         52,  53,  54,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  70,  71,  73,  74,  75,  76,  77,  80,  83,\n",
        "         84,  86,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99,\n",
        "        100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115,\n",
        "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "        129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
        "        147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162,\n",
        "        163, 165, 166, 167])\n",
        "array3 = np.array([  1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
        "         17,  18,  20,  21,  22,  23,  24,  25,  28,  29,  30,  31,  32,\n",
        "         33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,\n",
        "         48,  50,  51,  52,  54,  55,  56,  57,  58,  59,  62,  63,  65,\n",
        "         67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "         82,  84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,\n",
        "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111,\n",
        "        112, 113, 114, 118, 120, 121, 122, 123, 124, 126, 128, 129, 131,\n",
        "        132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 146, 147,\n",
        "        148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162,\n",
        "        163, 164, 165, 166])\n",
        "array4 = np.array([  0,   2,   3,   4,   6,   8,   9,  10,  11,  12,  15,  16,  18,\n",
        "         19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  31,  32,  33,\n",
        "         34,  35,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
        "         49,  50,  51,  53,  56,  57,  58,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  71,  72,  73,  74,  75,  76,  78,  79,  80,\n",
        "         81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
        "         94,  95,  98, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111,\n",
        "        112, 114, 115, 116, 117, 119, 121, 123, 124, 125, 127, 128, 129,\n",
        "        130, 131, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145,\n",
        "        146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159,\n",
        "        162, 163, 164, 166, 167])\n",
        "array5 = np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
        "         14,  15,  16,  17,  18,  19,  21,  23,  24,  26,  27,  28,  29,\n",
        "         30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,  43,\n",
        "         44,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,\n",
        "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  72,  73,  74,\n",
        "         77,  78,  79,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
        "         92,  94,  95,  96,  97,  99, 101, 102, 103, 105, 106, 107, 108,\n",
        "        110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 127,\n",
        "        128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 143, 144,\n",
        "        145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160,\n",
        "        161, 162, 164, 165, 167])\n",
        "# Criar a lista composta\n",
        "kfold_train = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "Cn4vhJ9LPncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as cinco arrays numpy\n",
        "array1 = np.array([  4,   6,   8,  18,  24,  29,  31,  38,  41,  42,  43,  51,  58, 62,  65,  67,  73,  74,  86,  88,  89,  90,  95, 101, 107, 111, 128, 133, 144, 147, 149, 152, 156, 162])\n",
        "array2 = np.array([  2,   9,  11,  12,  21,  28,  32,  33,  35,  39,  46,  48,  50, 56,  72,  78,  79,  81,  82,  85,  87,  91, 103, 106, 108, 131, 134, 140, 141, 146, 148, 158, 159, 164])\n",
        "array3 = np.array([  0,   3,  15,  16,  19,  26,  27,  34,  40,  49,  53,  60,  61, 64,  66,  68,  69,  83,  92,  94, 110, 115, 116, 117, 119, 125, 127, 130, 137, 143, 145, 154, 157, 167])\n",
        "array4 = np.array([  1,   5,   7,  13,  14,  17,  23,  30,  36,  44,  52,  54,  55, 59,  70,  77,  96,  97,  99, 102, 105, 113, 118, 120, 122, 126, 132, 136, 138, 151, 160, 161, 165])\n",
        "array5 = np.array([ 10,  20,  22,  25,  37,  45,  47,  57,  63,  71,  75,  76,  80,  84,  93,  98, 100, 104, 109, 112, 114, 121, 123, 124, 129, 135, 139, 142, 150, 153, 155, 163, 166])\n",
        "\n",
        "\n",
        "# Criar a lista composta\n",
        "kfold_test = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "c3Ia2J4DPpO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4aPPergNnL"
      },
      "source": [
        "### Bertimbau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl6AUm0K3yE"
      },
      "source": [
        "- https://huggingface.co/neuralmind/bert-base-portuguese-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RQrbYpK5vC"
      },
      "source": [
        "```\n",
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "11bbc028092243b5b664c8622a38aa45",
            "54f672bef8a74227b98ee6eab66d0d60",
            "958187b7be6f4bf8bdb6aa0c59a6501e",
            "15d0dd793a8f486c95c3ba9567adb45e",
            "94bb8460ba7f41bfb6f38ef84dfda09d",
            "6875f091ec88449a9b3379c800d88484",
            "0eabdc992813424fa4cc3bb0a4e56fbb",
            "8aa0fb3e128f4d37ba431ff108325984",
            "bfa96137f4064e79818af35293be2a44",
            "ad9f380c5a5f4c1c9add118e36338141",
            "b17a723ee30540ec99c503d3d2175416",
            "bf127e6e03194ab29f7313d7a43f6b25",
            "165ce79ff72d4c32b76cd638990508e4",
            "5b5d9f506c3b4427ac4f82c32ac4f90f",
            "311c89e536f84b3085a085674c4b24fc",
            "e8234769b9654b269f1265c15a8678d7",
            "d25701ff8f6d42f6850760f4d9e297b8",
            "4d29d7f7bf9542f0b8ba6bfb3d626088",
            "c9afd8d756f84ff98472aba708d8bb30",
            "7144420163e9468399eb0dcc7166ccd4",
            "a516dc1ee8f645118ff1054ca2cb4729",
            "5810bd601ee4496895b2f3113409e97d",
            "4c896f8135d84ac4afa93d89a86ccdf4",
            "4892d966442242ee952802a1153c5f25",
            "1d63cef4aec94f0495000c7200fe5d44",
            "0062821461a447b9890e80633bdede65",
            "be242364a45c439d9fbf88b51e41d813",
            "a42c8a66127249cca24b4f0c3e826695",
            "3d707a4764a34b82b96531ac19e4324c",
            "11fb11180bfb4936997b61224cfd2e75",
            "6542d912ea444bda96948b59c2981881",
            "6486ec6ea55844038ba4501c7d2c9360",
            "c6e7bb41f317449e905711f1cec2103f",
            "93373ff42e1b458c895c36f8a0096f44",
            "fd4dd7bad2754f429c2d0b9b8d5c430c",
            "40ca864b40574b028d7b2c45aff92135",
            "5753c15183fb4979bb56f56f2799b15f",
            "caa8a59c60104d278abef9b1e950fa7a",
            "a0a0853cd67444079c63a45feaa92855",
            "7373b065941b43bcb7fe9d7b37747d56",
            "197c1bbe866d4b16be2b45a152c9bd6c",
            "e3512765c81541d4939165dcac34c9b0",
            "7bdcaf32b3b64fe9b99bc88a66a0e847",
            "f2d1e71b219f45aa8be35e14d6c25af6",
            "a61a5df0bd464277a5c90f4ba72e1a6d",
            "7ab8fa807e39474d98abb0d5eb067840",
            "51ed0bc8fb094a2aa4ce78a759324b0b",
            "e1f210eb72ba435d83266e01efd5d2b4",
            "ee13579fb3ef448a84ce5456fbf568a0",
            "e46981efd0db476a8ed777e6f2c39cab",
            "098416e73df44bebaf4548626238a2f1",
            "d0b4845bb6524afd9d8d60f18a362062",
            "76fd8a70adb44136be07f7668e178ba0",
            "44dcaa94da404334aa08ee7219320713",
            "989bf017ad4e43c0b22f827b2f409b24"
          ]
        },
        "id": "WPj7c-IBgWRx",
        "outputId": "23054c1a-dd9c-4ab8-f865-b1f3e45fa915"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11bbc028092243b5b664c8622a38aa45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf127e6e03194ab29f7313d7a43f6b25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c896f8135d84ac4afa93d89a86ccdf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93373ff42e1b458c895c36f8a0096f44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a61a5df0bd464277a5c90f4ba72e1a6d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1299c395db4d424e9417d8823c594111",
            "77f4a2c6cffd4ece8944f96c6ce746c3",
            "e276f43acdbc4e1390a4dc739413878f",
            "b16deb42b6854aa3ab42475b39e96e9a",
            "d2df488b7c154cd58acc750f81ca9513",
            "6d58fa5ec2844260920d45d22e48eb66",
            "07f46cb149934d0292a737e93ce077ca",
            "64b6267348a546dbbb6f121f756248e3",
            "de1d54e68c5e47538352b07d3745439c",
            "1ffe84dd509441aa96568d10e06c5661",
            "7fc8adae98014c25a74be02ce7cd168e"
          ]
        },
        "id": "qSErznNMh4P5",
        "outputId": "787c85b9-5b4d-4b60-a537-082af08c89f0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1299c395db4d424e9417d8823c594111"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s_rPsFgWUr"
      },
      "outputs": [],
      "source": [
        "frase = 'A avaliação de prontidão tecnológica em tecnologias de interesse militar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJbGMogWaZ",
        "outputId": "488939d6-ef97-4cbe-d8c0-c7a5f0b6bc31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  frase,\n",
        "  max_length=20,   #Perceba que irei cortar um token da minha frase anterior\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fSHD8KgqDb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYjQrfrglJS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvfsVlnFglPp"
      },
      "outputs": [],
      "source": [
        "novo_df = df[[\"resumo\", \"rotulo\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nth2Ui-qg6Ix"
      },
      "outputs": [],
      "source": [
        "def adapto_faixa(faixa):\n",
        "    faixa -=1\n",
        "    return faixa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKj3lo8Qg9Vj",
        "outputId": "9218aa68-eb6b-44c4-cb6d-e7c337046672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-3dc243303e67>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)\n"
          ]
        }
      ],
      "source": [
        "novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_cANKWhOhCix",
        "outputId": "5f1341c4-1a6f-4306-9daa-a54efc3038c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                resumo  rotulo\n",
              "163  Rio de Janeiro (RJ) – O Centro de Avaliações d...       2\n",
              "164  Este trabalho apresenta um sistema para contro...       2\n",
              "165  No contexto das comunicações táticas baseadas ...       2\n",
              "166  O valor da velocidade de alvos móveis em image...       0\n",
              "167  Neste artigo é apresentada a análise da seção ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58a0d3db-e777-4597-ae3a-4f8481992817\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resumo</th>\n",
              "      <th>rotulo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Rio de Janeiro (RJ) – O Centro de Avaliações d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Este trabalho apresenta um sistema para contro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>No contexto das comunicações táticas baseadas ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>O valor da velocidade de alvos móveis em image...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Neste artigo é apresentada a análise da seção ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58a0d3db-e777-4597-ae3a-4f8481992817')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-58a0d3db-e777-4597-ae3a-4f8481992817 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-58a0d3db-e777-4597-ae3a-4f8481992817');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f84d74cd-2370-43c5-9a45-5484b3aa0c12\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f84d74cd-2370-43c5-9a45-5484b3aa0c12')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f84d74cd-2370-43c5-9a45-5484b3aa0c12 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "novo_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZK6EQ4xhFHI"
      },
      "outputs": [],
      "source": [
        "class_names = ['Faixa 1', 'Faixa 2', 'Faixa 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmol6gshNCR"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFoXmzH5hFOo"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      #padding='longest',\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K-OuLWzhYhB"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.resumo.to_numpy(),\n",
        "    targets=df.rotulo.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBrznGAWiJWg"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-yCbjniOLI",
        "outputId": "ffa4eadd-6666-4aa3-91d1-bcdd41b506ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxhW_89kRiv"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiZnSuuTiTOZ"
      },
      "outputs": [],
      "source": [
        "class TRLClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(TRLClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    #The last_hidden_state is a sequence of hidden states of the last layer of the model\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kX7ClUGiiCQ"
      },
      "outputs": [],
      "source": [
        "model = TRLClassifier(len(class_names)) #Pois tenho 3 classes\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXw5SIa0hkah"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTSemYwhnlI"
      },
      "source": [
        "### Separação e preparação do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lembrar que estamos utilizando o Kfold"
      ],
      "metadata": {
        "id": "z09fs6pHk5FG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLzT3ttq-0t"
      },
      "source": [
        "    - 63,7% treinamento - 16% validação                20% teste\n",
        "\n",
        "      ------------ 80%----------------------------------20%--"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o índice dos conjuntos\n",
        "index_df_train = kfold_train[particao]\n",
        "index_df_test = kfold_test[particao]"
      ],
      "metadata": {
        "id": "FeqFToMFnaMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = novo_df.iloc[index_df_train], novo_df.iloc[index_df_test]"
      ],
      "metadata": {
        "id": "EpR4mtHLoDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Bhg2qAhSlg"
      },
      "outputs": [],
      "source": [
        "#Separando o treinamento em treino e validação\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, stratify= df_train['rotulo'], random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FAkW0qhW2J",
        "outputId": "5cbc06b6-8423-47db-9a80-f93098e46f0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((107, 2), (27, 2), (34, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxinAAqhW5A",
        "outputId": "b1fc5a12-8226-43ad-d464-65ce413e9434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fsBv27hW8L",
        "outputId": "53d78ed5-add4-430b-d0b6-221aea208781"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbQEYoXhvVA",
        "outputId": "85da6077-fa09-4029-b24e-830cbbb2faf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7qyCztOhvX4",
        "outputId": "97223e91-96e4-4316-c465-bef092c0aaf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP6kiy4whvaa",
        "outputId": "4e6133d6-275f-439c-bf40-4205d173db4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzMSQeLlRHn"
      },
      "source": [
        "### Treinamento mesmo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oRhXpG_iBWB",
        "outputId": "eb3b99c7-d363-41a5-e713-8e41e236af77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8dJvCviFTI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKAqpPliFWJ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZisESWlkqH",
        "outputId": "066998e9-8a3d-4180-a402-142ecd5e6342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.8528080220733371 accuracy 0.616822429906542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6867160610854626 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7667747661471367 accuracy 0.719626168224299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6919980049133301 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.707172766327858 accuracy 0.719626168224299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5896043330430984 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6076374958668437 accuracy 0.7383177570093458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5652200467884541 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5794783362320491 accuracy 0.7570093457943925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6272863745689392 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3940060851829393 accuracy 0.822429906542056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9431546926498413 accuracy 0.48148148148148145\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.24248833475368364 accuracy 0.9065420560747662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.171468511223793 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.09722439698608858 accuracy 0.9813084112149532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4415316469967365 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.06921658335652735 accuracy 0.9719626168224298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.796772217261605 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.04502131513852094 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.0249787718057632 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.027331883337215652 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.407396823167801 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.003070485520376159 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.374574512243271 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.002063050417096487 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3224663585424423 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.001731727846033339 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.315301388502121 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0014536780488145138 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.307007133960724 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.001314623884224732 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.309820417314768 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0011697780781625105 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3183965422213078 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0010353703650512866 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.330511337146163 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0010060518231642032 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3399813743308187 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0009765119217003562 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3573601180687547 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0008931364240457437 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3737861327826977 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0008486150971813393 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.388831239193678 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000802706737886183 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4050235678441823 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007388853451370128 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4182288302108645 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007253613085984918 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.429103175876662 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007105108067792441 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4389871184248477 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006813086365582421 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.449049045331776 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006513216773912843 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4580864378949627 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005974829546175897 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4659979973221198 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006142057931616104 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4741458470234647 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005485876047584627 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.481665138504468 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006203606982515859 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4885406786343083 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005239857564447448 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.494415063469205 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005788267797990036 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.500390118919313 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006128743469681856 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.507636210648343 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005256536325240242 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.513685269339476 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005086733602053885 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5185430159326643 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00046343059511855245 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5226240043411963 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004760217645005988 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5264501583296806 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004956406919518486 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.529780746961478 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005089065442526979 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5331302986014634 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00048785271272728484 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5362467517261393 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004843050763676209 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.538745535945054 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004839987959712744 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5408820325974375 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00046940394011991363 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5426910018431954 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004447684415416526 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5440933832433075 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00047309926178838523 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5452083637355827 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005047216337905931 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.546047364012338 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004833465396326834 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5465715611935593 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004350189701654017 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5467521675745957 accuracy 0.5925925925925926\n",
            "\n",
            "CPU times: user 3min 9s, sys: 1min 21s, total: 4min 30s\n",
            "Wall time: 5min 19s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     torch.save(model.state_dict(), melhor_modelo)\n",
        "#     best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjGnJKDmZVJ"
      },
      "source": [
        "- Gráfico de desempenho no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQOy_NBBlks7"
      },
      "outputs": [],
      "source": [
        "A= history['train_acc']\n",
        "A2 = [tensor.item() for tensor in A]\n",
        "B= history['val_acc']\n",
        "B2 = [tensor.item() for tensor in B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "hHRH2aNQlkvy",
        "outputId": "5d567e4b-6e34-4c0f-b74e-435374e1929a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 1017,
              "height": 717
            }
          }
        }
      ],
      "source": [
        "plt.plot(A2, label='train accuracy')\n",
        "plt.plot(B2, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd9c9eGmecP"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jFRntT6mgoW",
        "outputId": "f79208cb-e113-448a-d55c-e3b89e81572b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7647058823529411"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBG2MIlgmkHv"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXTHRhD5mqnn",
        "outputId": "8b481d2b-99cd-4b6e-b146-44f44eb12ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTdojX6Jmqqm",
        "outputId": "5c35b0df-37f7-499e-a17e-ee77dd131959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Faixa 1       0.74      0.89      0.81        19\n",
            "     Faixa 2       0.60      0.38      0.46         8\n",
            "     Faixa 3       1.00      0.86      0.92         7\n",
            "\n",
            "    accuracy                           0.76        34\n",
            "   macro avg       0.78      0.71      0.73        34\n",
            "weighted avg       0.76      0.76      0.75        34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "HQ54xluu8RL3",
        "outputId": "b0a904f3-b7b2-4524-de77-41772b22f4a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 981,
              "height": 723
            }
          }
        }
      ],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('Faixa TRL verdadeira')\n",
        "  plt.xlabel('Faixa TRL predita');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZTajeJuiP_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11bbc028092243b5b664c8622a38aa45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54f672bef8a74227b98ee6eab66d0d60",
              "IPY_MODEL_958187b7be6f4bf8bdb6aa0c59a6501e",
              "IPY_MODEL_15d0dd793a8f486c95c3ba9567adb45e"
            ],
            "layout": "IPY_MODEL_94bb8460ba7f41bfb6f38ef84dfda09d"
          }
        },
        "54f672bef8a74227b98ee6eab66d0d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6875f091ec88449a9b3379c800d88484",
            "placeholder": "​",
            "style": "IPY_MODEL_0eabdc992813424fa4cc3bb0a4e56fbb",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "958187b7be6f4bf8bdb6aa0c59a6501e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa0fb3e128f4d37ba431ff108325984",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfa96137f4064e79818af35293be2a44",
            "value": 43
          }
        },
        "15d0dd793a8f486c95c3ba9567adb45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad9f380c5a5f4c1c9add118e36338141",
            "placeholder": "​",
            "style": "IPY_MODEL_b17a723ee30540ec99c503d3d2175416",
            "value": " 43.0/43.0 [00:00&lt;00:00, 1.03kB/s]"
          }
        },
        "94bb8460ba7f41bfb6f38ef84dfda09d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6875f091ec88449a9b3379c800d88484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eabdc992813424fa4cc3bb0a4e56fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aa0fb3e128f4d37ba431ff108325984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfa96137f4064e79818af35293be2a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad9f380c5a5f4c1c9add118e36338141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17a723ee30540ec99c503d3d2175416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf127e6e03194ab29f7313d7a43f6b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_165ce79ff72d4c32b76cd638990508e4",
              "IPY_MODEL_5b5d9f506c3b4427ac4f82c32ac4f90f",
              "IPY_MODEL_311c89e536f84b3085a085674c4b24fc"
            ],
            "layout": "IPY_MODEL_e8234769b9654b269f1265c15a8678d7"
          }
        },
        "165ce79ff72d4c32b76cd638990508e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d25701ff8f6d42f6850760f4d9e297b8",
            "placeholder": "​",
            "style": "IPY_MODEL_4d29d7f7bf9542f0b8ba6bfb3d626088",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "5b5d9f506c3b4427ac4f82c32ac4f90f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9afd8d756f84ff98472aba708d8bb30",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7144420163e9468399eb0dcc7166ccd4",
            "value": 209528
          }
        },
        "311c89e536f84b3085a085674c4b24fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a516dc1ee8f645118ff1054ca2cb4729",
            "placeholder": "​",
            "style": "IPY_MODEL_5810bd601ee4496895b2f3113409e97d",
            "value": " 210k/210k [00:00&lt;00:00, 3.30MB/s]"
          }
        },
        "e8234769b9654b269f1265c15a8678d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d25701ff8f6d42f6850760f4d9e297b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d29d7f7bf9542f0b8ba6bfb3d626088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9afd8d756f84ff98472aba708d8bb30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7144420163e9468399eb0dcc7166ccd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a516dc1ee8f645118ff1054ca2cb4729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5810bd601ee4496895b2f3113409e97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c896f8135d84ac4afa93d89a86ccdf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4892d966442242ee952802a1153c5f25",
              "IPY_MODEL_1d63cef4aec94f0495000c7200fe5d44",
              "IPY_MODEL_0062821461a447b9890e80633bdede65"
            ],
            "layout": "IPY_MODEL_be242364a45c439d9fbf88b51e41d813"
          }
        },
        "4892d966442242ee952802a1153c5f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a42c8a66127249cca24b4f0c3e826695",
            "placeholder": "​",
            "style": "IPY_MODEL_3d707a4764a34b82b96531ac19e4324c",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "1d63cef4aec94f0495000c7200fe5d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11fb11180bfb4936997b61224cfd2e75",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6542d912ea444bda96948b59c2981881",
            "value": 2
          }
        },
        "0062821461a447b9890e80633bdede65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6486ec6ea55844038ba4501c7d2c9360",
            "placeholder": "​",
            "style": "IPY_MODEL_c6e7bb41f317449e905711f1cec2103f",
            "value": " 2.00/2.00 [00:00&lt;00:00, 38.9B/s]"
          }
        },
        "be242364a45c439d9fbf88b51e41d813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a42c8a66127249cca24b4f0c3e826695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d707a4764a34b82b96531ac19e4324c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11fb11180bfb4936997b61224cfd2e75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6542d912ea444bda96948b59c2981881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6486ec6ea55844038ba4501c7d2c9360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e7bb41f317449e905711f1cec2103f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93373ff42e1b458c895c36f8a0096f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd4dd7bad2754f429c2d0b9b8d5c430c",
              "IPY_MODEL_40ca864b40574b028d7b2c45aff92135",
              "IPY_MODEL_5753c15183fb4979bb56f56f2799b15f"
            ],
            "layout": "IPY_MODEL_caa8a59c60104d278abef9b1e950fa7a"
          }
        },
        "fd4dd7bad2754f429c2d0b9b8d5c430c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0a0853cd67444079c63a45feaa92855",
            "placeholder": "​",
            "style": "IPY_MODEL_7373b065941b43bcb7fe9d7b37747d56",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "40ca864b40574b028d7b2c45aff92135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_197c1bbe866d4b16be2b45a152c9bd6c",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3512765c81541d4939165dcac34c9b0",
            "value": 112
          }
        },
        "5753c15183fb4979bb56f56f2799b15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bdcaf32b3b64fe9b99bc88a66a0e847",
            "placeholder": "​",
            "style": "IPY_MODEL_f2d1e71b219f45aa8be35e14d6c25af6",
            "value": " 112/112 [00:00&lt;00:00, 4.72kB/s]"
          }
        },
        "caa8a59c60104d278abef9b1e950fa7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0a0853cd67444079c63a45feaa92855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7373b065941b43bcb7fe9d7b37747d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "197c1bbe866d4b16be2b45a152c9bd6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3512765c81541d4939165dcac34c9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7bdcaf32b3b64fe9b99bc88a66a0e847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d1e71b219f45aa8be35e14d6c25af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a61a5df0bd464277a5c90f4ba72e1a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ab8fa807e39474d98abb0d5eb067840",
              "IPY_MODEL_51ed0bc8fb094a2aa4ce78a759324b0b",
              "IPY_MODEL_e1f210eb72ba435d83266e01efd5d2b4"
            ],
            "layout": "IPY_MODEL_ee13579fb3ef448a84ce5456fbf568a0"
          }
        },
        "7ab8fa807e39474d98abb0d5eb067840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e46981efd0db476a8ed777e6f2c39cab",
            "placeholder": "​",
            "style": "IPY_MODEL_098416e73df44bebaf4548626238a2f1",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "51ed0bc8fb094a2aa4ce78a759324b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0b4845bb6524afd9d8d60f18a362062",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76fd8a70adb44136be07f7668e178ba0",
            "value": 647
          }
        },
        "e1f210eb72ba435d83266e01efd5d2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44dcaa94da404334aa08ee7219320713",
            "placeholder": "​",
            "style": "IPY_MODEL_989bf017ad4e43c0b22f827b2f409b24",
            "value": " 647/647 [00:00&lt;00:00, 34.5kB/s]"
          }
        },
        "ee13579fb3ef448a84ce5456fbf568a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e46981efd0db476a8ed777e6f2c39cab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "098416e73df44bebaf4548626238a2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0b4845bb6524afd9d8d60f18a362062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76fd8a70adb44136be07f7668e178ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44dcaa94da404334aa08ee7219320713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "989bf017ad4e43c0b22f827b2f409b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1299c395db4d424e9417d8823c594111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77f4a2c6cffd4ece8944f96c6ce746c3",
              "IPY_MODEL_e276f43acdbc4e1390a4dc739413878f",
              "IPY_MODEL_b16deb42b6854aa3ab42475b39e96e9a"
            ],
            "layout": "IPY_MODEL_d2df488b7c154cd58acc750f81ca9513"
          }
        },
        "77f4a2c6cffd4ece8944f96c6ce746c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d58fa5ec2844260920d45d22e48eb66",
            "placeholder": "​",
            "style": "IPY_MODEL_07f46cb149934d0292a737e93ce077ca",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "e276f43acdbc4e1390a4dc739413878f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64b6267348a546dbbb6f121f756248e3",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de1d54e68c5e47538352b07d3745439c",
            "value": 438235074
          }
        },
        "b16deb42b6854aa3ab42475b39e96e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ffe84dd509441aa96568d10e06c5661",
            "placeholder": "​",
            "style": "IPY_MODEL_7fc8adae98014c25a74be02ce7cd168e",
            "value": " 438M/438M [00:01&lt;00:00, 250MB/s]"
          }
        },
        "d2df488b7c154cd58acc750f81ca9513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d58fa5ec2844260920d45d22e48eb66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f46cb149934d0292a737e93ce077ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64b6267348a546dbbb6f121f756248e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1d54e68c5e47538352b07d3745439c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ffe84dd509441aa96568d10e06c5661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc8adae98014c25a74be02ce7cd168e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}