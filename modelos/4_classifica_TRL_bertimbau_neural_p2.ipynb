{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jKuooZfNFU"
      },
      "source": [
        "# Classificação faixa TRL - Bertimbau + Rede Neural [kfold][P2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9-gf3IMD5C"
      },
      "source": [
        "- BERTimbau Base (aka \"bert-base-portuguese-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i41YvsyfSUR"
      },
      "source": [
        "## Código Versão 1.0 - 21 OUT 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfR22FfgbJ"
      },
      "source": [
        "- Uso dos classificadores: *Rede Neural*\n",
        "- **k-Fold cross-validation: Esse código executou a rodada 2**.\n",
        "- Matriz de confusão;\n",
        "- Semente 42\n",
        "- 50 épocas\n",
        "- BATCH_SIZE = 8\n",
        "- Limite 242 tokens\n",
        "- optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.csv   ou  dataset_pre_processado_1.csv  ou  dataset_pre_processado_stem_2.csv\n",
        "#     CSV1                  CSV2                                   CSV3\n",
        "dataset = \"dataset.csv\""
      ],
      "metadata": {
        "id": "vXMRttjhRQ2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lembre-se estamos usando o dataset: \" + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJkf84RR64K",
        "outputId": "ff7ba21b-d695-4554-b8d6-12b73f59d26b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lembre-se estamos usando o dataset: dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "particao=2  # O KFold apresenta 5 partições (rodadas)\n",
        "particao-=1  #Funcionará com o índice de kfold_train e kfold_test"
      ],
      "metadata": {
        "id": "k2TIOYIqQNps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melhor_modelo = 'best_model_TRL_bertimbau_base_neural_' + str(particao+1) + '.bin'\n",
        "melhor_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lFCPPlujqL5L",
        "outputId": "a1d0b446-302a-48bc-a0da-5b7adf57e44c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_model_TRL_bertimbau_base_neural_2.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 242"
      ],
      "metadata": {
        "id": "_6clpQcISMQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jlIjUzOgRY"
      },
      "source": [
        "- 63,7% treinamento - 16% validação...............20% teste\n",
        "- EPOCHS = 50\n",
        "- lr=2e-5\n",
        "- BATCH_SIZE = 8\n",
        "- Quantidade de tokens máxima = 242"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Msvyn0MfsOS"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC81VL_gQ-5"
      },
      "source": [
        "### Bibliotecas e ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZxPMZOfICS",
        "outputId": "8d969008-f1a3-4c35-9f33-df2ac2cf99f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5RDBcpVf0TS",
        "outputId": "24afdfae-7a18-42b4-f397-e17379c88ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmcqgWcf0WZ",
        "outputId": "13388f7f-73a0-481a-a60d-2530d3676bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 21 17:44:45 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoneZ3CngEDZ",
        "outputId": "20724506-5e93-43c4-cc21-9b60486dc68e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWHFe7ef0Zp",
        "outputId": "cbb9b54c-3c2a-4889-ac90-42f5b7e2d0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy       : 1.23.5\n",
            "pandas      : 1.5.3\n",
            "torch       : 2.1.0+cu118\n",
            "transformers: 4.34.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particoes KFOLD"
      ],
      "metadata": {
        "id": "oENoyHsUPjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para mais detalhes, consulte o código *1-kfold.ipynb* em que as partições foram sorteadas. As células a seguir recriam o resultado obtido."
      ],
      "metadata": {
        "id": "qL3FKCGlPtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = np.array([  0,   1,   2,   3,   5,   7,   9,  10,  11,  12,  13,  14,  15,\n",
        "         16,  17,  19,  20,  21,  22,  23,  25,  26,  27,  28,  30,  32,\n",
        "         33,  34,  35,  36,  37,  39,  40,  44,  45,  46,  47,  48,  49,\n",
        "         50,  52,  53,  54,  55,  56,  57,  59,  60,  61,  63,  64,  66,\n",
        "         68,  69,  70,  71,  72,  75,  76,  77,  78,  79,  80,  81,  82,\n",
        "         83,  84,  85,  87,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
        "        102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
        "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130,\n",
        "        131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145,\n",
        "        146, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
        "        164, 165, 166, 167])\n",
        "\n",
        "array2 = np.array([  0,   1,   3,   4,   5,   6,   7,   8,  10,  13,  14,  15,  16,\n",
        "         17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "         34,  36,  37,  38,  40,  41,  42,  43,  44,  45,  47,  49,  51,\n",
        "         52,  53,  54,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  70,  71,  73,  74,  75,  76,  77,  80,  83,\n",
        "         84,  86,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99,\n",
        "        100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115,\n",
        "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "        129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
        "        147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162,\n",
        "        163, 165, 166, 167])\n",
        "array3 = np.array([  1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
        "         17,  18,  20,  21,  22,  23,  24,  25,  28,  29,  30,  31,  32,\n",
        "         33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,\n",
        "         48,  50,  51,  52,  54,  55,  56,  57,  58,  59,  62,  63,  65,\n",
        "         67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "         82,  84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,\n",
        "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111,\n",
        "        112, 113, 114, 118, 120, 121, 122, 123, 124, 126, 128, 129, 131,\n",
        "        132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 146, 147,\n",
        "        148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162,\n",
        "        163, 164, 165, 166])\n",
        "array4 = np.array([  0,   2,   3,   4,   6,   8,   9,  10,  11,  12,  15,  16,  18,\n",
        "         19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  31,  32,  33,\n",
        "         34,  35,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
        "         49,  50,  51,  53,  56,  57,  58,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  71,  72,  73,  74,  75,  76,  78,  79,  80,\n",
        "         81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
        "         94,  95,  98, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111,\n",
        "        112, 114, 115, 116, 117, 119, 121, 123, 124, 125, 127, 128, 129,\n",
        "        130, 131, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145,\n",
        "        146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159,\n",
        "        162, 163, 164, 166, 167])\n",
        "array5 = np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
        "         14,  15,  16,  17,  18,  19,  21,  23,  24,  26,  27,  28,  29,\n",
        "         30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,  43,\n",
        "         44,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,\n",
        "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  72,  73,  74,\n",
        "         77,  78,  79,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
        "         92,  94,  95,  96,  97,  99, 101, 102, 103, 105, 106, 107, 108,\n",
        "        110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 127,\n",
        "        128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 143, 144,\n",
        "        145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160,\n",
        "        161, 162, 164, 165, 167])\n",
        "# Criar a lista composta\n",
        "kfold_train = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "Cn4vhJ9LPncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as cinco arrays numpy\n",
        "array1 = np.array([  4,   6,   8,  18,  24,  29,  31,  38,  41,  42,  43,  51,  58, 62,  65,  67,  73,  74,  86,  88,  89,  90,  95, 101, 107, 111, 128, 133, 144, 147, 149, 152, 156, 162])\n",
        "array2 = np.array([  2,   9,  11,  12,  21,  28,  32,  33,  35,  39,  46,  48,  50, 56,  72,  78,  79,  81,  82,  85,  87,  91, 103, 106, 108, 131, 134, 140, 141, 146, 148, 158, 159, 164])\n",
        "array3 = np.array([  0,   3,  15,  16,  19,  26,  27,  34,  40,  49,  53,  60,  61, 64,  66,  68,  69,  83,  92,  94, 110, 115, 116, 117, 119, 125, 127, 130, 137, 143, 145, 154, 157, 167])\n",
        "array4 = np.array([  1,   5,   7,  13,  14,  17,  23,  30,  36,  44,  52,  54,  55, 59,  70,  77,  96,  97,  99, 102, 105, 113, 118, 120, 122, 126, 132, 136, 138, 151, 160, 161, 165])\n",
        "array5 = np.array([ 10,  20,  22,  25,  37,  45,  47,  57,  63,  71,  75,  76,  80,  84,  93,  98, 100, 104, 109, 112, 114, 121, 123, 124, 129, 135, 139, 142, 150, 153, 155, 163, 166])\n",
        "\n",
        "\n",
        "# Criar a lista composta\n",
        "kfold_test = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "c3Ia2J4DPpO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4aPPergNnL"
      },
      "source": [
        "### Bertimbau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl6AUm0K3yE"
      },
      "source": [
        "- https://huggingface.co/neuralmind/bert-base-portuguese-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RQrbYpK5vC"
      },
      "source": [
        "```\n",
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "35730f9629b840938907a6f9ec695df6",
            "b694dbcb190843fc94ea42ef30d40186",
            "3ac3de84fea44868bbdabc9cb9bb915f",
            "b93731f8c8ea43759a9396a439827793",
            "8041233f1cff42c39cc1a75f2b4b3ab6",
            "899165cea6924292afb77ae8cd5b83c6",
            "ba68af6a671b49788ece949fac8477f8",
            "b56d04eb8fa84c9c8282a352f0f6dd05",
            "9fc434a3f7004649af1d821680ef7384",
            "652e9a7d642f4cbba7d5e6958cd88916",
            "c919386956e2456dafcdbd8197c3d383",
            "85ddd2c53bf84355b8928e01d007657b",
            "f7c1e330fc6341d38dc650390b5c965e",
            "f141c1a048cf43d5867cd07636c53c3e",
            "e3a126ca9d9c42468698877e47592207",
            "54fe4005ce5c49fc82246c4d59fcab8b",
            "6e406664dd8a4c56996b52f9a287843d",
            "0f1bf34aea2d40038c046d620b8a40b6",
            "1666a58574a643fdae50cf0534ec6040",
            "85f7f615f7fc43cd9da211b40556cf7f",
            "c627bfb4bc554051b966e4cc913826da",
            "7c8153ba89be40e99b705d3906305c75",
            "8974bfb0dd13431f84f9bfbe7da081c9",
            "df5da4436a654e57a3f7d935d154aabd",
            "d85153fb4f0040e18beb644862a853c4",
            "5630ffeebd634224a66591fff089ea8b",
            "f65dd7cfca7b48b9b4c19cb48c84a701",
            "364efc9b7b244da3b828ae3c6a48b5d2",
            "27535f3e2b9a4c96b667843809e9abb0",
            "9358b3680a8b46c2beaf9c9b72c80817",
            "3ba2ce0084a048d4bc046f04d435ec7f",
            "db94eb555ed64f77b182ce17c106eed4",
            "2311921088dd40ebaefc0a2c8f8a4459",
            "1541f4e706c245a7b3b9caaa735b658a",
            "f6fefdad18d248298fd259be006da265",
            "9974f14752ed4c4f89dd26a2e493fb66",
            "193a5b7e74e44cd197ce9a7ed651fa04",
            "2f43974ff2fc4ace9edfd1dd39578c87",
            "d40fe5eb275d470b9a968e88677ec99c",
            "005e5026fa764bfabb8ec19e14befc60",
            "18fa0e4d84304d0cb722c64222384a78",
            "9a9b55b5212a4f61a221940376a5bbeb",
            "08ce0ebf086043928732d67cf2b202cd",
            "f240714ef5f143bebdd6e804e80d55ce",
            "b6edebebebda4a71bef48d62c5f9a00a",
            "a55382c4e8704c019702298cbc986b06",
            "78340c90741f4fc89863e6efc18e8dbe",
            "2cdf03288c884cd4b808791be3be3232",
            "2e47a34817f24107883243f1c1889181",
            "9d7488f9b4e3496693e6aea62f420517",
            "c565dd104ce54681bce8001010b6fa44",
            "dd65b5d3b5d94b038f3885ae963bf060",
            "c4bf4c2b91634c258e04a160a42f8899",
            "02868689dfa643f4a0a6fb7d50035041",
            "a8aa2d8339a54b15a8129d93108ab487"
          ]
        },
        "id": "WPj7c-IBgWRx",
        "outputId": "cf4aa19e-f173-4d3b-d40e-f6927bd52af0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35730f9629b840938907a6f9ec695df6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85ddd2c53bf84355b8928e01d007657b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8974bfb0dd13431f84f9bfbe7da081c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1541f4e706c245a7b3b9caaa735b658a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6edebebebda4a71bef48d62c5f9a00a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5c9ecb9ed9af41f3a095e5497a8ab098",
            "6598299b0f7f4a38bcf3163c35e4d327",
            "51257e72aeb34c788729a857f6926678",
            "5611fafe04a443b4acc3f4368c42bbdd",
            "8315a8238ecf411b95f7581a8f7ea1f3",
            "aecc57febe614312883f380ac84dcbb5",
            "53c6f0e6c5a9457cbb981fb6cd35683c",
            "4a587cb15c5c40058bb45783ef0ba1dd",
            "fa26c481a75243f2b92150f4f644bf2a",
            "9b2a1cbd1a954737b99ae318a6fc7c2c",
            "19d72777d8014ab6b191309188da011b"
          ]
        },
        "id": "qSErznNMh4P5",
        "outputId": "0f53e433-1bd7-4a1d-d082-114dd6a32a7b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c9ecb9ed9af41f3a095e5497a8ab098"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s_rPsFgWUr"
      },
      "outputs": [],
      "source": [
        "frase = 'A avaliação de prontidão tecnológica em tecnologias de interesse militar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJbGMogWaZ",
        "outputId": "a4107956-2ff7-4337-bdda-5958a7b560d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  frase,\n",
        "  max_length=20,   #Perceba que irei cortar um token da minha frase anterior\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fSHD8KgqDb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYjQrfrglJS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvfsVlnFglPp"
      },
      "outputs": [],
      "source": [
        "novo_df = df[[\"resumo\", \"rotulo\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nth2Ui-qg6Ix"
      },
      "outputs": [],
      "source": [
        "def adapto_faixa(faixa):\n",
        "    faixa -=1\n",
        "    return faixa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKj3lo8Qg9Vj",
        "outputId": "513495c1-b9c9-4e70-def1-0d172e9d65be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3dc243303e67>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)\n"
          ]
        }
      ],
      "source": [
        "novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_cANKWhOhCix",
        "outputId": "5fd7b20f-a2bc-4df9-8f77-2b1e6013dc6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                resumo  rotulo\n",
              "163  Rio de Janeiro (RJ) – O Centro de Avaliações d...       2\n",
              "164  Este trabalho apresenta um sistema para contro...       2\n",
              "165  No contexto das comunicações táticas baseadas ...       2\n",
              "166  O valor da velocidade de alvos móveis em image...       0\n",
              "167  Neste artigo é apresentada a análise da seção ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-421217d7-50c1-4a5f-b686-02e505208bc8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resumo</th>\n",
              "      <th>rotulo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Rio de Janeiro (RJ) – O Centro de Avaliações d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Este trabalho apresenta um sistema para contro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>No contexto das comunicações táticas baseadas ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>O valor da velocidade de alvos móveis em image...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Neste artigo é apresentada a análise da seção ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-421217d7-50c1-4a5f-b686-02e505208bc8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-421217d7-50c1-4a5f-b686-02e505208bc8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-421217d7-50c1-4a5f-b686-02e505208bc8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7e2f1ce0-f72d-489e-b311-3b367d1389b4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e2f1ce0-f72d-489e-b311-3b367d1389b4')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7e2f1ce0-f72d-489e-b311-3b367d1389b4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "novo_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZK6EQ4xhFHI"
      },
      "outputs": [],
      "source": [
        "class_names = ['Faixa 1', 'Faixa 2', 'Faixa 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmol6gshNCR"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFoXmzH5hFOo"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      #padding='longest',\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K-OuLWzhYhB"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.resumo.to_numpy(),\n",
        "    targets=df.rotulo.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBrznGAWiJWg"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-yCbjniOLI",
        "outputId": "b4e3f303-9ba6-4a37-88b4-155a85c64b89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxhW_89kRiv"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiZnSuuTiTOZ"
      },
      "outputs": [],
      "source": [
        "class TRLClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(TRLClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    #The last_hidden_state is a sequence of hidden states of the last layer of the model\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kX7ClUGiiCQ"
      },
      "outputs": [],
      "source": [
        "model = TRLClassifier(len(class_names)) #Pois tenho 3 classes\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXw5SIa0hkah"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTSemYwhnlI"
      },
      "source": [
        "### Separação e preparação do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lembrar que estamos utilizando o Kfold"
      ],
      "metadata": {
        "id": "z09fs6pHk5FG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLzT3ttq-0t"
      },
      "source": [
        "    - 63,7% treinamento - 16% validação                20% teste\n",
        "\n",
        "      ------------ 80%----------------------------------20%--"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o índice dos conjuntos\n",
        "index_df_train = kfold_train[particao]\n",
        "index_df_test = kfold_test[particao]"
      ],
      "metadata": {
        "id": "FeqFToMFnaMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = novo_df.iloc[index_df_train], novo_df.iloc[index_df_test]"
      ],
      "metadata": {
        "id": "EpR4mtHLoDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Bhg2qAhSlg"
      },
      "outputs": [],
      "source": [
        "#Separando o treinamento em treino e validação\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, stratify= df_train['rotulo'], random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FAkW0qhW2J",
        "outputId": "ead7afcf-01c1-47e8-c2ab-111411b3cfe5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((107, 2), (27, 2), (34, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxinAAqhW5A",
        "outputId": "6d1bdec7-c102-485f-acbc-4412d4004bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fsBv27hW8L",
        "outputId": "b3446972-8de4-4c58-c7e9-40e16f6cc937"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbQEYoXhvVA",
        "outputId": "f62e5a46-f127-4d2d-c6dd-2e07005c53e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7qyCztOhvX4",
        "outputId": "c0ebf69e-5b98-4cf6-de40-65c3bdbbc099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP6kiy4whvaa",
        "outputId": "c9dc61ac-f058-4c0c-dad4-2b845fb6895d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzMSQeLlRHn"
      },
      "source": [
        "### Treinamento mesmo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oRhXpG_iBWB",
        "outputId": "91c5248d-8775-4006-915e-cc88bf33c48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8dJvCviFTI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKAqpPliFWJ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZisESWlkqH",
        "outputId": "38e96885-571c-40a8-912a-98f412254583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.945504925080708 accuracy 0.5607476635514018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6475046053528786 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7559278181621006 accuracy 0.719626168224299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5790888071060181 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6948541785989489 accuracy 0.719626168224299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5627795606851578 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7117579472916467 accuracy 0.6915887850467289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.719423919916153 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5404581140194621 accuracy 0.7757009345794392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5805580168962479 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.36107257327863146 accuracy 0.8785046728971962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.455632820725441 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.24665575647460564 accuracy 0.9439252336448597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5211080387234688 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.13308764551766217 accuracy 0.9719626168224298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7445288203889504 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.053785220437150984 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7473208447918296 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.005247026299392539 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9210892524570227 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0024957843374327888 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9661032296717167 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0020267232736971763 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0312139503657818 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.001624031857188259 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0737030487507582 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0013484316295944154 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.096935087814927 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0012170329324102827 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.107182390987873 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.001087446343652638 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1201471649110317 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0010028039229967231 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.134150579571724 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0008918999062318887 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1470672823488712 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0008836923017432648 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1584165841341019 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0008062907103781722 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1692985333502293 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007816466220122363 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1790700815618038 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007520149021209883 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.187501572072506 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000699217790887425 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1945950761437416 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006488201823750776 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.201206911355257 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006835802861522618 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2079170942306519 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006296409098597776 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2121704071760178 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005965152480972133 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2179651074111462 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005740552088744673 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2231171429157257 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000532003102957138 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.228187020868063 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005777178323894207 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2332770973443985 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004984446838664423 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2378727309405804 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005661801593045571 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2421751506626606 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004884880992384362 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2466282360255718 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005117887319231938 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2502563036978245 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004976471931773371 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2534270733594894 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00048426338097280156 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2561943531036377 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00045494979713112116 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2589300014078617 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00043080539243029695 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2615287490189075 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00044646728825422803 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2637523896992207 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00045559261343441904 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2655506692826748 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004840042446241049 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2674596011638641 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004661112346054454 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2694025188684464 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00042525477640862973 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2709875516593456 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00043698417513431717 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2723449058830738 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004370553984439799 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.27348068729043 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004304724965809977 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2743735015392303 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00042716603534894863 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2751272656023502 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004661513604722651 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.275631833821535 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00043647454211687934 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.275941800326109 accuracy 0.8148148148148148\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004016639993226688 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.2760531418025494 accuracy 0.8148148148148148\n",
            "\n",
            "CPU times: user 2min 47s, sys: 1min 16s, total: 4min 3s\n",
            "Wall time: 5min 2s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     torch.save(model.state_dict(), melhor_modelo)\n",
        "#     best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjGnJKDmZVJ"
      },
      "source": [
        "- Gráfico de desempenho no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQOy_NBBlks7"
      },
      "outputs": [],
      "source": [
        "A= history['train_acc']\n",
        "A2 = [tensor.item() for tensor in A]\n",
        "B= history['val_acc']\n",
        "B2 = [tensor.item() for tensor in B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "hHRH2aNQlkvy",
        "outputId": "857073d8-8286-45e6-c59c-41eb0f9a66a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 1017,
              "height": 717
            }
          }
        }
      ],
      "source": [
        "plt.plot(A2, label='train accuracy')\n",
        "plt.plot(B2, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd9c9eGmecP"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jFRntT6mgoW",
        "outputId": "a012a821-caf6-4ffd-ad9c-ed295c734089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6764705882352942"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBG2MIlgmkHv"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXTHRhD5mqnn",
        "outputId": "1c07c2fc-9cf9-4331-8c3d-e7595ba0c27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTdojX6Jmqqm",
        "outputId": "85dd6cc1-9c7f-4287-fa28-cd05413ddd02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Faixa 1       0.67      0.84      0.74        19\n",
            "     Faixa 2       0.33      0.12      0.18         8\n",
            "     Faixa 3       0.86      0.86      0.86         7\n",
            "\n",
            "    accuracy                           0.68        34\n",
            "   macro avg       0.62      0.61      0.59        34\n",
            "weighted avg       0.63      0.68      0.64        34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "HQ54xluu8RL3",
        "outputId": "19373608-9682-4257-fd1b-a64ddfb875de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 981,
              "height": 730
            }
          }
        }
      ],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('Faixa TRL verdadeira')\n",
        "  plt.xlabel('Faixa TRL predita');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZTajeJuiP_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35730f9629b840938907a6f9ec695df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b694dbcb190843fc94ea42ef30d40186",
              "IPY_MODEL_3ac3de84fea44868bbdabc9cb9bb915f",
              "IPY_MODEL_b93731f8c8ea43759a9396a439827793"
            ],
            "layout": "IPY_MODEL_8041233f1cff42c39cc1a75f2b4b3ab6"
          }
        },
        "b694dbcb190843fc94ea42ef30d40186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_899165cea6924292afb77ae8cd5b83c6",
            "placeholder": "​",
            "style": "IPY_MODEL_ba68af6a671b49788ece949fac8477f8",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "3ac3de84fea44868bbdabc9cb9bb915f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b56d04eb8fa84c9c8282a352f0f6dd05",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fc434a3f7004649af1d821680ef7384",
            "value": 43
          }
        },
        "b93731f8c8ea43759a9396a439827793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_652e9a7d642f4cbba7d5e6958cd88916",
            "placeholder": "​",
            "style": "IPY_MODEL_c919386956e2456dafcdbd8197c3d383",
            "value": " 43.0/43.0 [00:00&lt;00:00, 1.08kB/s]"
          }
        },
        "8041233f1cff42c39cc1a75f2b4b3ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "899165cea6924292afb77ae8cd5b83c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba68af6a671b49788ece949fac8477f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b56d04eb8fa84c9c8282a352f0f6dd05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fc434a3f7004649af1d821680ef7384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "652e9a7d642f4cbba7d5e6958cd88916": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c919386956e2456dafcdbd8197c3d383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85ddd2c53bf84355b8928e01d007657b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7c1e330fc6341d38dc650390b5c965e",
              "IPY_MODEL_f141c1a048cf43d5867cd07636c53c3e",
              "IPY_MODEL_e3a126ca9d9c42468698877e47592207"
            ],
            "layout": "IPY_MODEL_54fe4005ce5c49fc82246c4d59fcab8b"
          }
        },
        "f7c1e330fc6341d38dc650390b5c965e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e406664dd8a4c56996b52f9a287843d",
            "placeholder": "​",
            "style": "IPY_MODEL_0f1bf34aea2d40038c046d620b8a40b6",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "f141c1a048cf43d5867cd07636c53c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1666a58574a643fdae50cf0534ec6040",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85f7f615f7fc43cd9da211b40556cf7f",
            "value": 209528
          }
        },
        "e3a126ca9d9c42468698877e47592207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c627bfb4bc554051b966e4cc913826da",
            "placeholder": "​",
            "style": "IPY_MODEL_7c8153ba89be40e99b705d3906305c75",
            "value": " 210k/210k [00:00&lt;00:00, 6.41MB/s]"
          }
        },
        "54fe4005ce5c49fc82246c4d59fcab8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e406664dd8a4c56996b52f9a287843d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f1bf34aea2d40038c046d620b8a40b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1666a58574a643fdae50cf0534ec6040": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f7f615f7fc43cd9da211b40556cf7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c627bfb4bc554051b966e4cc913826da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c8153ba89be40e99b705d3906305c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8974bfb0dd13431f84f9bfbe7da081c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df5da4436a654e57a3f7d935d154aabd",
              "IPY_MODEL_d85153fb4f0040e18beb644862a853c4",
              "IPY_MODEL_5630ffeebd634224a66591fff089ea8b"
            ],
            "layout": "IPY_MODEL_f65dd7cfca7b48b9b4c19cb48c84a701"
          }
        },
        "df5da4436a654e57a3f7d935d154aabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_364efc9b7b244da3b828ae3c6a48b5d2",
            "placeholder": "​",
            "style": "IPY_MODEL_27535f3e2b9a4c96b667843809e9abb0",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "d85153fb4f0040e18beb644862a853c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9358b3680a8b46c2beaf9c9b72c80817",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ba2ce0084a048d4bc046f04d435ec7f",
            "value": 2
          }
        },
        "5630ffeebd634224a66591fff089ea8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db94eb555ed64f77b182ce17c106eed4",
            "placeholder": "​",
            "style": "IPY_MODEL_2311921088dd40ebaefc0a2c8f8a4459",
            "value": " 2.00/2.00 [00:00&lt;00:00, 116B/s]"
          }
        },
        "f65dd7cfca7b48b9b4c19cb48c84a701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "364efc9b7b244da3b828ae3c6a48b5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27535f3e2b9a4c96b667843809e9abb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9358b3680a8b46c2beaf9c9b72c80817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba2ce0084a048d4bc046f04d435ec7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db94eb555ed64f77b182ce17c106eed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2311921088dd40ebaefc0a2c8f8a4459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1541f4e706c245a7b3b9caaa735b658a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6fefdad18d248298fd259be006da265",
              "IPY_MODEL_9974f14752ed4c4f89dd26a2e493fb66",
              "IPY_MODEL_193a5b7e74e44cd197ce9a7ed651fa04"
            ],
            "layout": "IPY_MODEL_2f43974ff2fc4ace9edfd1dd39578c87"
          }
        },
        "f6fefdad18d248298fd259be006da265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d40fe5eb275d470b9a968e88677ec99c",
            "placeholder": "​",
            "style": "IPY_MODEL_005e5026fa764bfabb8ec19e14befc60",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "9974f14752ed4c4f89dd26a2e493fb66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18fa0e4d84304d0cb722c64222384a78",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a9b55b5212a4f61a221940376a5bbeb",
            "value": 112
          }
        },
        "193a5b7e74e44cd197ce9a7ed651fa04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08ce0ebf086043928732d67cf2b202cd",
            "placeholder": "​",
            "style": "IPY_MODEL_f240714ef5f143bebdd6e804e80d55ce",
            "value": " 112/112 [00:00&lt;00:00, 8.20kB/s]"
          }
        },
        "2f43974ff2fc4ace9edfd1dd39578c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d40fe5eb275d470b9a968e88677ec99c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "005e5026fa764bfabb8ec19e14befc60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18fa0e4d84304d0cb722c64222384a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a9b55b5212a4f61a221940376a5bbeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08ce0ebf086043928732d67cf2b202cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f240714ef5f143bebdd6e804e80d55ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6edebebebda4a71bef48d62c5f9a00a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a55382c4e8704c019702298cbc986b06",
              "IPY_MODEL_78340c90741f4fc89863e6efc18e8dbe",
              "IPY_MODEL_2cdf03288c884cd4b808791be3be3232"
            ],
            "layout": "IPY_MODEL_2e47a34817f24107883243f1c1889181"
          }
        },
        "a55382c4e8704c019702298cbc986b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d7488f9b4e3496693e6aea62f420517",
            "placeholder": "​",
            "style": "IPY_MODEL_c565dd104ce54681bce8001010b6fa44",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "78340c90741f4fc89863e6efc18e8dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd65b5d3b5d94b038f3885ae963bf060",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4bf4c2b91634c258e04a160a42f8899",
            "value": 647
          }
        },
        "2cdf03288c884cd4b808791be3be3232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02868689dfa643f4a0a6fb7d50035041",
            "placeholder": "​",
            "style": "IPY_MODEL_a8aa2d8339a54b15a8129d93108ab487",
            "value": " 647/647 [00:00&lt;00:00, 36.7kB/s]"
          }
        },
        "2e47a34817f24107883243f1c1889181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d7488f9b4e3496693e6aea62f420517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c565dd104ce54681bce8001010b6fa44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd65b5d3b5d94b038f3885ae963bf060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4bf4c2b91634c258e04a160a42f8899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02868689dfa643f4a0a6fb7d50035041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8aa2d8339a54b15a8129d93108ab487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c9ecb9ed9af41f3a095e5497a8ab098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6598299b0f7f4a38bcf3163c35e4d327",
              "IPY_MODEL_51257e72aeb34c788729a857f6926678",
              "IPY_MODEL_5611fafe04a443b4acc3f4368c42bbdd"
            ],
            "layout": "IPY_MODEL_8315a8238ecf411b95f7581a8f7ea1f3"
          }
        },
        "6598299b0f7f4a38bcf3163c35e4d327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aecc57febe614312883f380ac84dcbb5",
            "placeholder": "​",
            "style": "IPY_MODEL_53c6f0e6c5a9457cbb981fb6cd35683c",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "51257e72aeb34c788729a857f6926678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a587cb15c5c40058bb45783ef0ba1dd",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa26c481a75243f2b92150f4f644bf2a",
            "value": 438235074
          }
        },
        "5611fafe04a443b4acc3f4368c42bbdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b2a1cbd1a954737b99ae318a6fc7c2c",
            "placeholder": "​",
            "style": "IPY_MODEL_19d72777d8014ab6b191309188da011b",
            "value": " 438M/438M [00:01&lt;00:00, 249MB/s]"
          }
        },
        "8315a8238ecf411b95f7581a8f7ea1f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aecc57febe614312883f380ac84dcbb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53c6f0e6c5a9457cbb981fb6cd35683c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a587cb15c5c40058bb45783ef0ba1dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa26c481a75243f2b92150f4f644bf2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b2a1cbd1a954737b99ae318a6fc7c2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d72777d8014ab6b191309188da011b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}