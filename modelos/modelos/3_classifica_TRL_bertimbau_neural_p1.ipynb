{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jKuooZfNFU"
      },
      "source": [
        "# Classificação faixa TRL - Bertimbau + Rede Neural [kfold][P1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9-gf3IMD5C"
      },
      "source": [
        "- BERTimbau Base (aka \"bert-base-portuguese-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i41YvsyfSUR"
      },
      "source": [
        "## Código Versão 1.0 - 21 OUT 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfR22FfgbJ"
      },
      "source": [
        "- Uso dos classificadores: *Rede Neural*\n",
        "- **k-Fold cross-validation: Esse código executou a rodada 1**.\n",
        "- Matriz de confusão;\n",
        "- Semente 42\n",
        "- 50 épocas\n",
        "- BATCH_SIZE = 8\n",
        "- Limite 242 tokens\n",
        "- optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.csv   ou  dataset_pre_processado_1.csv  ou  dataset_pre_processado_stem_2.csv\n",
        "#     CSV1                  CSV2                                   CSV3\n",
        "dataset = \"dataset.csv\""
      ],
      "metadata": {
        "id": "vXMRttjhRQ2z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lembre-se estamos usando o dataset: \" + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJkf84RR64K",
        "outputId": "b2a3644c-b389-404d-e4ee-ffd1c88b4bb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lembre-se estamos usando o dataset: dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "particao=1  # O KFold apresenta 5 partições (rodadas)\n",
        "particao-=1  #Funcionará com o índice de kfold_train e kfold_test"
      ],
      "metadata": {
        "id": "k2TIOYIqQNps"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melhor_modelo = 'best_model_TRL_bertimbau_base_neural_' + str(particao+1) + '.bin'\n",
        "melhor_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lFCPPlujqL5L",
        "outputId": "9f80c54c-c958-4736-d222-9798e263e528"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_model_TRL_bertimbau_base_neural_1.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 242"
      ],
      "metadata": {
        "id": "_6clpQcISMQV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jlIjUzOgRY"
      },
      "source": [
        "- 63,7% treinamento - 16% validação...............20% teste\n",
        "- EPOCHS = 50\n",
        "- lr=2e-5\n",
        "- BATCH_SIZE = 8\n",
        "- Quantidade de tokens máxima = 242"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Msvyn0MfsOS"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC81VL_gQ-5"
      },
      "source": [
        "### Bibliotecas e ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZxPMZOfICS",
        "outputId": "bd7cd96d-dd27-4bff-e049-7637cf5fce94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5RDBcpVf0TS",
        "outputId": "c4b37513-8d7f-4cc2-8170-b0f188662448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.4/1.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmcqgWcf0WZ",
        "outputId": "86e1c8b9-eecd-45f4-e7a7-f34ffe445af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 23 12:58:05 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoneZ3CngEDZ",
        "outputId": "6a97ce15-e998-4188-b504-d494cbf6c52b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWHFe7ef0Zp",
        "outputId": "232a2bed-2ccb-4db2-984d-675b5bca0e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy       : 1.23.5\n",
            "pandas      : 1.5.3\n",
            "torch       : 2.1.0+cu118\n",
            "transformers: 4.34.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particoes KFOLD"
      ],
      "metadata": {
        "id": "oENoyHsUPjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para mais detalhes, consulte o código *1-kfold.ipynb* em que as partições foram sorteadas. As células a seguir recriam o resultado obtido."
      ],
      "metadata": {
        "id": "qL3FKCGlPtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = np.array([  0,   1,   2,   3,   5,   7,   9,  10,  11,  12,  13,  14,  15,\n",
        "         16,  17,  19,  20,  21,  22,  23,  25,  26,  27,  28,  30,  32,\n",
        "         33,  34,  35,  36,  37,  39,  40,  44,  45,  46,  47,  48,  49,\n",
        "         50,  52,  53,  54,  55,  56,  57,  59,  60,  61,  63,  64,  66,\n",
        "         68,  69,  70,  71,  72,  75,  76,  77,  78,  79,  80,  81,  82,\n",
        "         83,  84,  85,  87,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
        "        102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
        "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130,\n",
        "        131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145,\n",
        "        146, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
        "        164, 165, 166, 167])\n",
        "\n",
        "array2 = np.array([  0,   1,   3,   4,   5,   6,   7,   8,  10,  13,  14,  15,  16,\n",
        "         17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "         34,  36,  37,  38,  40,  41,  42,  43,  44,  45,  47,  49,  51,\n",
        "         52,  53,  54,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  70,  71,  73,  74,  75,  76,  77,  80,  83,\n",
        "         84,  86,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99,\n",
        "        100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115,\n",
        "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "        129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
        "        147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162,\n",
        "        163, 165, 166, 167])\n",
        "array3 = np.array([  1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
        "         17,  18,  20,  21,  22,  23,  24,  25,  28,  29,  30,  31,  32,\n",
        "         33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,\n",
        "         48,  50,  51,  52,  54,  55,  56,  57,  58,  59,  62,  63,  65,\n",
        "         67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "         82,  84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,\n",
        "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111,\n",
        "        112, 113, 114, 118, 120, 121, 122, 123, 124, 126, 128, 129, 131,\n",
        "        132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 146, 147,\n",
        "        148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162,\n",
        "        163, 164, 165, 166])\n",
        "array4 = np.array([  0,   2,   3,   4,   6,   8,   9,  10,  11,  12,  15,  16,  18,\n",
        "         19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  31,  32,  33,\n",
        "         34,  35,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
        "         49,  50,  51,  53,  56,  57,  58,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  71,  72,  73,  74,  75,  76,  78,  79,  80,\n",
        "         81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
        "         94,  95,  98, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111,\n",
        "        112, 114, 115, 116, 117, 119, 121, 123, 124, 125, 127, 128, 129,\n",
        "        130, 131, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145,\n",
        "        146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159,\n",
        "        162, 163, 164, 166, 167])\n",
        "array5 = np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
        "         14,  15,  16,  17,  18,  19,  21,  23,  24,  26,  27,  28,  29,\n",
        "         30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,  43,\n",
        "         44,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,\n",
        "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  72,  73,  74,\n",
        "         77,  78,  79,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
        "         92,  94,  95,  96,  97,  99, 101, 102, 103, 105, 106, 107, 108,\n",
        "        110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 127,\n",
        "        128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 143, 144,\n",
        "        145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160,\n",
        "        161, 162, 164, 165, 167])\n",
        "# Criar a lista composta\n",
        "kfold_train = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "Cn4vhJ9LPncE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as cinco arrays numpy\n",
        "array1 = np.array([  4,   6,   8,  18,  24,  29,  31,  38,  41,  42,  43,  51,  58, 62,  65,  67,  73,  74,  86,  88,  89,  90,  95, 101, 107, 111, 128, 133, 144, 147, 149, 152, 156, 162])\n",
        "array2 = np.array([  2,   9,  11,  12,  21,  28,  32,  33,  35,  39,  46,  48,  50, 56,  72,  78,  79,  81,  82,  85,  87,  91, 103, 106, 108, 131, 134, 140, 141, 146, 148, 158, 159, 164])\n",
        "array3 = np.array([  0,   3,  15,  16,  19,  26,  27,  34,  40,  49,  53,  60,  61, 64,  66,  68,  69,  83,  92,  94, 110, 115, 116, 117, 119, 125, 127, 130, 137, 143, 145, 154, 157, 167])\n",
        "array4 = np.array([  1,   5,   7,  13,  14,  17,  23,  30,  36,  44,  52,  54,  55, 59,  70,  77,  96,  97,  99, 102, 105, 113, 118, 120, 122, 126, 132, 136, 138, 151, 160, 161, 165])\n",
        "array5 = np.array([ 10,  20,  22,  25,  37,  45,  47,  57,  63,  71,  75,  76,  80,  84,  93,  98, 100, 104, 109, 112, 114, 121, 123, 124, 129, 135, 139, 142, 150, 153, 155, 163, 166])\n",
        "\n",
        "\n",
        "# Criar a lista composta\n",
        "kfold_test = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "c3Ia2J4DPpO8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4aPPergNnL"
      },
      "source": [
        "### Bertimbau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl6AUm0K3yE"
      },
      "source": [
        "- https://huggingface.co/neuralmind/bert-base-portuguese-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RQrbYpK5vC"
      },
      "source": [
        "```\n",
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "9dc0277146c14409b098431936ca6466",
            "d43ad8ddc7964bfb860b397c26599045",
            "e8eaa91d87ec44ac901461252d529de4",
            "77dfec1b5066419c995358a072ec2005",
            "625b56dbd67743e18481cdde92482806",
            "e8be4828eb9c48339c15cebb5a5ae3a1",
            "a8aeb4384ed142a69231b074b563eedf",
            "222ba7aa142747508cde2fcdadd3fbeb",
            "2c3def4fd7ad43e3ac2c3c56a2be542a",
            "eab5341711d94ef7a369d825f8d1b53d",
            "84b98293684e4a62af013ad460a9fc83",
            "88def2ef2aee47e984e60c7e50c32d2a",
            "6b49d90d57874d72a6b16778854a0569",
            "25d1c44930e846bb98df1a2027e43506",
            "3e94c6a8c95d48719e5c030a4e518147",
            "5ebae6c5c87641f4ab3553c0b725d173",
            "446b7117e387491b9b562ced826e01d6",
            "364469d681f644c0943c307758b599e3",
            "df99bf0570c542398b1b98758934edee",
            "03374da1f5d34c89a9345953a6a9788a",
            "32c93260409346648e7c139a19a3408f",
            "b784892dfdb2447589fc7ffc6dd50ea4",
            "740d4c81306e4bed9ca377bc6dfba51e",
            "d46852284a1a441e89865847564708fa",
            "2f6471e04efc4ee1853934e09b5fb2e2",
            "c38503cb031f422e8893b41f159637cd",
            "839c14a342834c41ba4defca1ac8e009",
            "3074b89317684612a5419a373b3eabdf",
            "6f3f6f9425144a9ba5cad49c85ebd92b",
            "c713f10c89e04f68ab1fed89e807f12e",
            "d4b83420316846e494760c2df31a58a2",
            "1b0902a8b6eb4226a95acadd5b37082c",
            "4369d52fde0a4570a41d52a662bd2166",
            "fb2f19566ec5480b941189b78d15e2ba",
            "13a1b514b2cf455fa3bf7d25e10a889e",
            "6c873e755ceb451eb2b42ad54512c5a6",
            "36f10e7d1c2a4d79b79b30c200acd58e",
            "f6917514757f4e1b86a275f80cbde8d6",
            "e9d33263d1e9450dbbdb0660ea1e0628",
            "dd85408e3dfc434ca1638b4495e53690",
            "f31cbf81b7ed4e4c9790162a100a832d",
            "cad6c537adc94a0c9c4375b5a6aba516",
            "f9fa38ae325646aa890ca0a654ce737c",
            "2095e023d1974069baefdb700fc3e51f",
            "e50b6ab989dc44a7af102b2dbfccfa86",
            "9584a125787e4e3c8d728457cbd3589d",
            "c8cbb6a6f4e74a4c92a491c6f5ae8e50",
            "f490afd9208c4a85bd8e6b2f139766a4",
            "5b24353c16744d4cb4eb59b88a829579",
            "09e42a98993b4363961e94d77010c5e9",
            "77bad87ea27d4ef4b87db94a36e02ebc",
            "af8cfd0c0ca14e34af44587352f1c873",
            "ec0e9e0ec94e4d18bc2ace64e6241087",
            "72f4a74315eb459f851ae401abd77666",
            "53ad859ba4c84a5fbaeb0ad628753cff"
          ]
        },
        "id": "WPj7c-IBgWRx",
        "outputId": "a2b94213-47bb-4f2a-ac46-00bbbd7071eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dc0277146c14409b098431936ca6466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88def2ef2aee47e984e60c7e50c32d2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "740d4c81306e4bed9ca377bc6dfba51e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb2f19566ec5480b941189b78d15e2ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e50b6ab989dc44a7af102b2dbfccfa86"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "759e371e234f450badcd68e07f8f8267",
            "5462ca9cfcbe4cf4aed733ba711b2b91",
            "6bb04d6ccd79441a9f7fa89fc0538566",
            "643d8c890db24eada71b18b716b8b46b",
            "c2313de4a48e405aa5053d8bf7836897",
            "cc38b0257ad5480eaefbfb287108e259",
            "8450943f881943a2afc016987af3b8ad",
            "c6a8db41085d42a2a8f017f491a1ff9f",
            "3aa9c35c898543e7823bee3f08438b34",
            "0685d5c36c4b479380b6aeabc0b03362",
            "095743efb047431c9bdeffcdf848882e"
          ]
        },
        "id": "qSErznNMh4P5",
        "outputId": "4edb0ff3-da88-440f-d2b7-19735677faae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "759e371e234f450badcd68e07f8f8267"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y6s_rPsFgWUr"
      },
      "outputs": [],
      "source": [
        "frase = 'A avaliação de prontidão tecnológica em tecnologias de interesse militar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJbGMogWaZ",
        "outputId": "6bd1d386-aa55-46c3-9bb6-50e8063c8bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  frase,\n",
        "  max_length=20,   #Perceba que irei cortar um token da minha frase anterior\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fSHD8KgqDb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0bYjQrfrglJS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LvfsVlnFglPp"
      },
      "outputs": [],
      "source": [
        "novo_df = df[[\"resumo\", \"rotulo\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Nth2Ui-qg6Ix"
      },
      "outputs": [],
      "source": [
        "def adapto_faixa(faixa):\n",
        "    faixa -=1\n",
        "    return faixa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKj3lo8Qg9Vj",
        "outputId": "a1b13cbb-7562-4c60-c2b2-e15a8d3d7645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-3dc243303e67>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)\n"
          ]
        }
      ],
      "source": [
        "novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_cANKWhOhCix",
        "outputId": "3368862f-ddd3-4558-d9a3-196711a88af2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                resumo  rotulo\n",
              "163  Rio de Janeiro (RJ) – O Centro de Avaliações d...       2\n",
              "164  Este trabalho apresenta um sistema para contro...       2\n",
              "165  No contexto das comunicações táticas baseadas ...       2\n",
              "166  O valor da velocidade de alvos móveis em image...       0\n",
              "167  Neste artigo é apresentada a análise da seção ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a006c4ea-6ab8-430a-95f6-0348619d5b07\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resumo</th>\n",
              "      <th>rotulo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Rio de Janeiro (RJ) – O Centro de Avaliações d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Este trabalho apresenta um sistema para contro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>No contexto das comunicações táticas baseadas ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>O valor da velocidade de alvos móveis em image...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Neste artigo é apresentada a análise da seção ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a006c4ea-6ab8-430a-95f6-0348619d5b07')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a006c4ea-6ab8-430a-95f6-0348619d5b07 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a006c4ea-6ab8-430a-95f6-0348619d5b07');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-403ce5cb-7f22-47cc-a2c6-70b7f139b9a4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-403ce5cb-7f22-47cc-a2c6-70b7f139b9a4')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-403ce5cb-7f22-47cc-a2c6-70b7f139b9a4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "novo_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2ZK6EQ4xhFHI"
      },
      "outputs": [],
      "source": [
        "class_names = ['Faixa 1', 'Faixa 2', 'Faixa 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmol6gshNCR"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cFoXmzH5hFOo"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      #padding='longest',\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0K-OuLWzhYhB"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.resumo.to_numpy(),\n",
        "    targets=df.rotulo.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WBrznGAWiJWg"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-yCbjniOLI",
        "outputId": "41291dcc-3594-4d2d-a8ec-18c430322bad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxhW_89kRiv"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aiZnSuuTiTOZ"
      },
      "outputs": [],
      "source": [
        "class TRLClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(TRLClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    #The last_hidden_state is a sequence of hidden states of the last layer of the model\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6kX7ClUGiiCQ"
      },
      "outputs": [],
      "source": [
        "model = TRLClassifier(len(class_names)) #Pois tenho 3 classes\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXw5SIa0hkah"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTSemYwhnlI"
      },
      "source": [
        "### Separação e preparação do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lembrar que estamos utilizando o Kfold"
      ],
      "metadata": {
        "id": "z09fs6pHk5FG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLzT3ttq-0t"
      },
      "source": [
        "    - 63,7% treinamento - 16% validação                20% teste\n",
        "\n",
        "      ------------ 80%----------------------------------20%--"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o índice dos conjuntos\n",
        "index_df_train = kfold_train[particao]\n",
        "index_df_test = kfold_test[particao]"
      ],
      "metadata": {
        "id": "FeqFToMFnaMj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = novo_df.iloc[index_df_train], novo_df.iloc[index_df_test]"
      ],
      "metadata": {
        "id": "EpR4mtHLoDzD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "D3Bhg2qAhSlg"
      },
      "outputs": [],
      "source": [
        "#Separando o treinamento em treino e validação\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, stratify= df_train['rotulo'], random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FAkW0qhW2J",
        "outputId": "ae65f69e-9d90-4a38-a088-99d313ec6101"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((107, 2), (27, 2), (34, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxinAAqhW5A",
        "outputId": "80a6aa20-80db-4ba9-e029-c5240e5faf34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fsBv27hW8L",
        "outputId": "cb44089d-b2b4-4e80-fe7c-1c65608daf52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbQEYoXhvVA",
        "outputId": "a18066ef-21c7-4954-e18c-8792685eccd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7qyCztOhvX4",
        "outputId": "b97c64a4-9b7f-4569-ed88-cec1a73d108c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP6kiy4whvaa",
        "outputId": "ed86216a-337d-409a-aea5-1b720965e6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 242])\n",
            "torch.Size([8, 242])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzMSQeLlRHn"
      },
      "source": [
        "### Treinamento mesmo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oRhXpG_iBWB",
        "outputId": "bb5bcf7b-bda1-4f36-ba0b-380ac80e898f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Hp8dJvCviFTI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3tKAqpPliFWJ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZisESWlkqH",
        "outputId": "76c4f07f-37c0-4eb5-cb3b-03d6a893f348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.9292477922780173 accuracy 0.5514018691588785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7218090444803238 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7611890041402408 accuracy 0.6728971962616822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7316565960645676 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6431923966322627 accuracy 0.7102803738317757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7133684009313583 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6712038527641978 accuracy 0.7289719626168224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.8936254978179932 accuracy 0.37037037037037035\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5857449035559382 accuracy 0.7570093457943925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7267834916710854 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.33347062872988836 accuracy 0.8878504672897196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3162895739078522 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.23868852867079632 accuracy 0.9439252336448597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0523010939359665 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.11129252153581806 accuracy 0.9813084112149532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.233010411262512 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.13708690025045403 accuracy 0.9719626168224298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.115311190020293 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.13835517100856773 accuracy 0.9626168224299064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.0616804137825966 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.05608281004242599 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.331125259399414 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.056965214757448326 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.446208417415619 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.05763530845953418 accuracy 0.9813084112149532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.665512502193451 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.039866412191518714 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5216279327869415 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.03394004434812814 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.38003671169281 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.042273515627519895 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.3905249387025833 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00958317270851694 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.512342169880867 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.04349828200897069 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5153437852859497 accuracy 0.5555555555555556\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.02133396267475161 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4659226089715958 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.04097202406514303 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4268138259649277 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.03742873296557393 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.373006656765938 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.020200067690374062 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.438634932041168 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.03280757603767727 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4206152260303497 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0266532691104138 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4396448582410812 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.021516903261986693 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.505938306450844 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011947249681855152 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5172008275985718 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.017894800186955502 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5216299891471863 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.007475231948774308 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.548035964369774 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.028173753172658116 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5778305530548096 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.019025926760930036 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.596326068043709 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.021221878352142603 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.624712646007538 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.023075884303710024 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.6691827178001404 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.018620362430478314 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.65843003988266 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.004069625158860747 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.611702010035515 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.021748461906099692 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5582972168922424 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.04765778362551438 accuracy 0.9813084112149532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.524679884314537 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.02483458255509114 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5024545788764954 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.006282001966610551 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.507246732711792 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.018179399801218615 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.51251482963562 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.03677971427428669 accuracy 0.9813084112149532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.533054992556572 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.007421146783079686 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5275550931692123 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.023494536215106825 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.524398699402809 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.015857803446124308 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5262098759412766 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.018464153470371718 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.541990026831627 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01963547949396473 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5509749352931976 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013523868875511522 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5469144731760025 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.017281579913937355 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5450199395418167 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.02155957869503514 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.545640930533409 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.017460335831856355 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5429019778966904 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01920578334414001 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5428928434848785 accuracy 0.5925925925925926\n",
            "\n",
            "CPU times: user 2min 44s, sys: 1min 12s, total: 3min 56s\n",
            "Wall time: 4min 55s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     torch.save(model.state_dict(), melhor_modelo)\n",
        "#     best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjGnJKDmZVJ"
      },
      "source": [
        "- Gráfico de desempenho no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RQOy_NBBlks7"
      },
      "outputs": [],
      "source": [
        "A= history['train_acc']\n",
        "A2 = [tensor.item() for tensor in A]\n",
        "B= history['val_acc']\n",
        "B2 = [tensor.item() for tensor in B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "hHRH2aNQlkvy",
        "outputId": "c1b71a28-518d-4e82-cd35-d5fe5a66e682"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 1017,
              "height": 717
            }
          }
        }
      ],
      "source": [
        "plt.plot(A2, label='train accuracy')\n",
        "plt.plot(B2, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd9c9eGmecP"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jFRntT6mgoW",
        "outputId": "51955b01-e8af-4ae2-b65a-f170d856fbf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6764705882352942"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CBG2MIlgmkHv"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXTHRhD5mqnn",
        "outputId": "c1d64aba-bfef-42b7-f168-01c846088294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTdojX6Jmqqm",
        "outputId": "bf118362-8e7d-451e-bf64-f49a777e763f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Faixa 1       0.88      0.74      0.80        19\n",
            "     Faixa 2       0.33      0.25      0.29         8\n",
            "     Faixa 3       0.58      1.00      0.74         7\n",
            "\n",
            "    accuracy                           0.68        34\n",
            "   macro avg       0.60      0.66      0.61        34\n",
            "weighted avg       0.69      0.68      0.67        34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "HQ54xluu8RL3",
        "outputId": "bcad4642-4dd4-43ac-8729-04085c63b44f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 981,
              "height": 730
            }
          }
        }
      ],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('Faixa TRL verdadeira')\n",
        "  plt.xlabel('Faixa TRL predita');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mZTajeJuiP_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9dc0277146c14409b098431936ca6466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d43ad8ddc7964bfb860b397c26599045",
              "IPY_MODEL_e8eaa91d87ec44ac901461252d529de4",
              "IPY_MODEL_77dfec1b5066419c995358a072ec2005"
            ],
            "layout": "IPY_MODEL_625b56dbd67743e18481cdde92482806"
          }
        },
        "d43ad8ddc7964bfb860b397c26599045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8be4828eb9c48339c15cebb5a5ae3a1",
            "placeholder": "​",
            "style": "IPY_MODEL_a8aeb4384ed142a69231b074b563eedf",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "e8eaa91d87ec44ac901461252d529de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_222ba7aa142747508cde2fcdadd3fbeb",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c3def4fd7ad43e3ac2c3c56a2be542a",
            "value": 43
          }
        },
        "77dfec1b5066419c995358a072ec2005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eab5341711d94ef7a369d825f8d1b53d",
            "placeholder": "​",
            "style": "IPY_MODEL_84b98293684e4a62af013ad460a9fc83",
            "value": " 43.0/43.0 [00:00&lt;00:00, 974B/s]"
          }
        },
        "625b56dbd67743e18481cdde92482806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8be4828eb9c48339c15cebb5a5ae3a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8aeb4384ed142a69231b074b563eedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "222ba7aa142747508cde2fcdadd3fbeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c3def4fd7ad43e3ac2c3c56a2be542a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eab5341711d94ef7a369d825f8d1b53d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84b98293684e4a62af013ad460a9fc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88def2ef2aee47e984e60c7e50c32d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b49d90d57874d72a6b16778854a0569",
              "IPY_MODEL_25d1c44930e846bb98df1a2027e43506",
              "IPY_MODEL_3e94c6a8c95d48719e5c030a4e518147"
            ],
            "layout": "IPY_MODEL_5ebae6c5c87641f4ab3553c0b725d173"
          }
        },
        "6b49d90d57874d72a6b16778854a0569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_446b7117e387491b9b562ced826e01d6",
            "placeholder": "​",
            "style": "IPY_MODEL_364469d681f644c0943c307758b599e3",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "25d1c44930e846bb98df1a2027e43506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df99bf0570c542398b1b98758934edee",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03374da1f5d34c89a9345953a6a9788a",
            "value": 209528
          }
        },
        "3e94c6a8c95d48719e5c030a4e518147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c93260409346648e7c139a19a3408f",
            "placeholder": "​",
            "style": "IPY_MODEL_b784892dfdb2447589fc7ffc6dd50ea4",
            "value": " 210k/210k [00:00&lt;00:00, 2.69MB/s]"
          }
        },
        "5ebae6c5c87641f4ab3553c0b725d173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "446b7117e387491b9b562ced826e01d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "364469d681f644c0943c307758b599e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df99bf0570c542398b1b98758934edee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03374da1f5d34c89a9345953a6a9788a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32c93260409346648e7c139a19a3408f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b784892dfdb2447589fc7ffc6dd50ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "740d4c81306e4bed9ca377bc6dfba51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d46852284a1a441e89865847564708fa",
              "IPY_MODEL_2f6471e04efc4ee1853934e09b5fb2e2",
              "IPY_MODEL_c38503cb031f422e8893b41f159637cd"
            ],
            "layout": "IPY_MODEL_839c14a342834c41ba4defca1ac8e009"
          }
        },
        "d46852284a1a441e89865847564708fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3074b89317684612a5419a373b3eabdf",
            "placeholder": "​",
            "style": "IPY_MODEL_6f3f6f9425144a9ba5cad49c85ebd92b",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "2f6471e04efc4ee1853934e09b5fb2e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c713f10c89e04f68ab1fed89e807f12e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4b83420316846e494760c2df31a58a2",
            "value": 2
          }
        },
        "c38503cb031f422e8893b41f159637cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b0902a8b6eb4226a95acadd5b37082c",
            "placeholder": "​",
            "style": "IPY_MODEL_4369d52fde0a4570a41d52a662bd2166",
            "value": " 2.00/2.00 [00:00&lt;00:00, 88.9B/s]"
          }
        },
        "839c14a342834c41ba4defca1ac8e009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3074b89317684612a5419a373b3eabdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3f6f9425144a9ba5cad49c85ebd92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c713f10c89e04f68ab1fed89e807f12e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b83420316846e494760c2df31a58a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b0902a8b6eb4226a95acadd5b37082c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4369d52fde0a4570a41d52a662bd2166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb2f19566ec5480b941189b78d15e2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13a1b514b2cf455fa3bf7d25e10a889e",
              "IPY_MODEL_6c873e755ceb451eb2b42ad54512c5a6",
              "IPY_MODEL_36f10e7d1c2a4d79b79b30c200acd58e"
            ],
            "layout": "IPY_MODEL_f6917514757f4e1b86a275f80cbde8d6"
          }
        },
        "13a1b514b2cf455fa3bf7d25e10a889e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9d33263d1e9450dbbdb0660ea1e0628",
            "placeholder": "​",
            "style": "IPY_MODEL_dd85408e3dfc434ca1638b4495e53690",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "6c873e755ceb451eb2b42ad54512c5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f31cbf81b7ed4e4c9790162a100a832d",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cad6c537adc94a0c9c4375b5a6aba516",
            "value": 112
          }
        },
        "36f10e7d1c2a4d79b79b30c200acd58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9fa38ae325646aa890ca0a654ce737c",
            "placeholder": "​",
            "style": "IPY_MODEL_2095e023d1974069baefdb700fc3e51f",
            "value": " 112/112 [00:00&lt;00:00, 4.79kB/s]"
          }
        },
        "f6917514757f4e1b86a275f80cbde8d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9d33263d1e9450dbbdb0660ea1e0628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd85408e3dfc434ca1638b4495e53690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f31cbf81b7ed4e4c9790162a100a832d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad6c537adc94a0c9c4375b5a6aba516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9fa38ae325646aa890ca0a654ce737c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2095e023d1974069baefdb700fc3e51f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e50b6ab989dc44a7af102b2dbfccfa86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9584a125787e4e3c8d728457cbd3589d",
              "IPY_MODEL_c8cbb6a6f4e74a4c92a491c6f5ae8e50",
              "IPY_MODEL_f490afd9208c4a85bd8e6b2f139766a4"
            ],
            "layout": "IPY_MODEL_5b24353c16744d4cb4eb59b88a829579"
          }
        },
        "9584a125787e4e3c8d728457cbd3589d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09e42a98993b4363961e94d77010c5e9",
            "placeholder": "​",
            "style": "IPY_MODEL_77bad87ea27d4ef4b87db94a36e02ebc",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "c8cbb6a6f4e74a4c92a491c6f5ae8e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af8cfd0c0ca14e34af44587352f1c873",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec0e9e0ec94e4d18bc2ace64e6241087",
            "value": 647
          }
        },
        "f490afd9208c4a85bd8e6b2f139766a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f4a74315eb459f851ae401abd77666",
            "placeholder": "​",
            "style": "IPY_MODEL_53ad859ba4c84a5fbaeb0ad628753cff",
            "value": " 647/647 [00:00&lt;00:00, 48.6kB/s]"
          }
        },
        "5b24353c16744d4cb4eb59b88a829579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e42a98993b4363961e94d77010c5e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77bad87ea27d4ef4b87db94a36e02ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af8cfd0c0ca14e34af44587352f1c873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec0e9e0ec94e4d18bc2ace64e6241087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72f4a74315eb459f851ae401abd77666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53ad859ba4c84a5fbaeb0ad628753cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "759e371e234f450badcd68e07f8f8267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5462ca9cfcbe4cf4aed733ba711b2b91",
              "IPY_MODEL_6bb04d6ccd79441a9f7fa89fc0538566",
              "IPY_MODEL_643d8c890db24eada71b18b716b8b46b"
            ],
            "layout": "IPY_MODEL_c2313de4a48e405aa5053d8bf7836897"
          }
        },
        "5462ca9cfcbe4cf4aed733ba711b2b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc38b0257ad5480eaefbfb287108e259",
            "placeholder": "​",
            "style": "IPY_MODEL_8450943f881943a2afc016987af3b8ad",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "6bb04d6ccd79441a9f7fa89fc0538566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6a8db41085d42a2a8f017f491a1ff9f",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3aa9c35c898543e7823bee3f08438b34",
            "value": 438235074
          }
        },
        "643d8c890db24eada71b18b716b8b46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0685d5c36c4b479380b6aeabc0b03362",
            "placeholder": "​",
            "style": "IPY_MODEL_095743efb047431c9bdeffcdf848882e",
            "value": " 438M/438M [00:02&lt;00:00, 217MB/s]"
          }
        },
        "c2313de4a48e405aa5053d8bf7836897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc38b0257ad5480eaefbfb287108e259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8450943f881943a2afc016987af3b8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6a8db41085d42a2a8f017f491a1ff9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa9c35c898543e7823bee3f08438b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0685d5c36c4b479380b6aeabc0b03362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095743efb047431c9bdeffcdf848882e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}