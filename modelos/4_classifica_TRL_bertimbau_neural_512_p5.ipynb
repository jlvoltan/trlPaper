{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jKuooZfNFU"
      },
      "source": [
        "# Classificação faixa TRL - Bertimbau + Rede Neural + 512 tokens [kfold][P5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9-gf3IMD5C"
      },
      "source": [
        "- BERTimbau Base (aka \"bert-base-portuguese-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i41YvsyfSUR"
      },
      "source": [
        "## Código Versão 1.0 - 21 OUT 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfR22FfgbJ"
      },
      "source": [
        "- Uso dos classificadores: *Rede Neural*\n",
        "- **k-Fold cross-validation: Esse código executou a rodada 5**.\n",
        "- Matriz de confusão;\n",
        "- Semente 42\n",
        "- 50 épocas\n",
        "- BATCH_SIZE = 8\n",
        "- Limite 512 tokens\n",
        "- optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.csv   ou  dataset_pre_processado_1.csv  ou  dataset_pre_processado_stem_2.csv\n",
        "#     CSV1                  CSV2                                   CSV3\n",
        "dataset = \"dataset.csv\""
      ],
      "metadata": {
        "id": "vXMRttjhRQ2z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lembre-se estamos usando o dataset: \" + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJkf84RR64K",
        "outputId": "cf3af6bc-b67c-4e56-f4d6-ccea7550bc2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lembre-se estamos usando o dataset: dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "particao=5 # O KFold apresenta 5 partições (rodadas)\n",
        "particao-=1  #Funcionará com o índice de kfold_train e kfold_test"
      ],
      "metadata": {
        "id": "k2TIOYIqQNps"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melhor_modelo = 'best_model_TRL_bertimbau_base_neural_' + str(particao+1) + '.bin'\n",
        "melhor_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lFCPPlujqL5L",
        "outputId": "4a04493e-9fca-493a-a7f7-405df6e184cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_model_TRL_bertimbau_base_neural_5.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512"
      ],
      "metadata": {
        "id": "_6clpQcISMQV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jlIjUzOgRY"
      },
      "source": [
        "- 63,7% treinamento - 16% validação...............20% teste\n",
        "- EPOCHS = 50\n",
        "- lr=2e-5\n",
        "- BATCH_SIZE = 8\n",
        "- Quantidade de tokens máxima = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Msvyn0MfsOS"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC81VL_gQ-5"
      },
      "source": [
        "### Bibliotecas e ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZxPMZOfICS",
        "outputId": "a9fa4c76-ab44-404e-a55a-92c5bda83fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5RDBcpVf0TS",
        "outputId": "70883f40-c9eb-48ce-d9e0-1e69322da2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmcqgWcf0WZ",
        "outputId": "99374311-2ac7-4ad4-da5b-8e93e51a049e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 26 00:44:14 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoneZ3CngEDZ",
        "outputId": "e01d9165-0df2-41ef-e981-40522492c51c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWHFe7ef0Zp",
        "outputId": "1107ab85-7754-4dca-da31-f5b7156df70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy       : 1.23.5\n",
            "pandas      : 1.5.3\n",
            "torch       : 2.1.0+cu118\n",
            "transformers: 4.34.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particoes KFOLD"
      ],
      "metadata": {
        "id": "oENoyHsUPjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para mais detalhes, consulte o código *1-kfold.ipynb* em que as partições foram sorteadas. As células a seguir recriam o resultado obtido."
      ],
      "metadata": {
        "id": "qL3FKCGlPtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = np.array([  0,   1,   2,   3,   5,   7,   9,  10,  11,  12,  13,  14,  15,\n",
        "         16,  17,  19,  20,  21,  22,  23,  25,  26,  27,  28,  30,  32,\n",
        "         33,  34,  35,  36,  37,  39,  40,  44,  45,  46,  47,  48,  49,\n",
        "         50,  52,  53,  54,  55,  56,  57,  59,  60,  61,  63,  64,  66,\n",
        "         68,  69,  70,  71,  72,  75,  76,  77,  78,  79,  80,  81,  82,\n",
        "         83,  84,  85,  87,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
        "        102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
        "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130,\n",
        "        131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145,\n",
        "        146, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
        "        164, 165, 166, 167])\n",
        "\n",
        "array2 = np.array([  0,   1,   3,   4,   5,   6,   7,   8,  10,  13,  14,  15,  16,\n",
        "         17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "         34,  36,  37,  38,  40,  41,  42,  43,  44,  45,  47,  49,  51,\n",
        "         52,  53,  54,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  70,  71,  73,  74,  75,  76,  77,  80,  83,\n",
        "         84,  86,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99,\n",
        "        100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115,\n",
        "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "        129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
        "        147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162,\n",
        "        163, 165, 166, 167])\n",
        "array3 = np.array([  1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
        "         17,  18,  20,  21,  22,  23,  24,  25,  28,  29,  30,  31,  32,\n",
        "         33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,\n",
        "         48,  50,  51,  52,  54,  55,  56,  57,  58,  59,  62,  63,  65,\n",
        "         67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "         82,  84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,\n",
        "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111,\n",
        "        112, 113, 114, 118, 120, 121, 122, 123, 124, 126, 128, 129, 131,\n",
        "        132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 146, 147,\n",
        "        148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162,\n",
        "        163, 164, 165, 166])\n",
        "array4 = np.array([  0,   2,   3,   4,   6,   8,   9,  10,  11,  12,  15,  16,  18,\n",
        "         19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  31,  32,  33,\n",
        "         34,  35,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
        "         49,  50,  51,  53,  56,  57,  58,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  71,  72,  73,  74,  75,  76,  78,  79,  80,\n",
        "         81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
        "         94,  95,  98, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111,\n",
        "        112, 114, 115, 116, 117, 119, 121, 123, 124, 125, 127, 128, 129,\n",
        "        130, 131, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145,\n",
        "        146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159,\n",
        "        162, 163, 164, 166, 167])\n",
        "array5 = np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
        "         14,  15,  16,  17,  18,  19,  21,  23,  24,  26,  27,  28,  29,\n",
        "         30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,  43,\n",
        "         44,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,\n",
        "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  72,  73,  74,\n",
        "         77,  78,  79,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
        "         92,  94,  95,  96,  97,  99, 101, 102, 103, 105, 106, 107, 108,\n",
        "        110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 127,\n",
        "        128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 143, 144,\n",
        "        145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160,\n",
        "        161, 162, 164, 165, 167])\n",
        "# Criar a lista composta\n",
        "kfold_train = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "Cn4vhJ9LPncE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as cinco arrays numpy\n",
        "array1 = np.array([  4,   6,   8,  18,  24,  29,  31,  38,  41,  42,  43,  51,  58, 62,  65,  67,  73,  74,  86,  88,  89,  90,  95, 101, 107, 111, 128, 133, 144, 147, 149, 152, 156, 162])\n",
        "array2 = np.array([  2,   9,  11,  12,  21,  28,  32,  33,  35,  39,  46,  48,  50, 56,  72,  78,  79,  81,  82,  85,  87,  91, 103, 106, 108, 131, 134, 140, 141, 146, 148, 158, 159, 164])\n",
        "array3 = np.array([  0,   3,  15,  16,  19,  26,  27,  34,  40,  49,  53,  60,  61, 64,  66,  68,  69,  83,  92,  94, 110, 115, 116, 117, 119, 125, 127, 130, 137, 143, 145, 154, 157, 167])\n",
        "array4 = np.array([  1,   5,   7,  13,  14,  17,  23,  30,  36,  44,  52,  54,  55, 59,  70,  77,  96,  97,  99, 102, 105, 113, 118, 120, 122, 126, 132, 136, 138, 151, 160, 161, 165])\n",
        "array5 = np.array([ 10,  20,  22,  25,  37,  45,  47,  57,  63,  71,  75,  76,  80,  84,  93,  98, 100, 104, 109, 112, 114, 121, 123, 124, 129, 135, 139, 142, 150, 153, 155, 163, 166])\n",
        "\n",
        "\n",
        "# Criar a lista composta\n",
        "kfold_test = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "c3Ia2J4DPpO8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4aPPergNnL"
      },
      "source": [
        "### Bertimbau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl6AUm0K3yE"
      },
      "source": [
        "- https://huggingface.co/neuralmind/bert-base-portuguese-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RQrbYpK5vC"
      },
      "source": [
        "```\n",
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "49586e3ef35f418c9834f579f8e3127b",
            "55b635e83840445f806dbc06f6ba2952",
            "dcf708ea405943379c931248713dc20b",
            "cfe47406b784481a93cbfa4a76bde335",
            "333503810fdd44348c32655649bef518",
            "aced19eccbd2457fb8b6956e560ef5e9",
            "abfc20fa4b104311850120fd0e00a04b",
            "22354895248c4dbcb11b03cb87c16c95",
            "37dc3be83829465a85edf9d3a6894e25",
            "d14541d6c62e42259af364754d5c69b2",
            "291ba8f4255146149ae454f3f16e3c44",
            "a329b7532b7840b980be3fd3f6a6babd",
            "8061142a0a62440495a806eaa8919533",
            "c52f80d3a865412884b6e2a820a86e58",
            "45080521893b4bcbbba478e301e8361a",
            "32e941bb41984c8cb9ea6eb77b9aac2d",
            "3f71d495c2334358a6e2396c60739469",
            "c3fe3c5dd7054244a01ccb5ecca3d429",
            "037c44d50beb42bb97cffcdbead0b5a5",
            "f0de1fce48224a7bab4e3e5174a1f145",
            "1557d76213d04d55b737aed86fd59cc7",
            "6ac4a72d69c144e09f1e37c615183b21",
            "9f696c823a1640559a8b4d21b404e1b9",
            "0b8d0442fd19474e9bf696d49578d47d",
            "824e85318c4a4e37ae08262746e7e934",
            "3a7f336694ee49bea7b6f8cf6aab3d0f",
            "24c9047ddce14032bcffaba84ec993b1",
            "e48b838e61b942a180c61a6390c6d420",
            "5d9282289e7c4eddaeb7bcfea5dc3bd6",
            "0a539bd7e3c04c6f8411e3fc4f353a22",
            "7c43b7838b104c3e8e62009e9302afc9",
            "ad10bea2fe4949b3ba9fb241143b718c",
            "1a714cfc844b4d41a5a180ff15d6e636",
            "056a811909654e2fa259ecabec441e96",
            "4d1f33000ac64f2e9bef53e89203b9fa",
            "33e88798afab4e49a095278024d74f70",
            "813d21acb87143798c79efb3668f02ed",
            "c690bdf457f04aa2920f986cc718f560",
            "56c6644b19da45708ac45a3285eff1d4",
            "d278207ee0ef41c981ccddf791f0eae1",
            "bca35ede9de04d2dbd73249dc762ae70",
            "acaf4777d74e418bae970790160d4229",
            "c704368f120447a69d95378a2aff4cc1",
            "94a288ff2ceb4c98a0cf5ea52dfb840c",
            "9c89d54b36f044beb8bc436e8b1fe624",
            "016c44500d754211bc1db1b5de46afb6",
            "e9466442ac9d46c6ab5e9c2bedf57d59",
            "816f609877fc4ee0a66752ec6f6bcaf5",
            "b9549bf614a54a68b2e3e42317a62cd8",
            "99c987c9d85d43109f78d186d94e7cc4",
            "1a7c02cc4f3a4132878f56359cacda3f",
            "d3b02aa1e1c9403088e0da0163745956",
            "4ed01970fb5748b189239729fd170746",
            "88594bcc156141b3862bcb0634e711a2",
            "ee3163d5f056419fbf242f42158eeb9c"
          ]
        },
        "id": "WPj7c-IBgWRx",
        "outputId": "967a4ad6-d34a-4d1e-ab3a-468630a8c4a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49586e3ef35f418c9834f579f8e3127b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a329b7532b7840b980be3fd3f6a6babd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f696c823a1640559a8b4d21b404e1b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "056a811909654e2fa259ecabec441e96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c89d54b36f044beb8bc436e8b1fe624"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "babdb282921444c1bd05404e1a92f16a",
            "92eafe18f3e8490cb642945f737d90cb",
            "c8bf859e63fd43ab93399973b29f1a1d",
            "b82826cfb2fd4126a518e9b1a2c890a0",
            "dafd3265dad74de5b515a75c1ee3b8e9",
            "3cea3a0295864117bb9f103060929514",
            "3cc2fed01b6b4e2ead4a17588b6b9030",
            "fd2f08445f87416ca9c7d6d0a8c0636d",
            "6dec26ba1b904bd4a9281d29a1ad5a46",
            "34dfa3a129d445f4a7e563ecdfbe9c1c",
            "2d59e1cecb7248f98f5d149ee68ccabc"
          ]
        },
        "id": "qSErznNMh4P5",
        "outputId": "2fc2b13d-d41b-4f07-a0a2-abb813d06474"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "babdb282921444c1bd05404e1a92f16a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y6s_rPsFgWUr"
      },
      "outputs": [],
      "source": [
        "frase = 'A avaliação de prontidão tecnológica em tecnologias de interesse militar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJbGMogWaZ",
        "outputId": "9ea7720c-c879-45d5-cb13-3d93f8560884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  frase,\n",
        "  max_length=20,   #Perceba que irei cortar um token da minha frase anterior\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fSHD8KgqDb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0bYjQrfrglJS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LvfsVlnFglPp"
      },
      "outputs": [],
      "source": [
        "novo_df = df[[\"resumo\", \"rotulo\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Nth2Ui-qg6Ix"
      },
      "outputs": [],
      "source": [
        "def adapto_faixa(faixa):\n",
        "    faixa -=1\n",
        "    return faixa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKj3lo8Qg9Vj",
        "outputId": "f7b35b45-3217-449c-b3c2-039611e2d437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3dc243303e67>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)\n"
          ]
        }
      ],
      "source": [
        "novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_cANKWhOhCix",
        "outputId": "033b34cd-a898-4d9f-f115-6540cf5409ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                resumo  rotulo\n",
              "163  Rio de Janeiro (RJ) – O Centro de Avaliações d...       2\n",
              "164  Este trabalho apresenta um sistema para contro...       2\n",
              "165  No contexto das comunicações táticas baseadas ...       2\n",
              "166  O valor da velocidade de alvos móveis em image...       0\n",
              "167  Neste artigo é apresentada a análise da seção ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06984166-339f-4950-9752-d100a021a2d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resumo</th>\n",
              "      <th>rotulo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Rio de Janeiro (RJ) – O Centro de Avaliações d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Este trabalho apresenta um sistema para contro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>No contexto das comunicações táticas baseadas ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>O valor da velocidade de alvos móveis em image...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Neste artigo é apresentada a análise da seção ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06984166-339f-4950-9752-d100a021a2d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-06984166-339f-4950-9752-d100a021a2d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-06984166-339f-4950-9752-d100a021a2d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-171da9f4-bfd1-4200-9e51-98b6aca5c5cb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-171da9f4-bfd1-4200-9e51-98b6aca5c5cb')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-171da9f4-bfd1-4200-9e51-98b6aca5c5cb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "novo_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2ZK6EQ4xhFHI"
      },
      "outputs": [],
      "source": [
        "class_names = ['Faixa 1', 'Faixa 2', 'Faixa 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmol6gshNCR"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cFoXmzH5hFOo"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      #padding='longest',\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0K-OuLWzhYhB"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.resumo.to_numpy(),\n",
        "    targets=df.rotulo.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WBrznGAWiJWg"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-yCbjniOLI",
        "outputId": "ed1d568b-5f4e-49a0-9acb-46b6dfa951d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxhW_89kRiv"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aiZnSuuTiTOZ"
      },
      "outputs": [],
      "source": [
        "class TRLClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(TRLClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    #The last_hidden_state is a sequence of hidden states of the last layer of the model\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6kX7ClUGiiCQ"
      },
      "outputs": [],
      "source": [
        "model = TRLClassifier(len(class_names)) #Pois tenho 3 classes\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXw5SIa0hkah"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTSemYwhnlI"
      },
      "source": [
        "### Separação e preparação do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lembrar que estamos utilizando o Kfold"
      ],
      "metadata": {
        "id": "z09fs6pHk5FG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLzT3ttq-0t"
      },
      "source": [
        "    - 63,7% treinamento - 16% validação                20% teste\n",
        "\n",
        "      ------------ 80%----------------------------------20%--"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o índice dos conjuntos\n",
        "index_df_train = kfold_train[particao]\n",
        "index_df_test = kfold_test[particao]"
      ],
      "metadata": {
        "id": "FeqFToMFnaMj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = novo_df.iloc[index_df_train], novo_df.iloc[index_df_test]"
      ],
      "metadata": {
        "id": "EpR4mtHLoDzD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "D3Bhg2qAhSlg"
      },
      "outputs": [],
      "source": [
        "#Separando o treinamento em treino e validação\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, stratify= df_train['rotulo'], random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FAkW0qhW2J",
        "outputId": "adfe022a-7dca-40da-8d7b-3aed19bee9df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((108, 2), (27, 2), (33, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxinAAqhW5A",
        "outputId": "2d6e4601-e842-43a6-849e-6ab88a95e86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fsBv27hW8L",
        "outputId": "7cd18905-b8ee-46b8-9f3b-c80bafb6c067"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbQEYoXhvVA",
        "outputId": "15b5709b-15a9-40b5-f73d-cf2b5449efcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7qyCztOhvX4",
        "outputId": "2243d5a4-20aa-45fd-f91b-1be68f40eef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512])\n",
            "torch.Size([8, 512])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP6kiy4whvaa",
        "outputId": "c60c630b-7817-4bf5-9506-06ba30aeebdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512])\n",
            "torch.Size([8, 512])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzMSQeLlRHn"
      },
      "source": [
        "### Treinamento mesmo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oRhXpG_iBWB",
        "outputId": "ca4dd633-a669-4e27-cf88-e40c68ec418c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Hp8dJvCviFTI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3tKAqpPliFWJ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZisESWlkqH",
        "outputId": "1a1d1efd-7657-4ab4-9cd1-cc162458340a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.8037731094019753 accuracy 0.6666666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7746279053390026 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6510726915938514 accuracy 0.7407407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7201909273862839 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5200639335172517 accuracy 0.8148148148148148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7500551948323846 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.4242279274123056 accuracy 0.8333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7223070412874222 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.32035565549241646 accuracy 0.8888888888888888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9425771799869835 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1975501055962273 accuracy 0.9351851851851851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1916283042519353 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.11089420691132545 accuracy 0.9629629629629629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.409371953457594 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1454653691034764 accuracy 0.9629629629629629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3475800018932205 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.10620666009240917 accuracy 0.9629629629629629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.494994078137097 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.07474377052858472 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4141611804952845 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.06387281510978937 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.350928148094681 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.031480623658613434 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3360908489848953 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.02033054461103997 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4032065283245174 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.02529039402725175 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4036884339875542 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.018354410405403802 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3819191951770335 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.023870812941043238 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.3948435814818367 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.018457163782191595 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4101768120963243 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.016726971687083796 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4201060657869675 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01636959132572104 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.436318022570049 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.018515716884784133 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4600689964718185 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.009059895973353247 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.4912941367583699 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.014960433884490547 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5000313177588396 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011487166738204126 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5154325417024666 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.012131006639849926 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5202390311169438 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01693784459244593 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5256626884292928 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01619027552078478 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5450285629776772 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.024096203834882805 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5604152648229501 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01622556948651826 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5884380203860928 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.012063719558812278 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5756457343304646 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.012631977577776914 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5877623244232382 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.014769762696232647 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.598016935531632 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013972287450867173 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.595642406384286 accuracy 0.7407407407407407\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01619838425124596 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5874537826966844 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.014267839100544475 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5908058990025893 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.015046979641608362 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5918589550783508 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0160983208500381 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.592388194923842 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.017623379441959384 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5969315089969314 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013282767665389526 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5967609106664895 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011074782118417456 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6006131785325124 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.014395665882537807 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6009943300450686 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011724242291945432 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6016869959930773 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013948940979649447 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6021780293049233 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.013603158276444966 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.605341486447287 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011222052888894853 accuracy 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6086432164120197 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.011174225628825038 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6113058056653244 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.016724981941349273 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6132282640719495 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.015025698872964963 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6140278801285604 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.01557115588885998 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6148344204520981 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.012317238302784972 accuracy 0.9907407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6155987010533863 accuracy 0.7777777777777777\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.018413380143881244 accuracy 0.9814814814814814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.615790399548132 accuracy 0.7777777777777777\n",
            "\n",
            "CPU times: user 8min 26s, sys: 21.5 s, total: 8min 47s\n",
            "Wall time: 9min 46s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     torch.save(model.state_dict(), melhor_modelo)\n",
        "#     best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjGnJKDmZVJ"
      },
      "source": [
        "- Gráfico de desempenho no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RQOy_NBBlks7"
      },
      "outputs": [],
      "source": [
        "A= history['train_acc']\n",
        "A2 = [tensor.item() for tensor in A]\n",
        "B= history['val_acc']\n",
        "B2 = [tensor.item() for tensor in B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "hHRH2aNQlkvy",
        "outputId": "aeb13141-a16d-455c-8382-077aa01a844b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB/MAAAWaCAYAAAAHDkSJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAB7CAAAewgFu0HU+AAEAAElEQVR4nOzdd3hUZfrG8Xtm0nsPkFClKAqKq+5PV9YV2RU7CogdcS3YsKLi6i66dlfX3lYF21pA7F1R14ooKCLSCU1IT0gvk/P744RJziSBCZnJmZl8P9eVy5xnzpl5QibHmbnP+74OwzAMAQAAAAAAAAAAAACAoOG0uwEAAAAAAAAAAAAAAGBFmA8AAAAAAAAAAAAAQJAhzAcAAAAAAAAAAAAAIMgQ5gMAAAAAAAAAAAAAEGQI8wEAAAAAAAAAAAAACDKE+QAAAAAAAAAAAAAABBnCfAAAAAAAAAAAAAAAggxhPgAAAAAAAAAAAAAAQYYwHwAAAAAAAAAAAACAIEOYDwAAAAAAAAAAAABAkCHMBwAAAAAAAAAAAAAgyBDmAwAAAAAAAAAAAAAQZAjzAQAAAAAAAAAAAAAIMoT5AAAAAAAAAAAAAAAEGcJ8AAAAAAAAAAAAAACCDGE+AAAAAAAAAAAAAABBhjAfAAAAAAAAAAAAAIAgQ5gPAAAAAAAAAAAAAECQIcwHAAAAAAAAAAAAACDIEOYDAAAAANCNzjzzTA0bNkzDhg3TmDFj7G5HCxcu9PQzbNgwzZ8/3+6WgtZ1111n+bcKhM2bN1se48EHHwzI4wAAAAAAgl+E3Q0AAAAAAHqezZs364gjjgjoY1xyySW69NJLA/oYAAAAAAAAgcLIfAAAAAAAAEhiZgAAAAAACCaE+QAAAAAAAAAAAAAABBmm2QcAAAAAdLtevXrpk08+8WnfK6+8Uj/99JNn+95779W+++67y+OSkpJ2uz8AAAAAAAC7EeYDAAAAALpdRESEcnNzfdo3Ojrasp2RkeHzscHoueees7sFi9///vdauXKl3W2gWW5uLr8PAAAAAIAkptkHAAAAAAAAAAAAACDoEOYDAAAAAAAAAAAAABBkmGYfAAAAANBjrFq1SmvWrFFhYaFqamqUk5Oj4447rsP9q6urtXr1aq1fv16lpaWqra1VYmKi0tLStM8++6hfv37d2H1bmzZt0i+//KJt27bJ7XYrPT1dv/vd79S3b19b+mloaND333+vzZs3q6SkRImJierfv78OOOCANssldNYvv/yilStXqqioSPHx8erVq5dGjRqltLQ0P3XfdQUFBfrpp5+0detW1dXVKS0tTSNHjtSQIUO65fHz8/O1fPly/fbbb6qsrJQkxcTEKDMzU3379tWwYcMUFRXVLb14W7FihVatWqWSkhLV19crPT1dubm5GjVqlN97Wrp0qTZu3KiCggI1NjZqyJAhOvzww/36GAAAAADQHQjzAQAAAABhY8yYMdqyZYsk6aCDDvKsT//qq69q9uzZWr16tWX/xMTENmH+li1b9M477+jTTz/Vzz//rIaGhg4fLycnR2eddZZOOeUUxcTE+NTjmWeeqe+++85z/IIFCzq9708//aR7771XCxculGEYbY7bd999NXPmTI0aNWqX/SxcuFBnnXWWZ/v222/XSSed1Kl96+vr9cgjj+jll19WSUlJm+Pi4uI0ZcoUTZs2zed/px1ef/11Pfjgg9q8eXOb2yIjIzV27Fhdc8016tOnT6d+Fn9at26d7r77bv3vf/9TY2Njm9sHDRqka6+9Vn/60592eV+bN2/WEUcc4dm+5JJLdOmll+70mI8//lhPPvmklixZstP9IiMjtd9+++noo4/WaaedZrmt9XOttYceekgPPfRQu/e3q+dvbW2t5syZoxdffFHbtm1rd5+4uDiNGzdOl112mXr16rXT/ncYNmyY5/sTTzxRd9xxh5qamjR79mz997//bfNc2XPPPXX44YfrlFNO8fwbRUdH64svvlBycrJPj7nDJZdcoo8++kiS5HQ69fHHHysnJ6dT9wEAAAAAvmKafQAAAABA2Kqvr9dll12m66+/vk2Q3x63260jjjhC99xzjxYvXrzTIF8yg//bb79dkydP9lxEEGjPPfecTj/9dH377bftBvmSGfafeeaZevfddwPez7Zt23Tqqafq0UcfbTfIl8wZDh599FGdc845nhHju9LQ0KDp06fr2muvbTfI37HPe++9pxNPPFELFy7c7Z+hK95//31NmDBBCxYsaDfIl8yw/4ILLtCcOXP8+thut1vXXnutLr744l0G+ZL577Vo0SLde++9fu2jPWvWrNHRRx+tf//73x0G+ZL53Jg/f76OPPJIvfnmm7v1WOXl5ZoyZYruuuuuDp8rknTKKad4vq+rq+v04xUVFemzzz7zbB9yyCEE+QAAAAACipH5AAAAAICwdeutt+r999+XJDkcDg0fPlw5OTlyOBzatGlTm+DPMAxLQO5wOJSbm6v+/fsrKSlJDodDpaWl+vXXX1VaWurZb8WKFTrnnHM0f/58xcfHB+zneeONN3TLLbd4tocOHap+/fopKipKGzdu1C+//OLpv6GhQTNnztTw4cM1YMCAgPRTU1OjCy64QCtWrJAkJSQkaOTIkUpLS1NVVZV+/PFHy7/TDz/8oNtvv1233nrrLu/7qquu0gcffGCpxcTEaN9991VmZqa2b9+uZcuWqaSkRGVlZbr00kt1/fXX+/cH3IWFCxfqqquu8oT4AwYM0KBBgxQXF6fffvtNS5cutQT8d9xxh/bZZx8dcMABfnn8Bx54QK+//rqlFhcXp7322kuZmZmKjIxUVVWVCgoKtHbtWtXU1PjlcXdlxYoVmjJlisrKyiz13NxcDRkyRNHR0dq0aZOWL1/ueb7W1tbqmmuuUU1NjSZPnuzzYxmGoRkzZnhmFYiIiNCIESPUq1cv1dXVacOGDZ59x40bp9tuu03l5eWSpHnz5unMM8/0+bFee+01ywU+EydO9PlYAAAAANgdhPkAAAAAgLC0bNkyT8B3/PHH66qrrmozjXd7o3gjIiJ0xBFHaNy4cRo9erQSExPb7NPU1KSvvvpKd911l1atWiVJysvL07/+9S/94x//CMBPI5WWlurGG2+UJM/U8v3797fss3btWl1xxRVauXKlJDMgve+++3TfffcFpKcHHnhAZWVlSklJ0YwZMzR+/HhFRLR81NDY2Kinn35a9957rye0nTdvnqZOnarBgwd3eL/z5s2zBPkul0sXXHCBzjvvPMXFxXnqbrdb77zzjm699VaVlZXp9ttvD8BP2bHp06ersbFRBxxwgK6//nrtvffeltu3bt2qa6+91jNrgGEYuvPOOzV37twuP3ZZWZmeeuopz3ZcXJxmzpyp8ePHt7sGvdvt1pIlS/TRRx95polv7d5771VdXZ22bdum008/3VM/66yzNGXKlHZ7aP273qG2tlZXXnmlJcjv16+fbr75Zh188MGWfTdt2qSbbrpJX3zxhSTz3+eWW27Rvvvuqz333HPn/wDNPvzwQ1VXV8vhcGjKlCm68MILlZKSYtlnx995TEyMjj/+eM/yGytWrNDPP/+sESNG+PRY8+bN83yflpZmWQ4BAAAAAAKBafYBAAAAAGGpurpaknT++efr7rvvbnc97tzcXMu2y+XSRx99pAceeEBHH310u0G+ZK6VPXr0aL388svab7/9PPX58+e3GY3sL9XV1aqrq9Ppp5+uhx56qE2QL0l77LGHnn76aSUlJXlqn3zyiWcksr/tCPL/+9//auLEiW3C3YiICJ1//vk6//zzLfX58+d3eJ91dXW6++67LbXbbrtNl112mSXIl8zf1/HHH69nnnlGiYmJAfu370hZWZnGjh2rOXPmtAnyJal379564okn1LdvX09t6dKlWrNmTZcf++uvv7aMEp81a5ZOPvnkdoN8yfy3OuCAAzRz5ky99957bW7PzMxUbm5um7+TpKQk5ebmtvvV3t/U008/rbVr13q2+/fvr5deeqlNkC9Jffv21RNPPKFx48Z5avX19Zo1a9Yuf/4ddvydz5o1SzNnzmwT5EvWv/PWU+1L8vnCikWLFikvL8+z3dFFEwAAAADgT4T5AAAAAICwtddee+nyyy/3eX+Hw6E+ffr4vH9cXJxuuukmz3Ztba0WLFjQmRY7ZejQoZo5c6YcDkeH+2RkZOjUU0/1bNfX1+vHH38MWE833nij9thjj53uc9555yk6OtqzvWjRog73fe+99yyh/Lhx4zR+/Pid3v+ee+6pK664wqd+/Sk9PV133HGHIiMjO9wnJiZG5513nqW2Y8aIrvjtt98s23/+8599Prb178KfGhoa9OKLL3q2HQ6H7rrrLqWnp3d4jNPp1K233qqsrCxPbcmSJfr55599ftzDDz+8TUjfkcGDB2v//ff3bL/zzjs+LT/gHfozxT4AAACA7kCYDwAAAAAIW1OmTJHL5QroY+y5556Wkb8//fRTwB5rypQpOw2Od/jjH/9o2d4x7b6/5eTk6Oijj97lfomJiZYAdeXKlZ5p9729//77lm3vILwjkyZNandUdiBNnjy5w9kbWjvssMMs2ytWrPB7LyUlJX6/z85auHChCgoKPNujR4+2zFzRkYSEBJ177rmW2ptvvunz455zzjk+7yuZv7cdKisr2zznvFVUVFiWfdh///13eQELAAAAAPgDYT4AAAAAIGwdfvjhfruvuro6FRcXa8uWLdq8ebPlq3WIvG7dOr89prfRo0f7tN+gQYMs24EKev/whz/I6fTto4XWPdXV1amqqqrd/VrPIpCTk6N99tnHp/uPiorSn/70J5/29Rdffx+9evWyLBFQWlra5cceOHCgZfuee+6R2+3u8v12xZIlSyzbxxxzjM/HHnvssZYZJ7zvqyOJiYk68MADfX4cSTrqqKOUnJzs2Z43b95O93/rrbdUW1vr2T755JM79XgAAAAAsLsidr0LAAAAAAChp0+fPl0aqZ2Xl6e3335bCxcu1KpVq3xej3379u27/Zg7k5CQoOzsbJ/29R4tXllZGYiWOjU62bunqqoqJSQkWGoFBQWWoHv48OGd6mf48OF6/fXXO3VMV3Tm509ISPCs7+6P38fBBx+s1NRUz7/Xu+++qxUrVmjy5MkaO3asZbaI7vLLL79Ytvfdd1+fj01PT1dubq42bdokyZy9wO1273JmjT333HOny060Jzo6WieccIKeffZZSdL333+v9evXt7lAYofWYX9iYqLGjRvXqccDAAAAgN3FyHwAAAAAQFhKTU3dreO2b9+uv/3tbxo3bpwefPBBfffddz4H+VLggnNfpnPfwXsq/sbGRn+3I0ltwvidiYiwjidoaGhos4/3v3OvXr061U/v3r07tX9X7e7vxB+/j7i4OP3973+3BNnr1q3T7bffriOOOEJjxozRjBkz9PLLL2v9+vVdfjxftJ4BwuFwqH///p06vnWY3tDQoIqKil0ek5aW1qnH2KH1VPuSNHfu3Hb3+/XXXy0XKRxzzDGKjY3drccEAAAAgM4izAcAAAAAhKX4+PhOH1NeXq4pU6Zo3rx5Ha7pviu7e9yu+DqdfXfyd0/e4W1nf4edubjAH+z+nRx99NF65JFH2r3oYcuWLXrzzTf197//XePGjdMxxxyj2bNnq6amJmD9tJ6VIjY2ttP/Pt4XR/gyy0Xr5Qs6Y/Dgwfrd737n2X7jjTfavcjilVdesWwzxT4AAACA7hR8nwQAAAAAAGCTO+64Q8uXL/dsR0dHa/z48brrrrv0+uuv6+uvv9aPP/6oX3/9VStXrvR8HXTQQTZ2HT66OqNAfX29P9sJCWPGjNGHH36oO++8U4cddliH4faaNWt0xx136KijjvJ5Pfpw13p0flFRkT799FPL7bW1tXr77bc928OHD9fee+/dbf0BAAAAQMSudwEAAAAAIPxt3bpVr732mmc7KytLzzzzjAYNGrTLY6uqqgLZWo+RnJxs2fZlZHZr5eXl/mwnZOy46GT8+PFqbGzUr7/+qsWLF+u7777T119/rerqas++W7du1bnnnqu5c+f69NzujKSkJM/3NTU1ampq6tTofO+ZGVrfXyCMGzdOt912m2d5h7lz5+rPf/6z5/b333/f8hycOHFiQPsBAAAAAG+MzAcAAAAAQNLnn39umSJ/xowZPoedhYWFgWqrR8nKypLL5fJsr169ulPHr1mzxt8thZyIiAiNGDFCU6ZM0cMPP6yFCxfqrrvuUu/evT37VFZW6oEHHvD7Y7dev94wDG3cuLFTx+fl5Xm+j4yMbDPtvr9FR0frhBNO8Gx/+eWXys/P92y/+uqrnu9jYmJ0/PHHB7QfAAAAAPBGmA8AAAAAgKQNGzZYtg899FCfjtu6dasKCgoC0VKPExsbqyFDhni2ly9frsrKSp+PX7RoUSDaCmlRUVE64YQTNHv2bMXGxnrqn3/+udxud5v9HQ7Hbj+W9xT0P/30k8/HlpSUaNOmTZ7tPffc03JhR6C0nmrf7XZ7AvwNGzbou+++89w2bty4gF9cAAAAAADeCPMBAAAAAJDahMYJCQk+HffWW28Fop0e6/e//73n+7q6Or377rs+Hbdu3TrWgt+JgQMHar/99vNsV1dXe6aXby0qKsqy3dDQ4PNjjBo1yrL93nvv+Xzs22+/bZkZo3WvgbTHHnvogAMO8GzPnz9fhmFo7ty5lv0mTZrULf0AAAAAQGuE+QAAAAAASG1G3bae8rsjJSUlmjNnTmAa6qG8Q9MHHnhA5eXlOz3GMAzddtttgWwrLHhfoBIZGdlmH++/g84sIfH73/9emZmZnu3PP/9cy5Yt2+VxVVVVeuqppyy17pzSvvXo/E2bNunLL7/U66+/7qkNHDjQEvgDAAAAQHchzAcAAAAAQNLQoUMt27Nnz97p/jU1NbriiitUXFwcyLZ6nCFDhujwww/3bBcWFuqCCy5QaWlpu/s3NDTopptu0hdffNFdLQaF999/X2vWrPF5/6KiIn3zzTee7YyMDCUlJbXZLyYmRr179/Zsf//99+1Ox9+eyMhInXLKKZ7tpqYmXXPNNR3+7nbsc+ONN2rbtm2e2n777aeRI0f69Jj+MG7cOKWkpHi2b7zxRstFDIzKBwAAAGAXwnwAAAAAACT98Y9/tKwpPn/+fN1+++3trtn+/fff69RTT9W3334rh8NhCQLRdbNmzbKMIl+yZImOOuooPfjgg/r++++1fv16LV26VM8//7xOPPFEvfjii5LMULan+Oyzz3Tsscfq7LPP1iuvvKKCgoIO9/3+++81ZcoUy3P5uOOO63D/1qPQN27cqOnTp+vzzz/XunXrtHnzZs9X6wB+h3PPPVcDBw70bK9du1annnqqZf35HTZt2qRp06bpnXfe8dQiIyM1a9asDnsLhKioKI0fP96zvXXrVks/J554Yrf2AwAAAAA7RNjdAAAAAAAAwSAtLU1Tp07VI4884qnNmTNHr7zyivbbbz+lp6ersrJSK1eu1G+//ebZZ+rUqVq2bFm7YSV2T69evfTwww9r2rRpqqmpkSSVlpbqoYce0kMPPdTuMUceeaROO+00vf/++56aw+Holn7tYhiGvvnmG8+I++zsbA0aNEjJycmKjIxUeXm5Vq5cqfz8fMtxOTk5uvjiizu839NPP92yhv3HH3+sjz/+uM1+OTk5WrBggaUWExOje++9V1OmTNH27dslSevXr9eZZ56pfv36aciQIYqKitLmzZu1bNkyz2NI5u/r+uuv11577bV7/yBdcPLJJ7e7ZMaYMWOUlpbW7f0AAAAAgESYDwAAAACAxyWXXKK1a9fqgw8+8NSqq6v19ddft7v/5MmTNWPGDE2ZMqW7Wuwx/u///k9z5szRzJkztW7dup3ue8455+jqq6/Wl19+aanHxcUFssWgk5+f3ya49zZ06FA9/vjjSkxM7HCfUaNG6dprr9Xdd9/t8xT7rQ0fPlzPP/+8pk2bZrnwZePGjdq4cWO7x0RHR+vmm2+2jJDvTnvssYcOPPBALVq0yFKfOHGiLf0AAAAAgESYDwAAAACAh8vl0v3336/nnntOTzzxhGXd7NZGjRqlc845R3/5y1+6ucOeZb/99tMbb7yhd955R++//75WrVqloqIixcfHq3fv3jrooIM0ceJEDRkyRJJUUVFhOX5ngXWou+KKK7TPPvvos88+05IlS9pdDqK1oUOHavLkyTrllFMUEbHrj4OmTp2q0aNHa/78+Vq8eLE2bNigyspK1dfX+9TfsGHD9O6772r27Nl68cUXO1wGIC4uTkceeaSmT5+uPn36+HTfgTJ58mRLmN+nTx8deuihNnYEAAAAoKdzGK3nMwMAAAAAAJKkhoYGLV26VCtXrtT27duVkJCgzMxMDR8+XH379rW7PbTjgQce0MMPP+zZfvPNNzVs2DAbO+oeTU1NWrdunfLy8rRt2zZVVVVJkuLj49WrVy/ttddeysnJsbXHX3/9VStXrlRpaakaGhqUmpqqvn37av/991dUVJStve3w2Wef6YILLvBsX3rppbrkkkts7AgAAABAT0eYDwAAAAAAwsKUKVP07bffSjKnbV+8eLFPo9ABSZo+fbpniQ2n06kFCxaod+/eNncFAAAAoCdz2t0AAAAAAABAV23cuFELFy70bA8fPpwgHz4rKirSggULPNuHHnooQT4AAAAA2/GuNkzU19fr+++/15YtW1RSUqK0tDTl5OTogAMOCJrp6gAAAAAACATDMDRr1iy1nnzw2GOPtbEjhJoXXnhBDQ0Nnu1TTz3Vxm4AAAAAwESY30n19fVauXKlli1bpp9//lk///yz1q5dK7fb7dln5cqV3dZPbW2tHnjgAb366qsqKytrc3tKSoomTJig6dOnKyYmptv6AgAAAACgK5544gmlpKRo/PjxO71IvbKyUjfccIO++uorTy0xMVHHH398d7SJMLB582bNmTPHs923b18ddthh9jUEAAAAAM0I8zth4sSJWrFiheVKbTtt2bJF559/vtasWdPhPmVlZXrqqaf0+eef64knnlBOTk43dggAAAAAwO7Ztm2b7rnnHt1zzz068sgj9bvf/U4DBw5UcnKyampqtG3bNi1cuFDz589vc3H73/72NyUlJdnTOILe5s2bJUlVVVVatmyZHnroIVVXV3tuv+iii+RyuexqDwAAAAA8HEbrOeiwU8OGDfNpv+4YmV9ZWalTTz1Vq1at8tT22GMPHX300crOzta2bdv07rvvat26dZ7bhw4dqhdffFEJCQkB7w8AAAAAgK64+eab9cILL3T6uHPPPVczZswIQEcIFzv7fGfUqFH673//K6fT2Y0dAQAAAED7GJm/mxISEjR8+HCNGDFCixcv1pIlS7r18f/1r39Zgvy//vWvmjFjhhwOh6d2ySWX6K677tLTTz8tSVq1apXuuece/eMf/+jWXgEAAAAA6Kzk5ORO7Z+dna0rr7xS48ePD0xDCHu5ubn697//TZAPAAAAIGgwMr8TbrnlFu2zzz4aMWKEBg0a5AnOr7vuOr322mue/QI9Mn/Tpk066qijPNP9H3744Xrsscc63H/atGn69NNPJUmRkZF677331Ldv34D2CAAAAABAV23YsEH/+9//tGTJEq1bt07btm1TVVWVDMNQYmKi0tPTNWLECB1yyCE68sgjFRUVZXfLCAGtR+bHxMSof//+Gjt2rKZOnarExEQbOwMAAAAAK8J8P+juMP+uu+7SU089JUlyOBx6//33NWDAgA73z8vL05FHHunZ/utf/6prrrkmoD0CAAAAAAAAAAAAAHYf84aFoE8++cTz/YEHHrjTIF+SBgwYoAMPPLDd4wEAAAAAAAAAAAAAwYcwP8Rs2LBBeXl5nu1DDjnEp+Na75eXl6eNGzf6uzUAAAAAAAAAAAAAgJ8Q5oeYVatWWbb3228/n44bNWrUTu8HAAAAAAAAAAAAABA8CPNDzNq1ay3b/fr18+m4vn377vR+AAAAAAAAAAAAAADBgzA/xGzevNnzvdPpVHZ2tk/HZWdny+ls+XVv2rTJ770BAAAAAAAAAAAAAPwjwu4G0DmVlZWe7+Pj4xUR4duvMDIyUrGxsaqqqpIkz3+7S319vcrKyjzb0dHRcrlc3doDAAAAAAAAAAAAAASC2+1WXV2dZzslJUVRUVFduk/C/BBTXV3t+T46OrpTx8bExHhC/Nb30x3KysqYDQAAAAAAAAAAAABAj5GVldWl45lmP8S0vpojMjKyU8e2vvKjtrbWbz0BAAAAAAAAAAAAAPyLMD/EtB6N39DQ0Klj6+vrPd/HxMT4rScAAAAAAAAAAAAAgH8xzX6IiYuL83zfepS+L1qPxm99P93Be0mAvn37dnsP4WbNmjVyu91yuVwaPHiw3e0AQFjhHAsAHSuqN/R5mfRZmfRTpWT4cEy0Qzo4WTosRepVsl6R7gbVOSO1JXWgPiuTFlVIDT7ckcsh7Z8g/SlVGp0spUY6uvSzAEA44rUsAAQO51j7fFtu6O6NUkE7YxyjHdIFOdKETGlLnfle5bNSaVWNb/ed4DLfX/wpRTogSYpy8j4DsEM4nGOrq6sty453dsn09hDmh5iEhATP99XV1WpsbFRExK5/jY2Njaqpafk/V3x8fED664jL5bJsx8XFWX4WdJ7T6ZTb7ZbT6eTfEgD8jHMsAFhtqTP0aqE0r0D6qty3AD/OKR2TLk3Iko5OkxIizA/ElpZJDW7zA7NJAxI1SVJ5o6G3iqR5hdL7xVL9Th5gcZn0ZJkZ7P8pRZqYKZ2YKWVF8YEbAEi8lgWAQOIc2/3KGgxduUaas6392w9LkZ7cU9oj1nw/kJQo7ZUhXShpbY2hVwvM9xnfV+zkQdzSV4XSHYVScoR0fLo0MUv6c6oU4+J9BtBdwvEc652P7g7C/BCTm5vr+d7tdis/P185OTm7PG7btm1qamrybPft2zcg/QEAAAAID5tqWwL8r7f7dky8Szo23RwRc1S6FO/jB1/JEQ6d0Us6o5e0vdHQ28Xm475XItU1tX+M25A+KTW/Ll4lHZZiaGKWdGKG1CuaD9wAAACAUPd2kaFpK6Xf6tveFu+S7txDmtZHcjraf/2/R6xD1/SXrukvra9peX/z3U6C/fJG6bl88yvRJR2fYWhipnRkGsE+AHsQ5oeYQYMGWbY3btzoU5jfekqH9u4HAAAAADbUtoxc+dbHAD/BJR3XPHLlyDQprosfcCVFOHRatnRatlTRaOidYrOfd4ul2g6C/SZJn5aZX5eskv6YYn7gdlKm1JtgHwAAAAgpJQ2GLl8tPZ/f/u1HpEr/GSYNiPX9tf7AWIeu7idd3c/39z0VbumFfPPLfN9jaEKWdFSaFEuwD6CbEOaHmGHDhlm2f/zxRx188MG7PG7JkiWW7aFDh/q1LwAAAAChydcRKq2ZI1TMKe7/EsAPshIjHDolWzolW6psNPRuidnnO8VSTQfBviHp8zLza/pq6dBk8wO3CZlSDsE+AAAAENReLzR04Sopv53R+Iku6e7B0nm9JUcHo/F90T/GoSv7SVf2831Gskq39GKB+WXOSGZoQqZ0dHrXL2gGgJ0hzA8x/fv3V//+/bVhwwZJ0tdff60LL7xwl8d9/fXXnu8HDBig/v37B6xHAAAAAMFtXY2huQXSq7taO7KVJJd0QoZ9a0cmRDh0cpZ0cpZU5Tb0XvOI/beLpOqdBPtflJtfl6+W/pBsfuA2MVPKjeEDNwAAACBYFNUbmr5aeqmg/duPTJMeHyb18/Pr+L4xDl3eV7q8r7SlriXY/6rcfD/Rniq39HKB+RXnlI5ON5f8OqYTS40BgK8I80PQEUccoaefflqStGjRIuXl5WnAgAEd7p+Xl6dFixZ5tseMGRPoFgEAQJipdRtaV2uuUY0W0U5pUIwU4eTNusTzJNjVNUkflUqvFkiLK307JiWiOcDPlMamSdFB8lyPdzk0Mcu8sKDabej95hH7bxebI2Y68lW5+XXlGungJDPYPzxVigiOHwtAiMuIZGmP1grrDW1rZ1QlgoPLYb6OZf1nU2OT+Tq2roMLBIEdVjdGq9HtUoQjQo5Ke9/4pERIudFdG6EeLOYVGLp4lVTY0Pa25Ajp3sHS2b0C/7PmRDs0PVeaniv9VmdofqF5AfT/yjoO9qubzIuM5xVKsU7pqHRzya+94wPaqs+SIqR+YfI88YfyRkMba+3uwuSQNDCWC0Cwa4T5QWLMmDHasmWLJCknJ0cLFizocN9TTz1Vzz33nBoaGmQYhu688049+uijHe5/xx13eL6PjIzUaaed5r/GAQBA2KpyG3q3uGVK645GvvZ0mZHS+EzzzfrhKT0v2K9uHiH9auGug1SEhtTmAH9SlrkWZVSQP6fjXA6dlCmdlCnVuA19UGI+H98sMte47Mg3280vAPCnUQnmhUKTsqQhccF9/gwEX9cgRnDYMU30xEzpqB44TXRjk6HPyqS5hdLrhe2HiEBbrZavXdTxXt1lzzhpYqahSVnSPvGhF9gW1Bu6ZJX5/432HJMuPTbMnuWy+kQ7dEmudEmutK3O0Pwi88Loz8ukjj4eqWmS5heaX8FkaKw0IcvQpExp34TQe550VUG9odeaL8z4tCy4BiBEO6Uj08zXj8dnSMlcaY52EOaHoH79+umkk07Syy+/LElasGCB7r77bl199dWWk7BhGLr77rv16aefemoTJkxQ3759u71nAAAQGiobDb3TPHX1uztZkxotChuk//xmfqVHSuMzzA9Ex6RKkUEegu6uHRd6vFpoXuhRRYAf8sznrkL+uRvrcmh8pjQ+05wp4sPmmQjeKJK28zwF0A2WVJpfN6yX9m0V7A8L42B/fU3LlMTf+bh0C4KD9zTRxzRPE310GE8T3dBk6NMyaW6B9HqRVEyAjxC3olq6ZYP5NTRWmphlvh8N9sDWMAy9VCBNX93+32FqhHTfEOmM7OD4OXpFO3RRjnRRjpTfOhgu7TjYDyaraqTbN5hfg2OlCc0XgIwK8udJV+TXN8+sUCB9Vha8v6e6JvNC9DeLpCiH9JfmYP+EDCklMjx/N+g8h2EYQXQNSnB79tln9dxzz7WpFxcXq6qqyrPdr1+/Nvv06tWr3WN36MzIfEmqrKzU5MmTtWbNGk9t8ODBOuqoo5Sdna38/Hy98847Wrdunef2IUOG6KWXXlJCQsJO7zsQKisrtXLlSs/2sGHDbOkjnCxdulQNDQ2KjIzUyJEj7W4HAMJKTzvHVjQaeqvYfIPzXolUG6zvcEJMqI1u3hUu9Ag/GZHSic3rx/8ppfsCfDvOsXVNhj4uMZ+/bxRJZY3d8rAA4LFPvHm+nZQl7RUf2PNtd5xn19UYmltgBhnfE+CHnVinGehPzDRHxSaE+CjB+iZDn5Q2vw4olEp4HYAeYHCs+Tc8MQgD2211hi5aZV5Q054TMqRHhobG0jWF9YZeax6xv6AsuEZ8+2JQjDQhS5qUKf0uMbieJ7tja/PSCPN2sTRCKIh0SGNTzb/hEzKktB4S7IfDZ7KByEMZmd8J5eXl2rhx4y73a28ft9u/w0ASEhL0+OOP67zzzvME9mvWrNGDDz7Y7v6DBg3SY489RoAOAAAkmWuEvVVkfgD7fonva0P2jLcOvtvZG8PSRmnONvPLXHfcCLp1x3elotHQ280j8N8t9v1Cj9D46XqmPtHSsc0fzh+W0nOWhYh2OnRMhnRMRssH+nMLzOc1U+oC8IddfVi8rMr8mpUnDY8zRz9PypKGx4XOB+drqg3NbR7htrjS9+NC46freXb2nK1pMl//vVooxTilo9LM5+yx6VJiiAT79U2GPmpeeuf1TlzIFxo/HezV+q/HvmfMrv6/s6ZGumOj+WUGtobtga1hGHo+X7p8tfl+2Vt6pPTgEGlyVuj8vzEzyqHz+0jn95GK6g29UWQGyV+UBccF8Lt6nqyrle7eaH4NiDFH7E/MlA5KCp3fwZYdAX6B9GW57wF+MPx0O+u1wTAH3LxXIl3gkI5INX834zOl9B4S7KMFYX4Iy83N1Wuvvab7779fr776qsrLy9vsk5ycrAkTJuiyyy5TTEyMDV0CAIBgUdZg6M3mEfgflEj1PrzDcTmkMSnmldonZphvVNFibU3LurA7G5VW1ig9s838So6Qjm+ewvTPqVJMkE1hur35Qo95nbjQwylzZPeELHPd8myeJwhiUU6Hjko31wYGAH9pbDL0Rbl5odBrRVJ+fcf7Lq+Wbs4zv4J9reNV1S0j8H/0McAPt9mJwtWOaaLn7WL64dom8zn9WpG5ru+4NDNMOC5DSgqyYL+uydCHJebP9GaxVO5jgH9AojSheQTzHrHB9TMh+Cxd+nNQjBptMgwt3C7POXpTXcf7thfYTsqSDuzGYH9LnaELV0pvF7d/+8RM6cGhof1eMiPKob/2kf7ax+5OWhiGoUUVLc+TvNqO982rle7ZZH71izYvAJmYKf0+SXIG2euTzbXNS/wUSl+1jcXaFeeUjm1eWu6oIFlOprTBnClzXoH04U4+p2s0zM/xPiiRpq2SxqSYnymN53O6HoNp9sNEfX29Fi1apC1btqi0tFSpqanKycnRgQceqKioKLvbY5r9AAiH6UYAIFiF0zm2tKH5yvAC6aNS88reXYlwmB+8Tsg03xhk8MbAJ7uzXmyiSzq++c3kkWn2BfvljYbebH6edOZCj8NTzOfJiZlSFs8T+CiczrEA0B63YejLMmluoTS/UNq2k2C/NXOtY3V5reOunmdXVJkj8OcVSD9X7Xp/yRxNOb75Nc2Y1O5bugX+UVBv6PXm14Kflvk2TXSUw3z9OjFLOi7dvnV9a92GPmheSuetImm7j5OjHpRoXog6MVMaSICPTgjG17KGYei77eb/d14tlDbsJLBtzQxszSnWfx+gkdiGYWjONunKNe1fYJMZKT08VJqYxd9hoBmGoR8q5JllZ52Pz5Pc6OYLnjKlg5PtC/Y31rZ85vLNdt+OSXA1z0yXJY1Lk+KCIMDvSHmrgRUf+DiwwuVoHliRaQ6sCIfPZYLxHNtZgchDCfPRLQjz/S8cTmoAEKxC/Rxb3CrA/7jUvIJ3VyIc5ijxCc1rcTFlV9dsqG0Zsf9tJ95kHpdu/g6OSpNiA/wms6yhZQrAD0t8u9DD5ZCOSJHnCnAu9MDuCPVzLAB0htsw9HXziP35hdJvPgb7XVnreHfOs8urzBH48wqlX3wM8DMizQv6JmaaHyQT4IeHolbB/idlvgX7kQ7pL2nmc+H4DCk1wO8latyG3m8egf9WsVTpY4D/f0lmjxOypP4xPF+xe4L9taxhGPq+wjyfzyuQ1vsY2PaNNsPASVnm34o/AttNtYbOX2kGk+05JUt6YAjvK+1gGIaWVJrPkXmF5rIMvugT1fI8+UM3BPt5NYbmNV+kstDHz1YSXebsMTsGTQT6s5VA2N685OG8AnOafV9nTDwspXnGxAypV3To/dxS8J9jfUGYj5BFmO9/4XBSA4BgFYrn2NYfui0o8y3A7+4P3XqqTa2uHv/axzef8c1Xj0/IlI5O99/V4yVeF3r4OlPD2FQzTOBCD/hDKJ5jAcAfmgxD35S3jJzcspMpkVsbFNMyYt+XtY59Oc8ahqFfquQZgf9rtW+9ZLUK8A9LkSII8MPajouEX22e5cvX9xhjmy8SHp8hpfnptWO129B7xWbo9HaxVOVjgH9Ikvn3MyFT6kuADz8IpdeyhmFocavAdq2PgW3OjmA/UzpkNwJbwzD05Fbp6jVSRTt/q9lR0qNDpfGZ/E0GA8Mw9FNlywUgq3x8nvSOMl8TTMqUDk2RXH4K9tfXtCzxs8jHWQ+TXObnFROypL8E4XKGXVHRaOid5v//vVtsLn+zKw5Jf0wx/983IVPqHULBfiidYztCmI+QRZjvf+FwUgOAYBUq59iC5nUuXy0Mvekwe6otdWaw/2qB9GW55MsL8TindEzziP1jdmNdt+IGQ683ryX3SSc+hP1zqwCfCz3gT6FyjgWAQNqx1vG85tcFG30M9s21jrXTtY47Os8ahqGfq+QZgb/SxwA/O6ol1Bmd4r8P6xFadizf9WonZnXq6vJdVW5D7zaPTHynWKr2McD4Q7L5OvakDCmXAB9+FqqvZXcEtjsu4lrdicD2pOaLuHwJbPNqzNH4H5e2f/sZ2dJ9Q/x3oQ/8yzAMLWv1WmFFJ14rnJhhvj4Zndz5i/3Wtgrwf/AxwE+OMP/fMiFT+nOaFN0DLjCsbDT0bknL/xdrOvn/xQmZUk6QB/uheo5tjTAfIYsw3//C4aQGAMEqmM+x+fWG5jd/6PtZmeTD63ZFO821wSZmmlONJUUE9wv3nuK3uubfZaH0vzLfgv1YpzlSf0KmOXI/oYPfZaHXTA2dudBjQpZ0PBd6IICC+RwLAHbYsdbxvOaL77q61nHr8+yIESO6FN7smEaXAB+tlTUYerPYfE/yQYlU7+NyTWNSzOfsiRlSZgfBfqXXCERfg4rROwL8TKlPkAcVCG3h8Fp2dy/u6hXVMjvLH1Os/29oMgw99pt03dr2l77oEyU9Nkw6NoO/z1BhGIaWVzc/Twqk5T4+TzIjW0bs72wWn9XVLUv8/Fjp232nRpgDDiZlmReLRfWAAL8jVc0z1rwaZjPWhMM5ljAfIYsw3//C4aQGAMEq2M6xW71CX18C/Binue76xCwz9E0kwA9q2+oMvdYcvn9e5vvveFyr33FtkzzPk8/KfAvwd1zoMaH5Qo9knifoBsF2jgWAYOKPtY7j8n7WsroIfeZO1/8cWT6vg9vVaZXRM5U3Gnq7yHzOvu/jur4uh/SnFPM16EmZ5gWrbzVfHPBeiW9TCIfL2sAIPeH2WtYfy670jZGmrTTfh7bn7F7SvYO5YDzULa8yPEs2LKvy7ZiMSHP0/KQs87y/rrbl4oClPt5HWoQ0vvn1yZhUKbIHB/gdqXYber/E/P/oW8XtX1DTnv9LMv+GJ2RJ/YMk2A+HcyxhPkIWYb7/hcNJDQBa+7jE0FNbpU0+fmAZSFXVVTIMQw6HQ/Fx8bb2UtNkXqHs66jtY5pHbR+zk1HbCG759S3T4n9a6luwH+Uwp8/vzIUeE5ovAmCmBnQ3XscCgG8Mw9CSypaRk76udRyjJtXK6dO+faPN144Ts8wPdAnw0RXbd4yq72QoH+HwbXS/U9LhzdP2n5gpZXdy2n7AH8L9tezyqpbpzn0NbDuSGy09MUwal87farhZUWWYSwUVSj/5OKo+zunbcilS24sACPB9V+M29GGJ+drxzSKpwsdg//dJ0n2Dpd8n2/tvHQ7nWMJ8hCzCfP8Lh5MaAEjmFI1XrpHmbLO7k9AU55SObV4j7OjdWE8dwW13pstvz47p+Sc2P0+YqQF24nUsAHTe7q513J5+0WZ4PzFTOogAHwGyO9Plt8fX6fmB7tKTXsuuqDI0t3mZP19HUe9wbm/p7sHM/tYTrKo2R+y/Wigt8THYb4+v0/PDd7VuQx+Vmq8d3yiStu8i2I92Svl/sHfQRzicYwORh0Z0tSkAAIDd9XaRoWkrpd/q7e4ktMS7pOOaR+AflS7FEeCHrcwoh87rI53XRypuaBmx/0mpORJ/Z+KaZ2qYmGWOxGemBgAAQpfD4dB+idJ+idItAzu/1vGAGDO8n5glHZho3h8QSAkRDk3OliZnm+v6vts8Yv+d4l2PzIxwmGshT8w010bOIMAHbLFnvEM3xks3DmgJbHe1vnn/GOk/w6Sxafzd9hRD4xy6foB0/QBpTbWhV5s/t/ihYtfHZkeZy61MzJT+mCK5eH3iVzEuh47LMJdVrGsy9HHziP03iqSyxrb71zWZF4z+LrH7e8XOEeYDAIBuV9Jg6PLV0vP5dncSOhJd5ovviZnSkWlSLAF+j5Me6dBf+0h/7WP+Db1RZI6Q+KhUamgO9uNd5tT5E7nQAwCAsOVwODQyQRqZIN08sGWt41cLpOWtgv1cZ51Oy43WpCxp/wQCfNgn3uXQpCxzuuRqt6H3is0RnG8VS1XNowQjHdLYVPOCkxMypDTW1gaCindgO695ppjFrYL9C3OkOwYxE1xPNjjOoWv7S9f2l9bXtDxPFrUK9nu3CvAPTSHA7y7RToeOyZCOyZDqmwx9UmoG+68XSqXNwf5BidIoJtQOSoT5AACgW71eaOjCVVJ+O6PxE13SNf2kzKju76u1zZs3y+12y+VyKTc3195mZK5leniKeUUtIJkfbk7tLU3tLZU2GFq43RzB9IdkLvQAAKAncTgc2idB2idBummgudbxgl/zlNZUo72jG7XvHqE5PSnCV5zLoQlZ5rT5NW5DX5dL9Yb0f0lSKgE+EBIGxzl0XX/puv7SuhpDi7ZLIxKk4fH8DaPFwFiHZvSTZvSTNtQaWlYppUeyxE8wiHI6dFS6OQjksaHm/4urmsyL6vjdBCfCfAAA0C2K6g1NXy29VND+7ePSpMeHSX1j7H/RuLSotGV9pj597W4H2KnUSIfGpdvdBQAACAbD4x1qjKpQQ0ODHI5Iu9sBdirW5dARaXZ3AaArBsU6NCjW7i4Q7PrHONQ/xu4u0J5Ip0OHpdrdBXaFMB8AAATcvAJDF6+SChva3pYcIf17sDSlF1N/AgAAAAAAAACwA2E+AAAImIJ6Q5esMtdgas+x6dKjw6ScaEJ8AAAAAAAAAABaI8wHAAB+ZxiGXiqQpq+WitsZjZ8aId0/RDo9m9H4AAAAAAAAAAC0hzAfAAD41bY6Qxetkl4vav/28RnSw0Ol3ozGBwAAAAAAAACgQ4T5AADALwzD0PP50uWrpdLGtrenR0oPDpEmZzEaHwAAAAAAAACAXSHMBwAAXbalztCFK6W3i9u/fVKm9OBQKSuKEB8AAAAAAAAAAF8Q5gMAgN1mGIbmbJOuXCOVtzMaPzPSnFJ/YhYhPgAAAAAAAAAAnUGYDwAAdsumWkPnr5Q+KGn/9lOypAeGSBmMxgcAAAAAAAAAoNMI8wEAQKcYhqEnt0pXr5Eq3G1vz46SHh0qjc8kxAcAAAAAAAAAYHcR5gMAAJ/l1Zij8T8ubf/2M7Olfw+R0iIJ8gEAAAAAAAAA6ArCfAAAsEtNhqHHfpOuXStVtTMav0+U9Ngw6dgMQnwAAAAAAAAAAPyBMB8AAOzU2hpD562QPitr//aze0n3DpZSGI0PAAAAAAAAAIDfEOYDAIB2NRmGHtoiXb9Wqm5qe3tutPTEMGlcOiE+AAAAAAAAAAD+RpgPAADaWF1t6K8rpC/L27/93N7S3YOl5AiCfAAAAAAAAAAAAoEwHwAAeLgNQ/dtkm5cL9W2Mxq/f4z0n2HS2DRCfAAAAAAAAAAAAokwHwAASJJWVBk6Z4X07fb2b78wR7pjkJTIaHwAAAAAAAAAAAKOMB8AgB6uscnQPZukWXlSXTuj8QfGSE/uKR2eSogPAAAAAAAAAEB3IcwHAPQYm2oNnb9S+rpcaiez7rHcRvtT6kvSpbnSbYOkeBdBPgAAAAAAAAAA3YkwHwDQI2yoNTRmibS+1u5OQsPgWOmpPaXRKYT4AAAAAAAAAADYgTAfABD28moMjflRyiPI3yWHpMv7Sv8cKMUxGh8AAAAAAAAAANsQ5gMAwtr65iB/A0H+Lu0dLz0+TDokmRAfAAAAAAAAAAC7EeYDAMLWuhpzav2Nddb63vHSI0OlKDJrj+QIaVic5HDwjwIAAAAAAAAAQDAgzAcAhKW1zUH+Jq8gf5946eP9pCySfAAAAAAAAAAAEMQI8wEAYWdNtTm1/mavIH9Ec5CfSZAPAAAAAAAAAACCnNPuBgAA8KfV1YYO/7FtkL9vgvTJfgT5AAAAAAAAAAAgNDAyHwAQNlZVm1Pr/1Zvre+XIH20n5QeSZAPAAAAAAAAAABCAyPzAQBhYWW1ocPbCfJHJZhT6xPkAwAAAAAAAACAUEKYDwAIeb9WmUH+Vq8g/3eJ5oj8NIJ8AAAAAAAAAAAQYgjzAQAhbXmVoTE/Stu8gvwDEqUP9yXIBwAAAAAAAAAAoSnC7gYAANhdv1QZOmKJVNBgrR+YKH2wr5RCkA8AAAAAAAAAAEIUYT4AICQtqzR0xI9SoVeQ//sk6f19peQIgnwAAAAAAAAAABC6mGYfABByfq40p9b3DvIPTjJH5BPkAwAAAAAAAACAUEeYDwAIKT81B/lFXkH+IUnSe/tKSQT5AAAAAAAAAAAgDDDNPgAgZPxYYejPP0nFXkH+H5Kld0dKiQT5AAAAAAAAAAAgTDAyHwAQEhZXGBr7Y9sgfzRBPgAAAAAAAAAACEOE+QCAoPdDhaE//yiVNFrrf0yW3iHIBwAAAAAAAAAAYYhp9gEAQe377Yb+8pNU5hXk/ylFemukFO8iyAcAAAAAAAAAAOGHkfkAgKC1aLuhP7cT5B+eQpAPAAAAAAAAAADCGyPzAQBBaWG5oSN/kra7rfUjUqU3RkhxBPkAAAAAAAAAACCMMTIfABB0vu0gyP9zqvQmQT4AAAAAAAAAAOgBCPMBAEHlmw6C/L+kSq+PkGIJ8gEAAAAAAAAAQA9AmA8ACBpflZlBfoVXkD8ujSAfAAAAAAAAAAD0LIT5AICg8GWZoaOWSpVeQf5RadL8faQYgnwAAAAAAAAAANCDEOYDAGz3vw6C/GPSpfkjCPIBAAAAAAAAAEDPQ5gPALDV56WGjv5JqvIK8o9Ll+btI0U7CfIBAAAAAAAAAEDPQ5gPALDNp6WGjlkqVTdZ6ydkSHMJ8gEAAAAAAAAAQA9GmA8AsMWCUkPHthPkj8+QXt5biiLIBwAAAAAAAAAAPRhhPgCg231cYgb5NV5B/kmZBPkAAAAAAAAAAAASYT4AoJt9VGLo+J+lWq8gf0Km9OJwKZIgHwAAAAAAAAAAgDAfANB9PihuP8iflCn9lyAfAAAAAAAAAADAgzAfANAt3is2NH6ZVOcV5E/Okl4gyAcAAAAAAAAAALAgzAcABNy7xYZO/LltkH9qlvTcXlIEQT4AAAAAAAAAAIAFYT4AIKDyagydvEyqN6z107OlZwjyAQAAAAAAAAAA2kWYDwAIqMvXSNVeI/LPzJbmEOQDAAAAAAAAAAB0iDAfABAwbxcZerPIWjslS3p6L8nlIMgHAAAAAAAAAADoCGE+ACAgatyGLlttrWVFSo8MJcgHAAAAAAAAAADYFcJ8AEBA3L5BWl9rrd01WEqJJMgHAAAAAAAAAADYFcJ8AIDfrak2dNdGa210snRmtj39AAAAAAAAAAAAhBrCfACAXxmGoemrpXqjpeZySA8NlRxMrw8AAAAAAAAAAOATwnwAgF+9ViS9X2KtTc+VRiQQ5AMAAAAAAAAAAPiKMB8A4DdVbkNXrLbWekdJ/xhgSzsAAAAAAAAAAAAhizAfAOA3/8yTNtVZa/cMlpIiGJUPAAAAAAAAAADQGYT5AAC/+LXK0L2brLUxKdLkLFvaAQAAAAAAAAAACGmE+QCALjMMQ5eskhqNllqkQ3poqORwMCofAAAAAAAAAACgswjzAQBd9lKB9GmZtXZlX2nPeIJ8AAAAAAAAAACA3UGYDwDoku2Nhq5eY631i5ZuGGBLOwAAAAAAAAAAAGGBMB8A0CWz1ktb6621fw+R4l2MygcAAAAAAAAAANhdhPkAgN32c6WhB7dYa0elSeMz7OkHAAAAAAAAAAAgXBDmAwB2i2EYuniV5DZaatFO6YGhksPBqHwAAAAAAAAAAICuIMwHAOyWZ7dJX5Zba9f0k/aIJcgHAAAAAAAAAADoKsJ8AECnlTYYumattTYwRrqunz39AAAAAAAAAAAAhBvCfABAp92wXipssNYeGCLFuhiVDwAAAAAAAAAA4A+E+QCATvmhwtBjW6y1EzKkYzII8gEAAAAAAAAAAPyFMB8A4LMmw9DFKyWjVS3WKd03xLaWAAAAAAAAAAAAwhJhPgDAZ09ulb6rsNb+1l/qH8OofAAAAAAAAAAAAH8izAcA+KSo3tD1a621obHSVf3s6QcAAAAAAAAAACCcEeYDAHwyc51U0mitPTRUinYyKh8AAAAAAAAAAMDfCPMBALv0bbmhp7ZaaydnSWPTCPIBAAAAAAAAAAACgTAfALBTbsPQxaustXiX9K897OkHAAAAAAAAAACgJyDMBwDs1KNbpCWV1to/Bki5MYzKBwAAAAAAAAAACBTCfABAh/LrDd243lrbO166LNeefgAAAAAAAAAAAHoKwnwAQIeuWSOVN1prDw2RIp2MygcAAAAAAAAAAAgkwnwAQLv+V2bouXxr7Yxs6bBUgnwAAAAAAAAAAIBAI8wHALTR0GTo4lXWWpJLumsPe/oBAAAAAAAAAADoaQjzAQBtPLhZ+qXKWvvnIKlXNKPyAQAAAAAAAAAAugNhPgDAYkudoVl51tp+CdKFfWxpBwAAAAAAAAAAoEcizAcAWFy9Rqp0W2sPD5UinIzKBwAAAAAAAAAA6C6E+QAAj09KDL1cYK1N7S0dnEyQDwAAAAAAAAAA0J0I8wEAkqS6JkOXrLbWUiOkOwbZ0w8AAAAAAAAAAEBPRpgPAJAk3btJWlltrd02SMqMYlQ+AAAAAAAAAABAdyPMBwBoQ62hW/KstQMTpXP72NIOAAAAAAAAAABAj0eYDwDQFaulmqaWbYekh4dKLgej8gEAAAAAAAAAAOxAmA8APdy7xYZeL7LWLugjHZBEkA8AAAAAAAAAAGAXwnwA6MFq3Yamr7LWMiOlWwfZ0w8AAAAAAAAAAABMhPkA0IPduVFaV+tV20NKjWRUPgAAAAAAAAAAgJ0I8wGgh1pbY+iOjdbaH5Kls3rZ0w8AAAAAAAAAAABaEOYDQA9kGIYuWyXVNbXUnJIeGio5HYzKBwAAAAAAAAAAsBthPgD0QG8USe+WWGuX5Er7JhDkAwAAAAAAAAAABAPCfADoYarchi5fba31ipJuGmhPPwAAAAAAAAAAAGiLMB8Aephb86SNddbavwZLyRGMygcAAAAAAAAAAAgWhPkA0IOsqDJ0zyZr7fAU6dQsW9oBAAAAAAAAAABABwjzAaCHMAxDl66WGoyWWoRDenCo5HAwKh8AAAAAAAAAACCYEOYDQA8xt1D6pNRau6KvNDyeIB8AAAAAAAAAACDYEOYDQA9Q0WjoytXWWm60dGN/e/oBAAAAAAAAAADAzhHmA0APcFOe9Fu9tfbvwVJCBKPyAQAAAAAAAAAAghFhPgCEuWWVhu7fbK39JVU6KdOefgAAAAAAAAAAALBrhPkAEMYMw9AlqyS30VKLckgPDpUcDkblAwAAAAAAAAAABCvCfAAIY8/nS/8rt9Zm9JOGxBHkAwAAAAAAAAAABDPCfAAIU2UNhmassdYGxEgz+9vTDwAAAAAAAAAAAHxHmA8AYermPKmgwVq7f4gU52JUPgAAAAAAAAAAQLAjzAeAMPRbnaFHf7PWjkuXjssgyAcAAAAAAAAAAAgFhPkAEIbu2CDVNbVsRzikfw+xrx8AAAAAAAAAAAB0DmE+AISZLXWG/rPVWpvaWxoUy6h8AAAAAAAAAACAUEGYDwBh5navUfmRDulv/e3rBwAAAAAAAAAAAJ1HmA8AYWRTraEnf7PWzukt9YthVD4AAAAAAAAAAEAoIcwHgDBy+wap3mjZjnJI1zMqHwAAAAAAAAAAIOQQ5gNAmNhYa+iprdbauX2kvozKBwAAAAAAAAAACDmE+QAQJm7bIDW0GpUf7ZRmMiofAAAAAAAAAAAgJBHmA0AY2FBraLbXqPzzeks50YzKBwAAAAAAAAAACEWE+QAQBm7Ns47Kj3FK1zEqHwAAAAAAAAAAIGQR5gNAiFtfY2jONmvt/D5SH0blAwAAAAAAAAAAhCzCfAAIcbdukBq9RuVf28++fgAAAAAAAAAAANB1hPkAEMLW1Rh6xmtU/rQ+Um9G5QMAAAAAAAAAAIQ0wnwACGG35EnuVqPyY53Stf1tawcAAAAAAAAAAAB+QpgPACFqTbWh5/KttQtzpOwoRuUDAAAAAAAAAACEOsJ8AAhRt2ywjsqPc0rX9LOvHwAAAAAAAAAAAPgPYT4AhKBV1Yae32atXZQjZTEqHwAAAAAAAAAAICwQ5gNACLolT2pqtR3vkmYwKh8AAAAAAAAAACBsEOYDQIhZWW3ov/nW2sU5Uiaj8gEAAAAAAAAAAMIGYT4AhJh/5llH5Se4pKv72tUNAAAAAAAAAAAAAoEwHwBCyK9Vhl70GpV/aa6Uwah8AAAAAAAAAACAsEKYDwAh5J95ktFqO9ElXcWofAAAAAAAAAAAgLBDmA8AIeKXKkMvF1hr03OltEhG5QMAAAAAAAAAAIQbwnwACBHeo/KTXNKVjMoHAAAAAAAAAAAIS4T5ABACllUamus1Kv+yvlIqo/IBAAAAAAAAAADCEmE+AISAm/Oso/KTI6Qrcu3qBgAAAAAAAAAAAIFGmA8AQW5ppaF5hdba5blSCqPyAQAAAAAAAAAAwhZhPgAEuZvzrNspEdLlfW1pBQAAAAAAAAAAAN2EMB8AgtiPFYbme43Kv6KvlBzBqHwAAAAAAAAAAIBwRpgPAEHMe1R+aoR0Wa4trQAAAAAAAAAAAKAbEeYDQJBaUmHo9SJr7cq+UhKj8gEAAAAAAAAAAMIeYT4ABKmb8qzbaRHSpYzKBwAAAAAAAAAA6BEI8wEgCH2/3dCbXqPyr+rHqHwAAAAAAAAAAICegjAfAIKQ96j8jEjpkhxbWgEAAAAAAAAAAIANCPMBIMh8t93QO8XW2tV9pURG5QMAAAAAAAAAAPQYhPkAEGRuWm/dzoyULs61pxcAAAAAAAAAAADYgzAfAILIt+WG3iux1mb0k+JdjMoHAAAAAAAAAADoSQjzASCI3JRn3c6KlC7MsaUVAAAAAAAAAAAA2IgwHwCCxDflhj7wGpV/DaPyAQAAAAAAAAAAeiTCfAAIErPWW7ezo6RpjMoHAAAAAAAAAADokQjzASAIfFVm6KNSa+3aflIco/IBAAAAAAAAAAB6JMJ8AAgCs/Ks272jpAv62NIKAAAAAAAAAAAAggBhPgDY7IsyQ594j8rvL8UyKh8AAAAAAAAAAKDHIswHAJvNWm/d7hMlnd/bnl4AAAAAAAAAAAAQHAjzAcBGn5ca+rTMWruuvxTDqHwAAAAAAAAAAIAejTAfAGw0K8+6nRMtncuofAAAAAAAAAAAgB6PMB8AbPJpqaHPy6y16xmVDwAAAAAAAAAAABHmA4AtDMPQrPXWWt9o6RxG5QMAAAAAAAAAAECE+QBgiwWl0hfl1tr1/aVoJ6PyAQAAAAAAAAAAQJgPAN3OMAz9w2tUfv8YaSqj8gEAAAAAAAAAANCMMB8AutlHpdLX26216/tLUYzKBwAAAAAAAAAAQDPCfADoRoZhaJbXqPwBMdLZvezpBwAAAAAAAAAAAMGJMB8AutEHJdK3XqPy/9ZfimRUPgAAAAAAAAAAAFohzAeAbtLeqPxBMdJZjMoHAAAAAAAAAACAF8J8AOgm75VI31VYa38bwKh8AAAAAAAAAAAAtEWYDwDdoL1R+XvESmdm29MPAAAAAAAAAAAAghthPgB0g3eKpe+9RuXf0F+KYFQ+AAAAAAAAAAAA2kGYDwABZhiGbsqz1obESqczKh8AAAAAAAAAAAAdIMwHgAB7q1j6wXtU/gBG5QMAAAAAAAAAAKBjhPkAEECGYeim9dba0Fjp1Cx7+gEAAAAAAAAAAEBoIMwHgAB6o0haUmmt/X0go/IBAAAAAAAAAACwc4T5ABAgTYahm/Kstb3ipMmMygcAAAAAAAAAAMAuEOYDQIC8XiT95DUq/8YBksvBqHwAAAAAAAAAAADsHGE+AARAk2HopvXW2vA4aRKj8gEAAAAAAAAAAOADwnwACID5hdLPVdba3wcyKh8AAAAAAAAAAAC+IcwHAD9rMgzdlGet7RMvTcy0pR0AAAAAAAAAAACEIMJ8APCzuQXSL96j8gdITkblAwAAAAAAAAAAwEeE+QDgR8urDP1tnbU2Ml46iVH5AAAAAAAAAAAA6ATCfADwg8YmQ7dvMLT/ImldrfW2vw9kVD4AAAAAAAAAAAA6J8LuBgAg1C2rNHTOCun7ira37Z8gjc/o/p4AAAAAAAAAAAAQ2gjzAWA3NTQZumOjdEue1GC0vX1orDR3H0blAwAAAAAAAAAAoPMI8wFgN/xYYY7G/7Gy7W1OSVf2lW4aKMW6CPIBAAAAAAAAAADQeYT5ANAJ9U2Gbt0g3b5BamxnNP5ecdLTe0q/TybEBwAAAAAAAAAAwO4jzAcAH/1QYeicX6Wfq9re5nJIM/pKfx8gxTAaHwAAAAAAAAAAAF1EmA8Au1DXZOim9dLdmyR3O6Px94k3R+MfkESIDwAAAAAAAAAAAP8gzAeAnfhuuzkaf3l129siHNLM/tLf+ktRToJ8AAAAAAAAAAAA+A9hPgC0o8Zt6B/rpXs3SU3t3L5vgjR7T2m/REJ8AAAAAAAAAAAA+B9hPgB4+brc0F9XSCvbGY0f6TBH4s/sL0UyGh8AAAAAAAAAAAABQpgPAM2q3YZuWCfdv1ky2rl9/wRp9l7SiARCfAAAAAAAAAAAAAQWYT4ASPpfmaFzV0hratreFuWQ/j5AmtGP0fgAAAAAAAAAAADoHoT5AHq0KrehmWulh7a0f/tBidJTe0l7xxPiAwAAAAAAAAAAoPsQ5gPosT4tNUfjr69te1u0U7p5oHRFrhTBaHwAAAAAAAAAAAB0M8J8AD1ORaOha9ZKj//W/u0HJ0lP7SntyWh8AAAAAAAAAAAA2IQwH0CP8lGJofNWSBvr2t4W65RuGSRNz5VcDoJ8AAAAAAAAAAAA2IcwH0CPUN5o6Oo10lNb2799dLL05J7SkDhCfAAAAAAAAAAAANiPMB9A2Huv2NAFK6XN7YzGj3NKt+8hXZwjORmNDwAAAAAAAAAAgCBBmA8gbJU2GLpqjTRnW/u3/ynFHI0/KJYQHwAAAAAAAAAAAMGFMB9AWHqryNC0ldLW+ra3JbikO/eQLujDaHwAAAAAAAAAAAAEJ8J8AGGluMHQ5aulF/Lbv31sqvSfPaX+MYT4AAAAAAAAAAAACF6E+QDCxuuFhi5cJeW3Mxo/0SXdM1j6a2/JwWh8AAAAAAAAAAAABDnCfABh4bVCQxOWtX/buDTp8WFSX0bjAwAAAAAAAAAAIEQQ5gMIeYZh6Jq1bevJEdK/B0tTejEaHwAAAAAAAAAAAKGFMB9AyPuiXFpbY60dmy49OkzKiSbEBwAAAAAAAAAAQOghzAcQ8uZstW7vGSe9MYLR+AAAAAAAAAAAAAhdTrsbAICuqGw0NLfQWjubafUBAAAAAAAAAAAQ4gjzAYS0eYVSlbtl2ynpzF62tQMAAAAAAAAAAAD4BdPsd0FTU5MWL16sjRs3qqioSElJSerdu7cOPPBAxcXFdVsfmzZt0s8//6zCwkJVV1crNjZWaWlpGj58uAYNGiSnk2s2EL68p9gflyb1jmZUPgAAAAAAAAAAAEIbYf5ucLvdeuqpp/Tcc8+poKCgze1xcXE65phjNGPGDCUnJwekB8MwNG/ePD3zzDNavXp1h/vl5OTolFNO0dlnn62oqKiA9ALYZW2Nof+VW2tn97anFwAAAAAAAAAAAMCfGLLdSdu3b9cZZ5yhe+65p90gX5Kqq6s1d+5cHX/88Vq+fLnfe6isrNRZZ52lG264YadBviRt2bJF99xzj0466SRt3bp1p/sCocZ7VH5ahHRchj29AAAAAAAAAAAAAP7EyPxOaGxs1GWXXabFixd7an369NHxxx+vnJwclZSU6OOPP9bPP/8sSdq2bZumTZumuXPnKjs72y89GIahiy66SN99952nFhkZqTFjxmjUqFFKTk5WRUWFli1bpo8++kg1NTWSpNWrV+vss8/W66+/rtjYWL/0AtipyTD07DZr7bRsKdrJFPsAAAAAAAAAAAAIfYT5nTB79mx9/fXXnu1jjz1Wt99+u2X6+mnTpunZZ5/VbbfdJsMwlJ+frxtvvFFPPPGEX3p4++23tXDhQs/2gAED9Nhjj2ngwIFt9s3Pz9fFF1/subggLy9PTz31lC655BK/9ALYaUGptKnOWpvKFPsAAAAAAAAAAAAIE0yz76PKyko9+eSTnu3hw4frzjvvbHcd+rPOOkunn366Z/vzzz/XDz/84Jc+3njjDc/3TqdTDzzwQLtBviRlZ2frkUceUVxcnKf21ltv+aUPwG5zvEblj4yX9kuwpxcAAAAAAAAAAADA3wjzffTGG2+orKzMsz1jxgxFRHQ8scHll19umc7+2Wef9Usfy5cv93w/YsQIDRs2bKf7Z2Vl6Y9//KNnOy8vT7W1tX7pBbBLWYOh+YXW2tm9JYeDKfYBAAAAAAAAAAAQHgjzffTJJ594vs/JydHBBx+80/0TExN15JFHera/+OIL1dfXd7mP8vJyz/d9+/b16Zh+/fp1eB9AKHq5QKptatmOcEinZ9vXDwAAAAAAAAAAAOBvhPk+qK2t1XfffefZPuSQQ3waAXzIIYd4vq+qqvLLVPtJSUme76urq306pqamxvO9y+VSSkpKl/sA7PSM1xT7x6VLmVGMygcAAAAAAAAAAED4IMz3wbp169TQ0ODZ3nfffX06btSoUZbtlStXdrmX/fbbz/P9jz/+6NNo/4ULF3q+HzFihKKjo7vcB2CXX6sMfbvdWju7tz29AAAAAAAAAAAAAIFCmO+DtWvXWrb79+/v03E5OTlyuVye7XXr1nW5l9NOO83zfUlJiR555JGd7v/yyy9r1apVnu2pU6d2uQfATnO8RuVnRUrj0uzpBQAAAAAAAAAAAAgUwnwfbN682bLdu7dvw4BdLpcyMzM925s2bepyL6NHj9bJJ5/s2X700Uc1c+ZMrVmzxrLfpk2bdNttt2nWrFme2uTJkzVu3Lgu9wDYpbHJ0HNeYf4ZvaRIJ1PsAwAAAAAAAAAAILxE2N1AKKisrLRsJycn+3xsUlKStm0z08eqqiq/9DNr1iylp6frySefVENDg+bPn6/58+crMTFRSUlJqqysVHl5uWf/xMREXXTRRYzKR8j7oETa5rWyxFSm2AcAAAAAAAAAAEAYIsz3QXV1tWW7M2vOx8TEdHg/u8vlcunyyy/XhAkTdOONN+qbb76RJFVUVKiiosKy78iRI3Xrrbdq6NChfnlsf1mzZo2cTiaG6IqGhgbPf5cuXWpzN93j/op+klouptnbVS332rXqGT89gO7UE8+xANBdOMcCQGBxngWAwOEcCwCBEw7n2KamJr/fJ2G+D+rq6izbkZGRPh8bFRXl+b62ttZvPb388st66KGHVFBQsNP9li5dqhNPPFEnnniirrvuOiUkJPith65wu91yu912txE2dpzgwllZk0uf1ydaasdGFvWInx2AvTjPAEDgcI4FgMDiPAsAgcM5FgACh3NsC8J8H3iPxG9oaPB5dH59fcuc4K1H6e+upqYmXXfddXrjjTc8tdGjR+v000/XyJEjlZSUpKqqKi1fvlyvvvqq3n77bTU2Nmru3Ln66aef9Oyzzyo1NbXLfXSVy+ViZH4XtT6RdeYCk1D1cU26GtTynIlSk46JrVCkM/x/dgDdr6edYwGgO3GOBYDA4jwLAIHDORYAAicczrFNTU1+H8xMmO+DuLg4y3ZdXZ3PYX7r0fje97M7HnvsMUuQP2PGDJ177rmWfVJSUnTIIYfokEMO0ZgxY3T11VerqalJq1at0g033KCHH364y3101eDBg4NmloBQtXTpUjU0NCgyMlIjR460u52Am7rIsGyfmOXUoXvvY1M3AMJdTzvHAkB34hwLAIHFeRYAAodzLAAETjicYysrK7Vy5Uq/3idDo33gHTqXl5f7fGzrNezj4+O71Edpaakef/xxz/bYsWPbBPnejjnmGJ1xxhme7Y8//jhk15lAz/VTpaElldba2b3s6QUAAAAAAAAAAADoDoT5PsjNzbVsb9261afj3G63ZU37vn37dqmPBQsWWEb6n3766T4d573fxx9/3KU+gO42x+tPLjdaGptmTy8AAAAAAAAAAABAdyDM98GgQYMs2xs3bvTpuC1btljWRfC+n87ynpZhn318m2J8wIABltkF1qxZ06U+gO5U32TohXxr7cxeksvhsKchAAAAAAAAAAAAoBsQ5vtg0KBBioyM9Gz/+OOPPh23ZMkSy/bQoUO71EdNTY1lOzY21udj4+LiPN/X1dV1qQ+gO71TLBU1WGtTmGIfAAAAAAAAAAAAYY4w3wexsbE68MADPdvffPONDMPY5XFff/215/u4uDgdcMABXeojKSnJsl1cXOzTcQ0NDSotLfVsJycnd6kPoDt5T7H/h2RpaByj8gEAAAAAAAAAABDeCPN9NHbsWM/3mzdv1jfffLPT/SsqKvTBBx94tkePHq2oqKgu9dC/f3/L9ldffeXTcYsWLVJDQ8vQZu/7AYLVtjpD75ZYa2czKh8AAAAAAAAAAAA9AGG+j44//njLiPZ//etfamxs7HD/++67zzIt/llnndXhvmPGjNGwYcM0bNgwjRkzpsP9DjnkEMv2E088oaqqqp323dDQoPvvv99S+8Mf/rDTY4Bg8UK+5G41CUacUzo5y75+AAAAAAAAAAAAgO5CmO+jxMREnXvuuZ7tX375Rdddd51lxPsOzz33nF544QXP9ujRo7s8xb4k5ebmWmYIyMvL0wUXXKCCgoJ29y8vL9f06dP1448/emojR470Sy9AoBmGoTnbrLWJWVJiBFPsAwAAAAAAAAAAIPxF2N1AKJk6daq+/PJLLVy4UJL01ltvafHixTruuOOUm5urkpISffzxx1q6dKnnmMzMTN1yyy1+6+G6667T4sWLVVJizj2+aNEijR07VmPHjtXIkSOVlJSkqqoqLV++XB988IFl5H5cXJxmzZrlt16AQPq+QvrFa+KJKUyxDwAAAAAAAAAAgB6CML8TIiMj9eCDD+qCCy7QkiVLJElbtmzRY4891u7+WVlZevTRR9Wrl/8SyL59++rJJ5/UpZdeqi1btkiS6urq9M477+idd97p8Li0tDTde++92nvvvf3WCxBIs7datwfESIel2NIKAAAAAAAAAAAA0O2YZr+TkpOT9cILL+iKK65QZmZmu/vExcVp4sSJeuutt7TPPvv4vYe9995bb775pi6++OIOe9ghJSVFU6dO1VtvvaWDDz7Y770AgVDrNvSS1+oRU3pJTgdT7AMAAAAAAAAAAKBnYGT+bnC5XJo2bZrOO+88LV68WBs2bFBxcbGSkpLUu3dvHXTQQYqLi/P5/hYsWNDpHhISEjR9+nRdeumlWrdunX755ReVlJSourpasbGxSklJ0Z577qmhQ4fK5XJ1+v4BO71RJJU1WmtMsQ8AAAAAAAAAAICehDC/C1wulw488EAdeOCBtvXgcDi0xx57aI899rCtB8Df5myzbo9JkQbEMiofAAAAAAAAAAAAPQfT7AMIKptrDX1YYq1N6W1PLwAAAAAAAAAAAIBdCPMBBJVnt0lGq+1ElzQh07Z2AAAAAAAAAAAAAFsQ5gMIGoZhtJli/+QsKc7FFPsAAAAAAAAAAADoWQjzAQSNr8ulNTXW2lSm2AcAAAAAAAAAAEAPRJgPIGjM9hqVPzRWOjjJnl4AAAAAAAAAAAAAOxHmAwgKVW5DrxRYa1N6Sw4HU+wDAAAAAAAAAACg5yHMBxAUXi2UKt0t205JZ/WyrR0AAAAAAAAAAADAVoT5AILCnK3W7b+kSTnRjMoHAAAAAAAAAABAz0SYD8B262sMfVZmrZ3d25ZWAAAAAAAAAAAAgKAQYXcDAPDMNut2aoR0fLo9vQBAUKn7QSr9p+RMltLukCK40iloGPVS6Syp5mPJaLS7m+DiiJUSTpaSpksOZtkBdslwS+X3SjWfSXFHSkmXSA6uuwcAvzAMafv9UuVcyaixuxsAYWBIbI2MGEMOh0PaHGt3OwDgHxF9pKTLpbixdneCdhDmA7BVk2G0CfNPyZZiXHz4D6CHq1sq/XaoZNSa27VfSbk/mME+7Fc4TaqcbXcXwavua8lokFKutrsTIPiVXGOG+ZJU867kLpbSbrK3JwAIF+V3SSXX2d0FgDAS62q1UW9bGwDgX/VLpJrPpb6rpYhedncDL1zuD8BWn5VJG2qttan8vwJAT9dUIRVMagnyJalxrVR4rjm6CPaqmEOQ74uS68yLUAB0rOqNliB/h7J/StUf2dMPAISTmi+kkr/Z3QUAAEBoMCol97Zd74duR5gPwFZztlq394mXfpdoTy8AEBQMQyo8X2pY1fa2qnnS9oe6vye0qF8mFV1kdxchwi3lT5bchXY3AgSnhnVS4ZR2bjCkgtOlxi3d3hIAhA13gVRwiiS33Z0AAACEhpg/SlEj7O4C7WCafQC22d5o6FWvz/fP7iVzzSkA6KkqHpeqXur49uKrpOjfSzEHdV9PMDVVSvmT2q63mnqTFDHInp6CTe0XUsUTLdvuLVLBmVKvd1kDHGjNqJPyT5aaytu/valQKjhV6r1AcvC2HQA6xXBLBWdI7t+s9aQLpehD7OkJQNjYuGmj3G63XC6X+vXtZ3c7AOAfEX2kmNGSw7XrfdHt+FQAgG1eKZBqmlq2IxzSGUyxD6Anq1ssFV1mrTniJaOqVaFBKjhZylkiuVK7tb0ezTCkogukhhXWetKFUurf7ekpGCWcKjWskWoXtNRqPpDK7pBSr7evLyDYFF8t1f9grTkSJaOiZbv2C6n0Rint9u7tDQBCXdltUo3XciWxf5bSH+QDagBdVta4VA0NDYqMjFS/xJF2twMA6AEYHgPANt5T7B+dLmVFMSofQA/VVG6O+la9tZ71gpR8jbXWuEEqPNsMmNE9Kv4jVf7XWovaX0q7t/39eyqHy3zOuryuziu9Uar5zJaWgKBT+UrbJVMih0p9l0kR/a31sjuk6ne6rzcACHU1C6TSf1hrrj5S1vME+QAAAAhJhPkAbLGy2tDX2621sxmVD6CnMgyp4BypcZ21nnyVFH+ClHaLFHOo9bbqN6VyguRuUfejVDzdWnMkSdmvSM4YW1oKahG9pKwXZX2r0WROGd6Yb1dXQHBoWC0VnmutOWKkrLlSRD8p6xVJkdbbC86SGjd2W4sAELIat0oFp0lqfcGrS8p6SXJl2dUVAAAA0CWE+QBs4T0qPzNSOibdnl4AwHbbH5Cq51tr0Qe3TK3siDQ/hHRmWPcpuVaq/bp7euypmrabMyYYddZ61mwpcg97egoFsX+SUm+21tzbzA/YDbctLQG2a6ppPp9UWOvpD0vRzVO0xhwkpf/L67gSKX+yZHjN3AIAaGE0mq8z3F4XDqbdKsWOtqcnAAAAwA8I8wF0O7dh6Llt1trp2VKkkyn2AfRAtQul4hnWmjNdyn7ZDPF3iMgxpy9X63Ol2wx43EXd0WnPYxjmCNrGNdZ60uVS/Em2tBRSUmZKsUdaa7ULpNJ/2tMPYLfiy6X6n6y1hLOkxKnWWtKlUvwEa63uW6lkZkDbA4CQVnqTVPuZtRZ3jJQ8o93dAQAAgFBBmA+g231UIv3mNbDo7N729AIAtnKXSAUnS2qw1rOekyL6tt0/7i9Syg1e97FZKjhTMpoC1maPtf1hqWqutRZ9kJR+pz39hBqH01yf1pVjrZfdLFV/ZE9PgF0qnpcqnrDWIodLGY9IDq8LWh0OKfMpKcJr9o/ye6Wq1wLbJwCEour3pbJbrDVXXynzGfP1CAAAABDCeEULoNvN8RqVv3+CNDKBUfkAehijSSqc0nYd5JSZUtxRHR+X+g8p5nBrreZ9qewO//fYk9UukoqvtNacqeZ61o4oe3oKRa4Mc5YJuVoVDangdKnxN7u6ArpX/a9S0QXWmiNOyp4rOePbP8aZbN7uiLbWC6dKDesC0ycAhKLGzVLBGV7FCCn7FcnFWn4AAAAIfYT5ALpVSYOh1wutNUblA+iRyu+Rqt+21mL+2HadcW8Ol5T1X8mVba2X3ijVfO7fHnsqd2n7MyZkPitF9relpZAW8wcp7XZrralQKjjFXN8WCGdNVVL+JMmottYzHpeihu/82OhRUvr9XvdXLuWfLBl1/u0TAEKR0SDlnyI1FVvraXdJMf9nT08AAACAnxHmA+hWL+ZL9UbLdpRDOi274/0BICzVftl27WNXlpT1ouSI2PXxEb3MfS0v5ZrMcLQx35+d9jyGYY58bcyz1pOvkeKPtaWlsJB8lRR3nLVW+4V5EQoQzooulhp+sdYSz5MSvUeRdiDxfCn+VGut/gep+Cr/9AcAoazkb1LdV9Za3Hgp+XI7ugEAAAACgjAfQLd6xmuK/RMypLRIptgH0IO4C80RRHK3KjrM0fYRfXy/n9jDpdSbvO57m1R4umS42z8Gu1b+b6n6DWst+g9S2i3t7w/fOJxS5hwpwmtmg7I7pOp3bWkJCLiK2VLlM9Za1L5tR9vvjMMhZT4uRQ6z1rc/LFW+0vUeASBUVb0lld9trUUMlDJnm+dOAAAAIEwQ5gPoNssqDX1fYa0xxT6AHsVoMtf0dG+x1lP/IcUe0fn7S7leij3SWqv5RCr95+732JPVfi2VXGutOTOk7JckR6Q9PYUTV5qU9Yokr3/LgjOlxo22tAQETN1Sqegia82RaP4NOGM7d1/ORCl7ruTwOq7wXKl+Vdf6BIBQ1JAnFU7xKkZJ2a9IrhQbGgIAAAAChzAfQLeZ7TUqv3eU9OdUe3oBAFuU3SbVfGitxY6VUm7YvftzOKWs5yRXjtfj3CxVf7R799lTuYuk/MmSWq/h7pCyXpAicu3qKvzEHCSl/8taayox/+2Nent6AvytqUIqmCQZtdZ65pNS1NDdu8+oEVLGw9aa0fw4TTW7d58AEIqMeqlgstRUaq2n/1uKPsCengAAAIAAIswH0C0amgy94BXmn9lLinAy/R2AHqLmU6n0H9aaq7eU+bzkcO3+/boyzZHjan0fhlRwutT42+7fb09iNEkFZ0nuzdZ6yt+kuL/Y01M4S7pUip9grdV9K5XMtKcfwJ8MQyo8X2rwGjGfdLGUcHLX7jtxqpRwtrVWv1Qqvqxr9wsAoaT4GqnuO2stfrKUdKE9/QAAAAABRpgPoFu8WywVNFhrU5liH0BP0bhNKjhVUlOrolPKekmKyO76/cccKqXdbq01FUoFp0hGY/vHoEXZnVLNe9ZazOFS6ixb2gl7DoeU+ZQUsYe1Xn6vVPW6LS0BflPxuFT1krUW9Tsp/R7/3H/Gw1Lk3l6P+R+p4nn/3D8ABLPKV6Xt91trkUOkzCfM1xcAAABAGCLMB9AtnvEalX9wkjQsjjfbAHoAwy0VnCa586311Fuk2D/673GSr5LijrXWar+QSv/uv8cIRzWfS6Veyxy4sqWs/3ZtxgTsnDPZXNfWEW2tF54tNayzpSWgy+oWS0Veo+Sdyc3r3Ue3f0xnOeOa7y/eWi+6QKpf7p/HAIBg1LBWKjzHWnPESFlzJWeSPT0BAAAA3YAwH0DAFdQbervYWjubUfkAeorSm6TaT6212KOklGv9+zgOp5T5jBTRz1ovu12qfte/jxUuGvM7mDHhRSmil11d9RzR+0vp91lrTeVS/smSUWdLS8BuayqX8idJqrfWM+dIkQP9+1hRe0kZj1trRrX5+E1V/n0sAAgGTbXmOc7Ybq2nPyhF72tPTwAAAEA3IcwHEHAv5EuNRst2rFM6Ocu+fgCg21R/KJXdYq25cqWsZ83w3d9caVLWK5IirfWCM6XGjf5/vFBmuKXC0yX3Vms99SYp9nB7euqJEi+Q4k+11up/kIqvsqcfYHcYhlRwjtToNatE8hVS/PjAPGbi6VLi+dZaw3Kp6CKzHwAIJ8VXSPVLrLWEM6XEv9rTDwAAANCNCPMBBJRhGJrjlZOclCklRzDFPoAw17hFKjhdUutQJULKfllyZQTucWN+L6Xfba01lUj5kyWjvv1jeqKyW6SaT6y12L9IKdfb009P5XBImY9LkcOs9e0PS5Wv2NMT0FnbH5Sq51tr0f8npd0R2MdNv1+K2s9aq3xWqpgd2McFgO5U+aJU8Zi1FrmXlPGo+ToCAAAACHOE+QACakml9LPXbJ9nM3MxgHBnNEoFp0hNRdZ62p1SzCGBf/yk6VLcSdZa3bdSyczAP3YoqP7YXP6gNVeOlPV8YGZMwM45E5vXAI+11gvPlRpW29MT4Kva76Tiq601Z5p54ZYjKrCP7Yxp/ttJtNaLL5bqlgb2sQGgO9SvlAq9ZiFxxEnZ8yRnvD09AQAAAN2MTysBBNRsr1H5/WOkw1Pt6QUAuk3JDVLtl9Za3AnmlMvdweGQsp6WIgZZ6+X3SlVvdE8PwarxN3N6fcuMCS4p+yXJlWlXV4gaIWU8bK0ZFc1rgNfY0xOwK+4SqeBkSQ3WetZzUkS/7ukhcrCU+ZS1ZtRKBZOkporu6QEAAqGp2jyXGZXWesajUtRwe3oCAAAAbECYDyBg6poMvZhvrZ3VS3IyFR6AcFb1tlR+p7UWMUDKnN29U4E6k80Rm/IaGVo4RWpY1+4hYc9olApOldwF1nrabVLMofb0hBaJU6WEs621+p+k4stsaQfYKaPJPJ82brDWU66T4o7u3l4SJklJl1hrDavM0ayG0f4xABDsii+V6n+21hL/KiWeZU8/AAAAgE0I8wEEzJtFUkmjtTaFKfYBhLOGDVKh9weMkVLWK5LLhmlJoveXMu6z1prKpfyTJaOu+/uxW+nfpdr/WWtxx0rJV7e/P7pfxsNS5N7WWsV/pIrn7ekH6Ej5PVL129ZazGgp9Z/29JP+Lyn6AGut6qW260wDQCioeEaqeNpaixoppT9oTz8AAACAjQjzAQTMHK8p9g9LkQbFMiofQJgy6qWCyVJTqbWefq8Uc6A9PUlS4jQp/hRrrf6Htms8h7vq96Sy2621iH5S5jOSg5fEQcMZ17wGuNc6uEUXSPXL7ekJ8Fb7lVQy01pzZkpZL0qOCHt6ckSbF445U6z1osulusV2dAQAu6d+mVR0obXmSJCy5krOWHt6AgAAAGzEJ5cAAuK3OkMflFhrZzMqH0A4K75WqltorcVPkpIutqefHRwOKfMJKXKotb79IanyFXt66m6Nm6SCM7yKO2ZMSLOlJexE1F5SxuPWmlEt5U+Smqrs6QnYwV0o5U+W5G5VdEhZL0gROXZ1ZYocKGXO8SrWN//tlNvREQB0TlOlec4yaqz1zCelqKHtHwMAAACEOcJ8AAHx3DapqdV2gkuamGVbOwAQWFWvSdvvs9YiBpsfPDqCYEYSZ6I5mskRY60Xnis1rLanp+5iNJjBW5PXFWbpd0kxv7enJ+xa4ulS4vnWWsNyqegi1gCHfYwmqeBMyb3FWk+5UYr7sz09eYs/QUq+ylprXCcVnMPfDoDgZhhS0TSpYYW1nnShlDDZnp4AAACAIECYD8DvDMPQnG3W2qQsKd4VBIEWAPhbwzqpcKq15og2pwp3JtnTU3uiR0rpD1trRkXziM2a9o8JByUzpbpvrLW4E6Wky+zpB75Lv1+K2s9aq3xWqphtSzuAym6Xaj6w1mLGSKl/t6efjqTdLkUfbK1Vz5e2P2BPPwDgi4onpcoXrLWo/aW0e+3pBwAAAAgShPkA/O7b7dLKamuNKfYBhKWm2vanL05/QIrez5aWdipxqpRwlrVW/5NUHKbBdtUbUvk91lrEICnz6eCYMQE754wxL4pxJFrrxRdLdUvt6Qk9V82nUqlXaO/qJWX9V3K47OmpI45IKftlyZlurRdfLdUubP8YALBT3Y9S8aXWmjO5+eLYmHYPAQAAAHoKwnwAfjd7q3V7cKx0aLI9vQBAQJVcJdUvttYSTpcSz7Onn11xOKSMR6TI4dZ6xX+kiuft6SlQGtZLhWd7FaPMD4VdKTY0hN0SOVjKfMpaM2qlgklSU4U9PaHnadwmFZwm6yJSTinrRSki266udi6ir5T1nFexUSo4WXKXtHsIANiiabt5caxRZ61nzpYiB9nTEwAAABBECPMB+FW129ArBdbalF6SgxGQAMJN5cvS9kestcg9pYzHgnvUtzNeyp4nOeKs9aILpPpf7enJ34w6M7BqKrPWM+6Tove3oyN0RcIkKekSa61hlVR4PmuAI/AMtxnku73WkEr9pxT7J1ta8lncUVLKTGutcaNUOEUymto/BgC6k2FIhedKjWus9aTLpfgTbWkJAAAACDaE+QD86rVCabu7Zdsh6Sym2AcQbupXmR88tuaIbZ4KNMGenjojai8p43FrzahuXjKgyp6e/Kl4hlT3vbUWP1lKnGZPP+i69H9J0QdYa1UvSRWPt78/4C+lN0u1n1prseOklOvs6aezUm+WYv5orVW/3XYJEgCww/ZHpKq51lr0QVL6nfb0AwAAAAQhwnwAfjXHa9DSn1OlvjFBPEIVADqrqcac4tuotNYzHpWi9rGnp92ReEbb5QAafpGKLgrt0c6Vc6XtD1prkUOlzP8E94wJ2DlHtJT1iuRMsdaLLpPqFrd7CNBl1R9KZf+01lw55vT1jhB5K+2IMJcDcGZa6yUzpdov7ekJACSpdpFUfIW15kw1/3/viLKnJwAAACAIhcgnEABCwYZaQwtKrbUpve3pBQACpni6VL/UWkuYKiVOsaefrki/X4ra11qrfFaqmG1PP13VsFoq/Ku15oiRsuZKzkR7eoL/RA6UMud4FeubZ5Qot6MjhLPGLVLB6ZJaX9zkkrJfllwZdnW1eyL6SFn/lTln1g5uKX+y5C60qysAPZm71FwSSQ3WeuazUmR/W1oCAAAAghVhPgC/eWar9ePO5AhpfIh91gkAO1XxrFTxpLUWuY+U8ZA9/XSVM7Z59JNX0F18sVS3tP1jglVTjZR/smRUWOvpD0nRI+3pCf4Xf4KUfJW11rhOKjgntGeUQHAxGqWCU6SmIms97Q4p5g/29NRVcWOllL9ba+7fpIIzJKPJnp4A9EyGIRVOlRrzrPXka6T4Y21pCQAAAAhmhPkA/KLJMPSM1xT7p2RJsS6mNAYQJuqXS0UXWmuOBCl7nuSMs6cnf4gaKmV6XaBg1JpLCTRVtH9MMCq+XKr/0VpLOEtKPMeObhBIabdL0Qdba9Xzpe0P2NMPwk/JDW2noI87ru2FJKEm9UYp9ghrreZDqew2e/oB0DOV/1uqfsNaizlUSrvFnn4AAACAIEeYD8AvviiT1tdaa1OZYh9AuGiqkvInSka1tZ75hBQ1zJ6e/CnhZCnpYmutYZVUeH5ojHaueEGqeMJaixwuZTwiObioLOw4Is2pzp3p1nrxDKl2oT09IXxUvyOV32mtRfSXMp8J/fOJwyVlviC5elnrpf+Qaj61pycAPUvtN1LJtdaaM0PKesn8/zsAAACANgjzAfjFHK9R+cPjpANZnhhAODAMc0R+w6/WeuI0KeFUe3oKhPR7pKjfWWtVL0kVj9vTj6/qf5WKLrDWHHFS9lzJGW9PTwi8iL5S1nNexQZz/V13iS0tIQw0bJAKzvQqRprLkbhSbWnJ7yKyzdDM8lFAk1RwqtS4raOjAKDr3EXmkkhqbFV0SFkvSBE5dnUFAAAABD3CfABdVtFoaG6BtTalt+QI9dFLACBJFU9LlV6hYdQoKf3f9vQTKI7o5gA82VovukyqW2xPT7vSVCXlT5KMKms943Eparg9PaH7xB0lpcy01ho3SoVTWAMcnWfUSwWTpaZSaz39X1LMQfb0FCixh0mpXtNZu/OlgtMkw21PTwDCm9EkFZwluTdb6yk3SHF/sacnAAAAIEQQ5gPosrmFUnWrz8xdDunMbPv6AQB/iXGukoovsRYdSVL2K5Izxp6mAilyoJQ5x6tYbwbmTeV2dLRzRZdIDb9Ya4nnSoln2NMPul/qzVLMH6216rel8nvs6Qehq+Q6qc5rmYb4CVLSpfb0E2gp10qxR1lrtZ9KpTfZ0w+A8FZ+l1TznrUWc7iU+g97+gEAAABCCGE+gC57Zqt1+6g0qVc0o/IBhDanKtU/5irJqLXekPm0FDnYnqa6Q/x4KflKa61xnVRwjrnkQLComC1VzrHWokZK6Q/Y0g5s4oiQsl6UXFnWeslMqfZLe3pC6Kl6TSr3mm0lYg8p8ykpXGeacjilrGclV661XnaLVP2hPT0BCE81/5NK/matubKlrP9KDpc9PQEAAAAhhDAfQJesqTb0hddgzbN729MLAPiPof7xtyraudFaTpouJUywp6XulHaHFP1/1lr1fGn7g/b0463+Z6noYmvNkSBlzZWcsfb0BPtE9DEDAbUOXd1S/imSu9CurhAqGtZJhVOttY6WHQk3rgwp+2VJEa2KhlRwutS4xa6uAIQTd4FUcIqk1svfOM0L8SJ62dUVAAAAEFIidr0LAHRszjbrdkakdGy6Pb3IcEsNKyUFw1qfDilyqOSIsruR4OEulRyx4Tk1OcJOesQrSov6yFqMPkhKv9uehrqbI9IMeDaPkppKWurFV0sRA6XIAba1JqNRKjhVMmqs9cwnpaih9vQE+8UeYU7VWzqrpebeIhWcYa55jqAS41wtl7NRkc4Iqd7Gke+GIRWe03YZkfT7pehR9vTU3WIOMS/gKrm6pdZUZIZv6Q+H78wEQJgLmvNs8ZWS22sqv9SbpNjD7ekHAAAACEGE+QB2m9sw9KxXmH9athTltOHDgvrl0tYxkju/+x+7I44kqddrUuwYuzuxX/FVUvl95gi3rJekuL/Y3RHQsbof1DvaK7R3pkpZL/esC3Qi+klZz0nbjmlVbJDyj7etpQ4lXSwlTLa7C9gt5QZzav2aj1tqNR9Km0fa1xPaNTSu1cZm29poX8JpUuL5dnfRvZKvlGq/kKrfaKnVfilt2de+ngB0SdCeZ2OPlFKut7sLAAAAIKQwzT6A3fZJqbS5zlo7266Z8oqmBVeQL0nGdnPaViMYZgqwUe2XUvm9kpqkplKp+JLgWncbaM1dJuVPktPRYK1nPmPvaHS7xB0tpVxndxc7F/U7Kf0eu7tAMHC4pMznJRfr/WA3RQ6TMh7veaPRHQ4p8//Zu/P4vMo6b/yfO0nTJE3a0tKWRUQBRXFHwVEGBVQQEARlU7aCzAguM+ozqIyj8zwzzjiM27g9Ov5kKCAuFJSCGz4IBR03BhAUkJFFKltL6Zo2abbz+yPTkJMWaJvkvu+k7/frlZf39c051/mmhAPyOdd1LkyanlXrToDJrHHXwQdFK/5TJAAAbA3/Bg1ss4tG7Jb30vbkpR01+I+fvX8YXE1Uj/qWJF3X1bqL2lrzH+Vx7x+Svvtr0ws8lY3bLY/8/ZxxbjLtqNr0VA92+Mek5cBad7F5DTOSeZcNvt8akqRp3uAOMP5vDlur0prMW5g0tNe6k9po3CGZe1mSKbXuBJiUGpN530oa59S6EQAAmHD8Vy5gm6zqLfLd5eXa/FothFu7YDPFSg2/RvZ34eh/xolqoDNZd9mm9a7FVW8FntaazyXrv1sqret/aTLrn2rTT72oNCXzLh/cFjVTUtv767CvKXsn865Opuwxzn8ATDitr0nmfn1wBWCtf099bfarKJ74qnUvSSVpenYy7ztJ84uyXWvZb/ABqaZnpuZ/TXz58jWqr7q6zzY+I5l7adLy5wEAALZeU60bACamby1LugeeGE+pJG+fW4NGiv5k7UXl2vT3JTt+tgbN/I/Vn08e/+snxuu/O7h1d+PMWnVUO+suT4p1m9a7r0+mn1n9fuDJdP8yefzcUqlvYEaWdP9rnl+xSjGNc5Odf1TrLmDLtb9t8Iu69Nvbb09vb2+mTJmSF7/4xbVuh+GmHTP4BUxo7rMAADB5WJkPbJMFI7bYP3rHZMfmSvUb6bo26X+oXOs4o/p9DNf+9pS2KC26k3Xfqlk7NfVkuxJ0LR7c0hzqQf/jybITk/QNlYqikvvX/WN6i3m16wsAAAAAgO2aMB/YaneuK/LrteXa6TvVppdNwuLmfZOpNV550LhjMu3ocm173Gq/996k+8bNf6//waTv3ur2A5tTDCTLTkv6lpTKy3rPypq+V9eoKQAAAAAAEOYD22DkqvydmpM3zqpBI/0rk/VXlmu1XpW/0cg+Nvw66bmzNr3UysjXH4zUdX11+oCnsvqTSdcPyrWWg7K05+za9AMAAAAAAP9DmA9slb6BIl9fWq6dMi9paqjBFvvrvpUUG4YVmuvn/bithyWNI7YrWLugJq3URDGQdD5NmN+9uCqtwJPq+mmy4iPlWuO8ZO43kjTVpCUAAAAAANhImA9slR+tSB7tKdfO2Lk2vWyydf20o5PG2bXpZaRKU9J+arnWeUlS9G3++Mmm67pNti1Px4iVzl3XJ0VRvZ5guP5lybKTkvQPK1YGg/ymWt3UAAAAAADgCcJ8YKsseLQ8fuX05PnTarAqv+eOZMNN5Vq9bLG/0ch++h9N1v+oNr1UW+eIBy2aX5LMeH+51v9I0vuH6vUEGxX9ybJTk/6Hy/Ud/k/SekhtegIAAAAAgBGE+cAWW95T5Orl5drpO23+2HE3clV+485J66G16eXJND8/mfrKcm1k35NR/6pk3XfKtY75yZTnDP51Gq77+mp1BU9Y9c9J14/LtdY3JDP/tjb9AAAAAADAZgjzgS32jWVJ77Bd0VsakpPm1qCRondwy/rh2k8b3Nq+3oxcnb/+6qR/+eaPnSzWXZYU3cMKTUn7yUmlkrQeXD62a3E1O4PBV0Cs/PtyrXGXZO7Xk0pjbXoCAAAAAIDNEOYDW2zBI+XxsTsmM6fUYIv99T8afN/1cB3zq9/Hlph2YlJpGVboTTq/UbN2qmLk7gNtRyWNcwY/txxU/l739UlRBKqi75Fk2duTDP+da0zmfitprMWTSQAAAAAA8OSE+cAW+c3aIr/pLNfm77z5Y8fdyLB46p8lzc+rTS9Pp3Fm0nZsuTaZt9rvuSvZ8MtybfjuBCNX5vcvTXrvHv++oOhLlr1t8HduuFn/lLQeWJueAAAAAADgKQjzgS1y4aPl8W5Tk0N2qEEj/Y8NblU/3Mit7OvNyP56fpNs+E0tOhl/axeUx43zkrY3PjFu2jNp3LV8TPf1494WZOX/TrpvKNfajkxmnFuTdgAAAAAA4OkI84Gn1TNQ5BsjFrOeulPSWKnBFvudlybpe2JcaU3aT6x+H1uj9ZCkcbdybTKuzi/6ks6Ly7X2U5LKlCfGlcqmq/O7Fo97a2zn1v8oWfVP5Vrjbsmci5KKfxUCAAAAAKA++S/YwNO6ennyeG+5Nn+nGjRSFJuG4NPekjTMqEEzW6HSmHScXq51XpoUPbXpZ7x0/TjpH7GFw+Z2TWg5qDzuXjz41xbGQ9+DybJTRhSbknmXJY2za9ISAAAAAABsCWE+8LQuGpHPHjgj2autBqvye36T9NxerrXPr34f22JkmD/weLL+e7XpZbyMfNBi6n5J8ws2PW7kyvz+ZUnvXePXF9uvojdZetLg32/DzfrXpOXPatMTAAAAAABsIWE+8JQe3VDkhyvKtfk716aXTcLipmcObmE/EUzZK2k5sFybTFvt9z+erLuqXNvcqvwkaXr2pq8d6Lp+fPpi+7biI8mG/yzX2o5JZryvFt0AAAAAAMBWEeYDT+mSpUn/sB3QpzUmx8+pQSPFhsGt6YdrP31ive96ZLi9/odJ36ObP3ai6fxGkmGvDahMTaadtPljK5VNV+d3C/MZY+uuTlZ/slxrenYy58LB30EAAAAAAKhzEygFA6qtKIoseKRcO35O0t5UgyBs3dXJwIgtAjrmV7+P0Zh2fFKZNqzQn3ReUrN2xtTIXQbajkkad3jy40eG+V03JMXAmLfFdqr3j8ljI15tkeZk3sKkcWYNGgIAAAAAgK0nzAee1K/XJHetL9dO36k2vWwSFre8NpmyR2162VYN7YOB/nBrFyRFsdnDJ4wNtyU9t5ZrT7bF/kYtB5XHA8uT3jvGtC22U0VPsuyEZGBluT77s8nUl9emJwAAAAAA2AbCfOBJLRixA/weLclrZtagkb5Hkq4flWsTbVX+RiP77r0z2XBTTVoZM2sXlMeNz0haX//U50x5VtL0rHKta/HY9cT26/FzN/17atqJyfRzatMPAAAAAABsI2E+sFld/UW+taxcO33npFKLd013XpJk2BbslWnJtOOq38dYaHlN0jRiR4GRuw5MJEVP0vn1cq3jtKTS+PTnjlyd33X9mLXFdqrzimTN58u1Kc9J5nw1qcW9CwAAAAAARkGYD2zWlcuT1X1PjCup0Rb7RbFp2D3thMEt6yeiSmXT1fnrvpkMdNWknVFb//3BLfKH29JdE1oPLo+7b0iKgc0fC0+n997ksTPLtUpLMndh0jC9Nj0BAAAAAMAoCPOBzVrwSHn8uh2SZ7bUYGXrhl8lvb8v157ufez1ruP0DD4e8T8GVifrr6xVN6Mz8kGLqQcMroTeEq0HlccDK5Ke345JW2xnBrqTpccnxZpyffYXkqkvqU1PAAAAAAAwSsJ8YBNLuotcu7Jcq8mq/GTTsLhpz6Tlz2vTy1hpembS+rpybeR75yeCvqXJ+h+Ua1vzoEXTMzd95UD34lG3xXbo8fcnPbeWa+2nJh3vqE0/AAAAAAAwBoT5wCYueTQpho2nNybHzqlBIwNdSee3yrWO+ZPj3dft88vjrv+X9P2pJq1ss86vJ+l/YlxpS9pP2Lo5Rq7O77p+tF2xven8ZrL2K+XalOcnO355ctwrAAAAAADYbgnzgZKiKHLRo+XaifOStsYahGLrvzti2+zK/2xRPwlMOzapDH+Pd5Gsvbhm7Wy1oth014RpxyUNHVs3T8vB5XH3jUkxMLre2H70/D557C/KtUpbMu/ypGFabXoCAAAAAIAxIswHSn62Ormnq1w7o1622G99fdK0W216GWsNbUn7SeXa2gWDIflEsOG/kt47yrWt2WJ/o5Er8wdWJj23bXNbbEcG1idLj0+KdeX6jl9JmvepTU8AAAAAADCGhPlAyYWPlMfPa0teOX3zx46rviVJ10/KtW0Ji+vZyJ+n756k+2e16WVrdY540KLpWUnLa7Z+nqZnJE17lWtdi7e1K7Yny9+T9P6uXOt4R9Jxam36AQAAAACAMSbMB4Z09hVZ+Fi5Nn+npFKL906vvSjJsFXqDTOStmOq38d4mvrKZMrzyrXOBTVpZasMdA++p3y4jvlJZRv/kTJydX739ds2D9uPtQs2faCk+cXJ7C/UpB0AAAAAABgPwnxgyBWPJev6nxg3JDmlFlvsFwODYd1w005KGlpr0Mw4qlQGQ/DhOi9LBtZt9vC6sX5RMrCqXGs/fdvnaz24PO6+MSn6N38s9PwuWf6ucq3SnsxdOPnuEQAAAAAAbNeE+cCQBY+Wx2+clewytQar8rt/lvTdV65Nti32N2o/NaVbcdGZrLu8Zu1skbUjVkS3HJJMeda2z9dyUHk8sDrp+c22z8fkNdCZLD0+KbrK9TlfS5qfW5ueAAAAAABgnAjzgSTJfV1FblhVrs3fuSatbBoWT3l+MnX/2vQy3pp2SVrfWK6N/PnrSd+DSdePy7XRPmjRtEsyZUQQ22WrfUYoimT5O5Pe35fr09+VtJ9Ym54AAAAAAGAcCfOBJMmCR8rjWU3JUTvWoJGBzmTdwnKt44zBLeknq5FhePcNSe99mz+21tZenKR4YlzpSKa9ZfTztozcan/x6Odkcln7/yWd3yjXmvdNZn+mNv0AAAAAAMA4E+YDGSiKXDxii/23z0umNtQgQF+3MCmGvzO+MWk/pfp9VNO0o5KGWeXa2otq08tTKYqkc0G51n5i0tA2+rlbDyqPu25Mir7Rz8vksOE3yeN/Va41zEjmLUwqU2vSEgAAAAAAjDdhPpDrVyZLNpRrdbPFftsbk6ZaNVMllalJ+9vLtc6LkmKgNv08mQ0/T3r/UK6Ndov9jVoOKo+LtcmGW8dmbia2gdXJ0uOTYsRNas6FyZQ9atMTAAAAAABUgTAfyIIRq/JfPC15WXsNGum9J+n+abnWPkZhcb0bGYr3PZB019l740c+aDFl72Tqq8Zm7qadkinPK9fq7een+ooieeyspO+ecn36+5Jpx9akJQAAAAAAqBZhPmznVvcVueKxcm3+zkmlFu+oH7m1fMPswS3otwfNL0uaX1yujQzPa2lgXdL57XKtY34ylr8nrQeXx12Lx25uJqY1X0rWXV6uTX1lMvv82vQDAAAAAABVJMyH7dy3lyXdw3Zzb6okJ8+rQSNF/6ZhfvvJSaW5Bs3UQKWy6er8dVcMbjFeD9ZdkRSdwwoNSfupY3uNkVvtd/80KXrH9hpMHN03JY9/oFxr2CGZ++3t574AAAAAAMB2TZgP27kFj5THR81O5jTXYFV+13VJ/5/KtbF6H/tE0X5ykqYnxkV30nlZzdopWbugPG49NGnadWyv0XpQeVx0JhtuGdtrMDH0r0yWHZ9kxMMccy5Opuxek5YAAAAAAKDahPmwHfv9uiK/XFOunb5zbXrZZEv55pckU19ak1ZqpnFO0vamcq0ettrvvX/T99ePx4MWjXOTKfuUayOvy+RXFMlj85O+B8r1GR9Kpr1ps6cAAAAAAMBkJMyH7diCR8vjuVOSw2fVoJH+Vcn675Zr29uq/I1G/twbfpH0/L42vWw08vUHDTskbUePz7VaDy6PuxaPz3WoX6s/k6y/qlxr+fNk1sdr0w8AAAAAANSIMB+2U30DRS4ZEeafslMypaEGW+yv+/bglvJDpvzPlvPbobbDB1eoDzdyi/tqKgaSzhHXb3970tAyPtdrOag87v5ZUvRu9tAJr29psurTgw9LFP217qY+dP88WfHhcq1hx2Tut5JK0+bPAQAAAACASUqYD9upH69MHukp186oly32245KGnesTS+1VpmStJ9arnVenBR9temne/Gm252P564Jra8tj4t1yYb/Gr/r1cpAV/LIQcmKvxncUn7Z2we3l9+e9S9Plp6YZPjveiWZe2nStGutugIAAAAAgJoR5sN2asEj5fF+HckLptVgVX7PXcmGX5Vr2+sW+xt1zC+P+x9Juv5fTVrZZFeAKS9Mmvcdv+s1zhm8xnBd14/f9Wpl3eVJ77DXJ6y7LFnzudr1U2vFQLLs1KT/wXJ95t8lbYfWpicAAAAAAKgxYT5shx7vLXLV8nJtfr2sym+cl7S9sTa91IvmFyZTX1GujfxzqoaBNYOh83AdZySVcX7oo/Xg8rh7Eob5m3t1wuPnJt2/rHordWHV+UnXj8q1loOTHf6+Nv0AAAAAAEAdEObDduibS5OeYTt6T21ITpr75MePm6Iv6bykXGs/1buxk6R9xO4E6xYl/Suq20PnZUnRNazQlHScMv7X3STM/8+k6Nn8sRNR7x+T7us2842+ZNmJSf/j1e6otrpuSFb+XbnWOC+Z+42k0libngAAAAAAoA4I82E7NHKL/WN2THaYUoMt9ruuSfofLde29y32N2p/W1KZOqzQk3R+o7o9jNwNoO3IpLEKT320vCbJsN/HoivZ8Ovxv261dF705N/rW5I8dvrgtvPbg76lybK3JRn+8zYkc7+ZNO1Uq64AAAAAAKAuCPNhO3N7Z5FbOsu1+bXKzEaGxVP3T5r3qU0v9aZxh6TtmHKtmlvt99ydbPh5uVatBy0aZyfNLy7XuhZX59rjrRjYzBb7I3aiWP/9ZPWnqtVR7RT9yWMnJ/0jni7a4f9sujsDAAAAAABsh4T5sJ25cERu9oypyetn1aCR/uXJuqvKtY75NWikjo388+i5Jdlwe3WuPXL1eMOcpO2I6lw7SVoOKo+7rq/etcdT9w1J3x/LtXlXbLrjwYq/Tbp/VrW2amLlPyZdPynXWg9LZv5tbfoBAAAAAIA6I8yH7UjPQJFLl5Zrp+6UNFZqsMV+5zeS9D4xrkxNpp1U/T7qWesbksZdy7XOBeN/3aI/WXtxudZxSlKZMv7X3mjkyuwNP0+KDdW7/ngZuSp/yguStqMG3w8//NUC6U+Wnpj0P1bF5qpo/bXJqn8o1xp3TeZeklT8qwkAAAAAACTCfNiu/ODxZHlvuXZ6zbbYX1Aetx07uLU8T6g0Jh2nlWtrv54UvZs/fqx0/b+k/6FyrVpb7G/U8pqUwu2iO+n+VXV7GGsDa5N1l5drHWcklUrS+rpkh78vf6//4WTZKYMPV0wmfQ8ny96epBhWbEzmfTtpnFOrrgAAAAAAoO4I82E7suDR8viAGclz22qwKn/DbUnPreVatcPiiaJ9fnk88NjgO9XH09oLy+PmlyfNLxrfa47UuEPS/NJyrXtxdXsYa52XJcX6YYXGpP2UJ4Yz/y5pfX35nK4fJ6v+uSrtVUXRlyx72+Dv8XCzPpG0HFCbngAAAAAAoE4J82E7sbSnyPcfL9fm12xV/oiwuPEZgyuT2VTzc5OpI0LOkX9+Y6l/RbLuynKtVg9atB5UHnddX5M2xszIv25tRyZN854YVxqTuZcmjTuXj1v5v5Ou68a9vapY+bGk+8Zyre1NyYz/VZt+AAAAAACgjgnzYTvx9UeT/mG7Wrc1JCfMrUEjRU/SeWm51nHaYJDJ5nXML4/Xfz/pWzo+1+r8VpKeYYXmpP1t43Otp9NycHm84RfJQHdtehmtnv9ONvxnuTbyr2uSNM5N5n4r5X88DwxuS9/3yDg2WAXrf5Cs+kS51vTMZM5FScW/jgAAAAAAwEj+6zlsB4qiyEUjttg/bm7S0VSDLfbXfy8ZWF6ubS7U5AntJySV1mGF/k0fiBgrnSNWj097c9I4a3yu9XRaDkzpH1PFhmTDL2vTy2h1LiiPG3YcXJm/Oa2vSWb9U7nWv3Qw0C/6xqW9cdf3p2TZqSOKU5K5l9Xu9wsAAAAAAOqcMB+2AzevTX63rlw7vV622G/582TKc2rTy0TRMD2Zdly5tvbCpCg2f/y26vldsuG/yrVabbGfJI0zk+aXlWtdi2vRyegU/cnai8u19lOSSvOTnzPjg0nrEeVa9+Jk5f8Z8/bGXdGbLD0xGVhRrs/+ZNLyytr0BAAAAAAAE4AwH7YDF45Ylf+sluS1M2vQSN+jyfoflmvtNQyLJ5KRoXrv75Kem8f2GiMftGjcJWk9dGyvsbVaDyqPu6+vSRuj0nVt0v9QufZ0D0lUGpK5FyeNu5Xrq/4pWX/N2PY33lacN/iKhOHa3pJM/6va9AMAAAAAABOEMB8mue7+It8c8Xr103dKGio12GK/8+tJ+p8YV9qS9uOr38dE1PLapOlZ5drI8H00it7/+eszTMdpSaVx7K6xLVoPLo+7f5kMdNWml2018q9T877J1Bc//XmNs5N5307SNKxYJMtOSfoeHMsOx8+6RcnqT5drTXskc/8jqcU9CAAAAAAAJhBhPkxyi5Ynq0a8ZrsmW+wXxaah5rTjkoaOGjQzAVUakvbTy7XObyYD3WMz//ofJv3LyrX2+WMz92i0HJjyP6p6Nl3lXc/6VybrryzXtubVBS2vSmadX64NLE+WnjT4AEY9670/eWzE72yak3kLk4YZNWkJAAAAAAAmEmE+THIXjdhi/5CZybNaa7AidsNNSe+d5Vot38c+EXWMCEYHVibrrxqbuUc+aDH1VUnz3mMz92g0TE+mvrxc65pAW+13fjMpNgwrNCftb9u6OWa8P2l7c7m24T+TFR8ZdXvjptiQLDshGVhdru/4uWTqvrXpCQAAAAAAJhhhPkxiD20o8uMV5dr8nWvTyyZhcdOzk5bX1KaXiWrKs5OWEdvOj8VW+/3LkvXfK9fq6UGLkT9z9+KatLFNOheUx9OOHtw+f2tUKsmcCzd9zcLqTybrrh5Nd+Pn8b9JNvxXuTbtpKTjnbXpBwAAAAAAJiBhPkxiFz+aDAwbdzQmb5lTg0YGupN13yzXOuYPbh3P1hkZsnf9OOl7aHRzrr00ybB3MVRak/YTRzfnWGo9qDzu/lUysL4mrWyVnjsGd6QYblsfkmjcYXB7+jSX64+dnvT+cdvmHC+dC5M1XyzXpjw3mfPVwQcTAAAAAACALSJJg0mqKIoseKRcO2Fu0tZYgzBt/ZWbbrc98v3vbJlpb0kqHcMKA8nai7d9vqJIOkes7p/21sHt7etFy58naRxW6E26f16rbrbcyF0TGndOWg/d9vmmviKZ/elybWBlsuzEpOjZ9nnHUu8fksfeUa5VWpK5C5OGjs2fAwAAAAAAbJYwHyapn69O/tBVrp1RL1vstxySTNm9Nr1MdA3TkvYTyrXOBYOh/LbouTXp+W251jF/2+YaLw0dg0H2cN3X16aXLVX0Jp2XlGvtpyWVptHNO/3dybTjy7UNv04e/+Do5h0LA13J0uOTYm25PvtLydQX16YnAAAAAACYwIT5MEkteLQ83rsteVUtFlv3/Snp+n/lWj29j30iGvnn1/vfyYZfbNtcIx+0aNp903fU14PWET11La5JG1ts/Q+T/mXl2lg8JFGpJHO+ljTtVa6v+VzSecXo5x+Nx9+X9NxWrrWf7u93AAAAAADYRsJ8mITW9Re5bESOePpOSaUW76tee3GSYavGK9MHt4pn20199eA7yIcbGcpviWJD0vmNcq399KRSh/9oaDmoPN7w62SgsyatbJG1C8rjqX+WND9vbOZumJ7MW5hUppbrj52Z9N47NtfYWmsvTdZ+tVybsk+y45cGH0AAAAAAAAC2Wh0mNsBofeexZG3/E+OGJKfuVINGimLTULP9xKShrQbNTCKVStI+v1zr/HYysG7r5ll3VTKwolyrty32N2o5IMnwLer7ku6f16qbp9b/WLL+6nJtrFenT31pMvsL5VqxZnCb+4Husb3W0+m5K1n+znKt0pbMu3zwtRAAAAAAAMA2EebDJLTgkfL40FnJrlNrsDp2w38mffeUa7bcHhsdp6V0Cy/WJuu+s3VzjFzN33JQMuXZo+1sfDS0J1P3K9e6r69NL0+n89IkfU+MK62DD7GMtY6zkvaTy7WeW5PH3z/213oyA+sGHyAoRjxIsuO/J83Pr14fAAAAAAAwCQnzYZK5v6vI9avKtfk716SVTcPiKXsPbjfO6DXtmrS+oVwbuQvCU+l7OOm6plyr11X5G7UeXB53La5JG0+pKDb9vZ/2lqRhxthfq1JJdvxKMmXE9v1rv5J0fnPsr7c5y9+d9N5RrnX8RdJxSnWuDwAAAAAAk5gwHyaZix8tj3doSo6eXYNGBtYlnZeVax3zvT97LI3c5aD7uqT3j1t2buclSQaeGFfak2nHjVVn46PloPJ4w03JwNqatPKkem5Nem4v10a+EmEsNbQPbmdfaS3XH/vLpOfu8btuMvjQQudF5VrzS5LZnxvf6wIAAAAAwHZCmA+TyEBR5KIRYf7b5iUtjTUI0NddnhSdwwoNSftp1e9jMmt7c9Iws1wbGa5uzuZWj7efUP/vN295dZIpwwr9Sfd/1qqbzRv559r0zKT1kPG9ZvMLkh2/XK4Vncmy45OB9eNzzZ7fJsvfVa5VOpJ5C5OG1s2fAwAAAAAAbBVhPkwiN6xK/thdrs3fqSatbLrle+thSdMuNWll0mpoSdrfXq6tXZAUA5s9fMiGXya9I1Ztj1zlX48apiVT9y/Xuq6vTS+bU2xIOr9RrrWfnlSq8I/ajtOTjjPLtZ7fJo+/d+yvNbA2WXp8Uoy42cz5WjLlOWN/PQAAAAAA2E4J82ESWfBIefzCacnLO2rQSO99Sfficm0ihMUT0cg/174/Jt03PPU5m6we3yuZesCYtjVuWg8uj7vrKMxfd3UysKJc65hfvevP/kIy5YXl2tr/SNZuwW4NW6ooBrfwH/kwyPT3DO7uAAAAAAAAjBlhPkwSa/qKXP5YuTZ/p6RSi3fUjwwPG3ZI2o6qfh/bg+aXJ1NeUK6N3BVhuIH1See3y7WO+Uktfk+2xcgwf8PNycCa2vQy0siHJFpem0zZo3rXb2hL5l2eVNrL9eXnJD2/G5trrP33ZN23yrWpr0hmf2ps5gcAAAAAAIYI82GSWLgs6Rq2u3pTJTmlFlvsFwObvre9/e2DW8Iz9iqVTVfnr7t8cCv0zVn33aQYHn5Xko7Txq29MTf1VUmahxUGku6f1qqbJ/Q9nHT9qFyrxW4UzXsnc75arhVdg9viD3SObu4NtyTL/7pca5iRzL0sqUwd3dwAAAAAAMAmhPkwSSx4tDw+cnYyt7kGq627r0/6HijXbLE/vtpPSdL4xLhYn3RetvljO0esHm99Q9K027i1NuYaWpOWPyvXuhbXpJWSzkuSDHuapjItmfbW2vTS/rak4+xyrff3yfKzB7fJ3xYDqwcfCEhPuT5nQTLl2ds2JwAAAAAA8JSE+TAJ/Pf6Iv+5ulybX4tV+cmmW7w3vyhp3rcmrWw3muYlbUeWayO3fE+S3geSruvKtYn4oEXLQeVx1/U1aWNIUWz6ez/thKShfbOHV8XszybNLyvXOi9N1n5t6+cqiuSxdyR995XrMz6QTDtmm1sEAAAAAACemjAfJoEFj5THc6YkR8yuQSMDq5N1V5Rr7WdMnPexT2QjQ/kN/5n0/He51nlRkmErsxtmJG3HjHdnY6/14PK459akf1VNWkmSbPjV4Mr34Wr9kERDSzJvYVKZXq4//t5kw2+2bq41X9j07+upr0pm/cuoWgQAAAAAAJ6aMB8muP4iuWRpuXbyvGRKQw0C9M7LBt/PPaQp6Ti5+n1sj9qOSBp2LNc6L3riczGwmdXjbxsMfSeaqX824h3tA0n3T2vWzia7IDTtlbT8eW16GW7Knsmc/yjXig2D2+UPrNmyObp/nTz+N+Vaw6xk3reTypSx6RMAAAAAANgsYT5McL/qbc9DG8q1M3auTS+bhJptRyaNc2vTy/am0py0n1Kurb0oKfoHP3f/NOm7v/z9Wq8e31YNLYMrw4frXlyTVjKwPun8VrnWMb9+dqNof2sy/a/Ltb57ksfOGtw+/6n0r0iWnZCkt1yfe0nStNuYtgkAAAAAAGxKmA8T3KINO5TGL+9IXtRegyCx5/fJhl+UaxM1LJ6oRv559z+UdF07+HnkgxZT9kmm7ledvsZD60Hlcdf1NWkj676bFMNXuVeSjtNq08uTmf2vydT9y7V1C5M1X3ryc4oieWx+0vdAuT7zw4O7QAAAAAAAAONOmA8T2JqiMdf3lN+JPX+nGjUzcgv3xrlCv2qb+uKked9ybe2FycDawfB2uI4z6mf1+LZoObg87vlN0r+y+n10LiiPW19ff6vWK83J3G8nDeUHf/L4B5LumzZ/zupPJ+uvLtdaDkx2+Mfx6REAAAAAANiEMB8msGt6d0jPsL+NmyvJ2+bVoJGiP+m8pFxrP8U7tWth5Or89Vcma76aFOuHFRs33ZJ/oml5ZVJpGVYoku4bq9tD35Kk6yflWr3uRjHlWcmci0cUewe30R/5EET3fyYrPlyuNcxJ5n4rqTSNZ5cAAAAAAMAwwnyYwK7umV0av3nHZNaUGqy27vpx0v9wudYxv/p9kLS/LUnzE+NiQ7Lib8vHtB2eNNVqC4cxUpmaTH11uda1uLo9rL0oybD3zjfMSNqOqW4PW2Pam5IZ55ZrfX9MHjtjcFv9JOl/LFl6YpL+YQdVkrnfSJp2qVKjAAAAAABAIsyHCeuevqm5a2BaqTZ/5xo1M/J97M0vT5pfVJtetneNs5NpR48o9pSH9bp6fGu1HlQed19fvWsXA5u+WmLa25KG1ur1sC1m/VMy9YBybf2iZPVnB3+mZacm/Q+Vvz/zY0nb66vXIwAAAAAAkESYDxPWVRvK77/epTk5dFYNGulfkaxbVK5NlrB4onqqP/+GHZO2N1Wvl/HUenB53HNb0v94da7d/dOk775ybSLsRlGZksz71uDvwXArPpQ8dlrSdU253vq6ZIePVq8/AAAAAABgiDAfJqDegSLf2zCzVDt1p6SxUoMt9ju/kfLK7+b/2eqdmmk9NGl8km0a2k9OKs2b/95EM3X/pDJiJXz3jdW59sjdKKY8f7CfiaDpGcncS5IMv1/0JZ2Xlo9r3CmZc2lSaaxmdwAAAAAAwP9oqnUDwNa79vGV+dLsc/LalhvS+D/vtt6hIckfa9DMwJryeNoxSWMttghgSKUpaT8tWX3+pt+bTLsmVJqTlgOSrmufqHVdn0w7dnyvO9CZrLu8XOs4I6nFwzTbqu2NycyPJKs+/iQHNCRzv5U0zatqWwAAAAAAwBOE+TABTV31tzlx2mXlYvE/X7U2EbYa3x50zN80zG9+aTL1JbXoZvy0HFwO87sXj/811y1MinXDCo1J+6njf92xtsP/Trp/tvk/sx3+MWl9bbU7AgAAAAAAhrHNPkxAbcVjtW5h8xp3Gdzindprfl4y9VXl2mRalb9R60Hlcc9vk/5x/vtj5Bb7bYcnTTuN7zXHQ6UxmfuNpHHE6vvWNyYzP1ybngAAAAAAgCHCfJiA2md9MJ0D02rdxghTkjlf9X7terLjl5JK++DnqQck099Z237Gw9T9kkpbudZ14/hdr/eepPun5dpEfkiiaedk3neeCPSn/lky95Kk4l8PAAAAAACg1myzDxPQC3d8ZW566Lqs678rz2juy1577lnjjirJ1JcmDTNq3AclU1+W7L4s6bs/mfL8ifVO9y1VmZK0/HnS9eMnat3XJ+1vHZ/rrV1QHjfMTtreND7XqpaWVye73Z30PZpM2TOp+FcDAAAAAACoB/6LPUxQUystaShemPUDU5LWF9e6HepVQ2vSvE+tuxhfrQeXw/yuxeNznaI/WXtRudZ+clJpHp/rVVPDjKTZwzgAAAAAAFBP7KMLwMTWclB53HtH0r9s7K/TdV3S/2C5NpG32AcAAAAAAOqaMB+AiW3qy5NKe7nWdcPYX2ftheVx80sHXy8BAAAAAAAwDoT5AExslSlJy5+Xa93Xj+01+lcl679brlmVDwAAAAAAjCNhPgATX+vB5XHX4rGdf923kqJ7WGFK0v72sb0GAAAAAADAMMJ8ACa+loPK4967kr5Hx27+tQvK47ajksYdx25+AAAAAACAEYT5AEx8U/dNKh3lWvfisZm7565kw6/KNVvsAwAAAAAA40yYD8DEV2lKWl9Tro3VVvtrLyyPG3dK2t44NnMDAAAAAAA8CWE+AJPDyK32u68f/ZxFX9J5SbnWfurgwwMAAAAAAADjSJgPwOTQenB53PvfSd/Do5tz/Y+S/kfLtY75o5sTAAAAAABgCwjzAZgcml+aNMwo17oXj27OkVvsT90/ad5ndHMCAAAAAABsAWE+AJNDpTFpeU251rV42+frX56sv7pc6zhj2+cDAAAAAADYCsJ8ACaPloPK467rt32uzm8k6X1iXGlJpp207fMBAAAAAABsBWE+AJNH68Hlcd89Sd+D2zbXyC32245NGmdu21wAAAAAAABbSZgPwOTR/OKkYWa5ti1b7W/4TdLzm3LNFvsAAAAAAEAVCfMBmDwqjUnLa8u17sVbP8/IVfmNz0haD9nmtgAAAAAAALaWMB+AyaX1oPK46/qtO7/oSTovLdc6Th98UAAAAAAAAKBKhPkATC4tB5fHffclfUu2/Pz130sGHi/XOuaPui0AAAAAAICtIcwHYHJpflHSMKtc61q85eeP3GK/5cBkyl6jbgsAAAAAAGBrCPMBmFwqDUnLa8u1LQ3z+x5N1v+wXOs4Y0zaAgAAAAAA2BrCfAAmn9aDyuPu67fsvM5LkvQ/Ma60JdOOG6uuAAAAAAAAtpgwH4DJp/Xg8rjvj0nvH5/6nKJI1i4o16YdnzR0jGFjAAAAAAAAW0aYD8DkM+UFScOO5Vr34qc+Z8NNSe+d5Zot9gEAAAAAgBoR5gMw+VQaktbXlmtdT7PV/toLy+OmPZKW14xtXwAAAAAAAFtImA/A5NQyYqv97sWDW+lvzkBXsu6b5VrH/KRSGY/OAAAAAAAAnpYwH4DJqfWg8rhvSdJ3/+aPXX9lMrB6WKGStJ82To0BAAAAAAA8PWE+AJPTlH2ShjnlWtfizR87cov91kOSKbuPS1sAAAAAAABbQpgPwORUqWy6Or/7+k2P6/tT0nVtudZ+xri1BQAAAAAAsCWE+QBMXq0Hl8ddi5OiKNfWXpxkWK0yPZl27Hh3BgAAAAAA8JSE+QBMXi0Hlcf9DyZ99z4xLopk7YLyMe0nJQ1t490ZAAAAAADAUxLmAzB5TXle0jivXOta/MTn7p8lffeUv99hi30AAAAAAKD2hPkATF6Vyqar87uvf+Lz2gvL35uydzL1lePeFgAAAAAAwNMR5gMwubUeXB53LR7cXn+gM1l3Wfl7HWcMPgAAAAAAAABQY021bgAAxtXIlfn9Dye9f0g2/CIp1g37RkPSfmo1OwMAAAAAAHhSVuYDMLlNeW7SuHO51n39plvst74xadqlen0BAAAAAAA8BWE+AJNbpbLpVvtrL0y6byjXOs6oXk8AAAAAAABPQ5gPwOQ3cqv9Db8qjxtmJdOOqlo7AAAAAAAAT0eYD8DkN3Jl/kjtb08qU6vTCwAAAAAAwBYQ5gMw+TXtmTTu+uTft8U+AAAAAABQZ4T5AEx+lcqTr85vfnHS/LLq9gMAAAAAAPA0hPkAbB9aDtp8veOMwbAfAAAAAACgjgjzAdg+bHZlflPSfnLVWwEAAAAAAHg6wnwAtg9Nz04adyvX2t6UNM6pTT8AAAAAAABPQZgPwPahUknaTyrXZrynNr0AAAAAAAA8jaZaNwAAVbPDx5KBtUnPLUnH/KT1dbXuCAAAAAAAYLOE+QBsPxrakzlfrnUXAAAAAAAAT8s2+wAAAAAAAABQZ4T5AAAAAAAAAFBnhPkAAAAAAAAAUGeE+QAAAAAAAABQZ4T5AAAAAAAAAFBnhPkAAAAAAAAAUGeE+QAAAAAAAABQZ4T5AAAAAAAAAFBnhPkAAAAAAAAAUGeE+QAAAAAAAABQZ4T5AAAAAAAAAFBnhPkAAAAAAAAAUGeE+QAAAAAAAABQZ4T5AAAAAAAAAFBnhPkAAAAAAAAAUGeE+QAAAAAAAABQZ4T5AAAAAAAAAFBnhPkAAAAAAAAAUGeE+QAAAAAAAABQZ5pq3cBENzAwkFtuuSVLlizJ8uXLM3369Oy8887Zb7/90tbWVvV+li1blttvvz2PPfZYVq1alZaWluy00055znOekz333DOVSqXqPQEAAAAAAACwdYT526i/vz8XXHBBLrnkkixbtmyT77e1teXII4/MueeemxkzZox7P9dee20WLFiQm2++OQMDA5s9ZubMmTnwwAPzyU9+UqgPAAAAAAAAUMdss78N1qxZk1NOOSWf/vSnNxvkJ8n69euzcOHCHH300bnzzjvHrZfVq1fnPe95T9797nfnpptuetIgP0lWrVqVq6++Ov39/ePWDwAAAAAAAACjZ2X+Vurr68tf//Vf55Zbbhmq7bLLLjn66KOz6667ZsWKFbn22mvz29/+Nkny6KOP5uyzz87ChQszb968Me1l7dq1ecc73jF0rSSZNWtWDjrooOy1116ZOXNmurq68sADD+S2227L7bffnqIoxrQHAAAAAAAAAMaeMH8rXXjhhfn5z38+NH7Tm96UT3ziE2lubh6qnX322bn44ovzz//8zymKIkuXLs1HP/rRfPWrXx2zPoqiyHve856hIL+pqSnvec978o53vKPUy3DLli3LZZddloYGGzIAAAAAAAAA1DOp7lbo7OzM1772taHxPvvsk/PPP3+z4flpp52Wk08+eWh8ww035Oabbx6zXhYuXJhf/vKXSZKGhoZ88pOfzDnnnPOkQX6SzJ07N+95z3uE+QAAAAAAAAB1Tqq7FRYtWpRVq1YNjc8999w0NT355gbve9/70traOjS++OKLx6SPdevW5ZOf/OTQ+LjjjssRRxwxJnMDAAAAAAAAUHvC/K3wk5/8ZOjzrrvumle96lVPeXxHR0cOO+ywofFPf/rT9PT0jLqPH/zgB1mzZk2SpLGxMe9973tHPScAAAAAAAAA9UOYv4W6u7vz61//emj86le/OpVK5WnPe/WrXz30ed26dWOy1f4VV1wx9Hn//ffP3LlzRz0nAAAAAAAAAPVDmL+F7rvvvvT29g6NX/KSl2zReS972ctK47vvvntUfaxfvz6333770Hi//fYb1XwAAAAAAAAA1J8nf+E7Jffee29pvPvuu2/RebvuumsaGxvT39+fZPChgNG44447huZKkr333jtJsmrVqnznO9/Jj370oyxZsiTr1q3LrFmzstdee+U1r3lN3vrWt6a9vX1U1wYAAAAAAACgOoT5W+jBBx8sjXfeeectOq+xsTFz5szJo48+miT505/+NKo+fv/735fGc+fOzY033pjzzjsvy5cvL33v0UcfzaOPPpqf/exn+fKXv5yPfexjOeKII0Z1fQAAAAAAAADGn232t1BnZ2dpPGPGjC0+d/r06UOf161bN6o+Vq5cWRrfdtttOeecc4aC/MbGxsydOzc77LDDJud94AMfyKWXXjqq6wMAAAAAAAAw/qzM30Lr168vjadOnbrF57a0tDzpPFtrzZo1pfH555+fvr6+TJs2LX/1V3+VY489duhBg4cffjgXXXRRLrroohRFkaIo8s///M95wQtekJe+9KWj6mO07rnnnjQ0eJZkNHp7e4f+9/bbb69xNwCTi3sswPhxjwUYX+6zAOPHPRZg/EyGe+zAwMCYzynM30IbNmwojadMmbLF5zY3Nw997u7uHlUfXV1dpXFvb29aWlqyYMGCvPjFLy59b5dddsl5552XPffcMx/96EeTJH19ffnUpz6Vr3/966PqY7T6+/vT399f0x4mk403OADGnnsswPhxjwUYX+6zAOPHPRZg/LjHPkGYv4VGrsTv7e3d4tX5PT09Q5+Hr9Ifiz6S5Oyzz94kyB/uhBNOyLXXXpsbbrghSXLTTTflv//7v/Pc5z53VL2MRmNjo5X5ozT8RrY1D5cA8PTcYwHGj3sswPhynwUYP+6xAONnMtxjBwYGxnwxszB/C7W1tZXGGzZs2OIwf/hq/JHzjLaPxsbGnHTSSU973imnnDIU5ifJL3/5y5qG+XvttVfa29trdv3J4Pbbb09vb2+mTJnylA9zALD13GMBxo97LMD4cp8FGD/usQDjZzLcYzs7O3P33XeP6ZyWRm+hkcHz6tWrt/jctWvXDn2eNm3amPax1157ZYcddnja817+8peXVsLfddddo+oDAAAAAAAAgPEjzN9Cz3jGM0rjRx55ZIvO6+/vz7Jly4bGu+2225j2scsuu2zRedOmTcv06dOHxitXrhxVHwAAAAAAAACMH2H+Ftpjjz1K4yVLlmzReQ899FDp3Qgj59lae+21V2nc3Ny8xecOP3b4eycAAAAAAAAAqC/C/C20xx57ZMqUKUPj3/zmN1t03q233loaj/Y99XvssUcplN+a7f7XrFkz9HnGjBmj6gMAAAAAAACA8SPM30Ktra3Zb7/9hsa/+MUvUhTF057385//fOhzW1tbXvGKV4yqj+bm5rzqVa8aGt99991bdN4DDzyQ7u7uofHI7foBAAAAAAAAqB/C/K3w+te/fujzgw8+mF/84hdPefzatWtzzTXXDI0PPPDArdoW/8m84Q1vGPq8cuXK/PrXv37ac4b3kST777//qPsAAAAAAAAAYHwI87fC0UcfXdqe/lOf+lT6+vqe9Ph/+7d/S1dX19D4tNNOe9JjDznkkOy9997Ze++9c8ghhzxlH0ceeWTmzJkzNP7MZz6TgYGBJz1+xYoV+Y//+I+h8U477STMBwAAAAAAAKhjwvyt0NHRkbPOOmtofMcdd+TDH/5went7Nzn2kksuyaWXXjo0PvDAA0e9xf5GbW1tede73jU0vvXWW/PBD36w9ODARkuXLs1ZZ52VlStXDtXe+c53jskOAQAAAAAAAACMj6ZaNzDRnHHGGfnZz36WX/3qV0mSq6++OrfcckuOOuqoPOMZz8iKFSty7bXX5vbbbx86Z86cOfn4xz8+pn2cdNJJ+cUvfpEf//jHQ338+te/zpFHHplnP/vZ6e3tzZ133pkf/OAHWb9+/dB5r3/96/O2t71tTHsBAAAAAAAAYGwJ87fSlClT8oUvfCHvfOc7c+uttyZJHnrooXzlK1/Z7PFz587Nl7/85ey0005j2kdDQ0M++clPpqenJ4sXL04yuAp/+Hb6Ix1++OH5l3/5l1QqlTHtBQAAAAAAAICxZZv9bTBjxoxceumlef/73196d/1wbW1tOe6443L11VfnhS984bj00dLSkn//93/Pxz/+8TzrWc960uP23HPPfPrTn85nP/vZtLS0jEsvAAAAAAAAAIwdK/O3UWNjY84+++z8xV/8RW655ZY88MADefzxxzN9+vTsvPPO2X///dPW1rbF81133XXb3Mvxxx+f448/PnfccUfuueeeLFu2LI2NjZk1a1Ze+tKXPmXQDwAAAAAAAED9EeaPUmNjY/bbb7/st99+tW4lL3jBC/KCF7yg1m0AAAAAAAAAMEq22QcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOlP1MP/mm2+u9iUBAAAAAAAAYEKpeph/8skn58gjj8yFF16YFStWVPvyAAAAAAAAAFD3arLN/n333Zd//dd/zWtf+9q8733vy89+9rNatAEAAAAAAAAAdamplhfv7e3NNddck2uuuSY777xzjjvuuLz1rW/NvHnzatkWAAAAAAAAANRU1Vfmn3766Zk5c2aKohiqFUWRhx9+OF/4whdyyCGH5C//8i9z7bXXpr+/v9rtAQAAAAAAAEDNVT3MP++883LjjTfmM5/5TA444IBUKpUkGfrf/v7+/PSnP8173/vevPa1r82nP/3pPPDAA9VuEwAAAAAAAABqpuphfpJMmTIlRxxxRC644IJce+21Oeecc7LTTjttslp/+fLl+drXvpY3vvGNOfXUU3P11Venp6enFi0DAAAAAAAAQNXUJMwfbpdddslf//Vf57rrrstXv/rVvOENb0hjY2OSJ1brF0WR//qv/8oHP/jBHHjggfn4xz+e3//+97VsGwAAAAAAAADGTc3D/I0qlUpe85rX5Atf+EJuvPHG/M3f/E2e9axnbbJaf/Xq1bn00ktz7LHH5rjjjstll12WdevW1bBzAAAAAAAAABhbdRPmDzdr1qycddZZ+eEPf5ivf/3rOeaYY9LS0jL0/aIoUhRFfve73+Xv//7v8+d//uf5yEc+kltvvbWGXQMAAAAAAADA2KjLMH+4V7ziFfmXf/mX/PSnP83f//3f5wUveEGS8hb8XV1d+c53vpO3v/3tedOb3pRLL700nZ2dtWwbAAAAAAAAALZZ3Yf5G7W3t+eYY47J2972tuy8884piiKVSmXoKxkM9u+55558/OMfzyGHHJIvfelL2bBhQ407BwAAAAAAAICt01TrBrbE7bffnoULF+YHP/hB1q9fn6S8Mn+4SqWSoiiyZs2afPGLX8xVV12VL3zhC3nuc59b9b4BAAAAAAAAYFvUbZi/evXqXHnllbn88stzzz33JNk0uG9packb3/jGnHjiieno6MgVV1yRRYsWZcWKFUOh/gMPPJD58+fnqquuyo477liLHwUAAAAAAAAAtkrdhfk///nPs3DhwvzkJz9Jb2/vUIC/cSV+kjznOc/JCSeckGOOOSYdHR1D9Q996EP5wAc+kEWLFuWLX/xiHn300STJypUrc8EFF+RDH/pQdX8YAAAAAAAAANgGdRHmL126NJdffnm+853v5OGHH04yuAq/UqkMrbBvbm4eWoW/7777PulcU6ZMyXHHHZdDDz00J598cv7whz+kKIrccMMNwnwAAAAAAAAAJoSahfn9/f35yU9+koULF+bnP/95BgYGNlmFXxRF9tprr6FV+NOnT9/i+adPn55zzjknH/jAB5IkDz300Nj/EAAAAAAAAAAwDqoe5t93331ZuHBhrrrqqqxYsSLJ5lfhH3bYYTnxxBPz8pe/fJuvtffeew997unpGXXvAAAAAAAAAFANVQ/zjzjiiKHQPimvwt9zzz2HVuHPmDFj1NdqaWkZ9RwAAAAAAAAAUG0122Z/+Cr8Qw89NCeeeGJe8YpXjOk1mpqasssuu4zpnAAAAAAAAAAw3moS5hdFkT322CMnnHBCjj322DFZhb858+bNy3XXXTcucwMAAAAAAADAeKl6mP+mN70pJ5100pivwgcAAAAAAACAyaLqYf6nPvWpal8SAAAAAAAAACaUhlo3AAAAAAAAAACUCfMBAAAAAAAAoM5UfZv9Rx99NBdeeOHQ+J3vfGdmzZq1VXM8/vjj+epXvzo0/ou/+IvsuOOOY9YjAAAAAAAAANRS1cP8b37zm7noootSqVTyohe9aKuD/CSZPXt2brnllvzud79LkkyfPj3vfve7x7pVAAAAAAAAAKiJqm+z/6Mf/Wjo84knnrjN85x44okpiiJFUeT73//+WLQGAAAAAAAAAHWhqmH+ww8/nAceeCBJUqlU8oY3vGGb53rDG96QhobB9u+///4sXbp0THoEAAAAAAAAgFqrapj/+9//PslgkP+sZz0r06dP3+a5ZsyYkWc961mbzA0AAAAAAAAAE11Vw/yHHnpo6PPuu+8+6vmGz/Hggw+Oej4AAAAAAAAAqAdVDfPXrVs39Lm9vX3U8w2fY/jcAAAAAAAAADCRVTXMb21tHfq8du3aUc/X2dk59LmpqWnU8wEAAAAAAABAPahqmD9r1qyhz0uWLBn1fMPnGD43AAAAAAAAAExkVQ3zN77jviiK3H///XnooYe2ea6HHnoo995779B41113HXV/AAAAAAAAAFAPqhrmv/CFL0xHR0cqlUqS5Ctf+co2z/Xv//7vQ59bW1vzspe9bNT9AQAAAAAAAEA9qGqY39DQkNe97nUpiiJFUeSKK67ID37wg62e5wc/+EEWLlyYSqWSSqWSgw8+OE1NTePQMQAAAAAAAABUX1XD/CR517velaamplQqlQwMDOSDH/xgvvSlL6Wvr+9pz+3v78+Xv/zlfPCDH0wyuF1/Q0ND3vWud4132wAAAAAAAABQNVVfzv7MZz4zZ511Vr7yla+kUqmkr68vX/ziF/PNb34zxxxzTF7xildkzz33HNqOf82aNbnvvvvyX//1X7nyyiuzfPnyFEUxtCr/zDPPzJ577lntHwMAAAAAAAAAxk1N9qZ/3/vel/vuuy8//vGPU6lUUhRFli9fngsuuCAXXHDBk55XFEWSDJ1z2GGH5X/9r/9VrbYBAAAAAAAAoCqqvs3+Rv/2b/+Wd77znUPjSqWSZDCw39zX8GOS5Oyzz85nP/vZ6jYNAAAAAAAAAFVQszC/oaEh73//+/Ptb387r3vd65I8sfJ+czZurX/ooYdm4cKFed/73peGhpq1DwAAAAAAAADjpibb7A/34he/OF/60peyYsWK/PrXv85tt92W5cuXZ9WqVUmSGTNmZM6cOXnpS1+a/fbbL7NmzaptwwAAAAAAAAAwzmoe5m80a9asvPGNb8wb3/jGWrcCAAAAAAAAADVln3oAAAAAAAAAqDPCfAAAAAAAAACoM8J8AAAAAAAAAKgzwnwAAAAAAAAAqDNNtW5goxUrVuS+++7L6tWr09nZmaIotur8Y445ZnwaAwAAAAAAAIAqq2mY/+ijj+bSSy/ND37wgzz88MOjmkuYDwAAAAAAAMBkUbMw/9vf/nY+8YlPZMOGDVu9Cn+jSqWSoihSqVTGuDsAAAAAAAAAqJ2ahPkXXnhh/vVf/3WzQfzw8ciQf+T3tvUhAAAAAAAAAACoZ1UP8++888586lOfSvLEyvpDDz00hxxySBobG3PuuecOfe/iiy/OunXrsnz58vzmN7/Jtddem9WrV6dSqWTWrFn54Ac/mF122aXaPwIAAAAAAAAAjKuqh/lf+cpX0t/fP3jxpqZ85jOfyaGHHpokeeihh0rH7r///kOfjz/++Hz0ox/N1772tXzlK1/JypUr86//+q+54IIL8vznP796PwAAAAAAAAAAjLOGal6su7s71113XSqVSiqVSs4888yhIH9LtLS05D3veU++8IUvpLGxMStWrMhf/uVfZuXKlePYNQAAAAAAAABUV1XD/N/85jfp6+tLURRpbGzM6aefvk3zHHzwwTnrrLOSJMuXL8+XvvSlsWwTAAAAAAAAAGqqqmH+gw8+mCSpVCrZc889M3v27Kc8vq+v70m/d9ZZZ6WpqSlFUeR73/ve0Nb9AAAAAAAAADDRVTXMX7169dDn3XfffZPvNzU1lcY9PT1POld7e3te8pKXDM178803j1GXAAAAAAAAAFBbVQ3zh6+eb2lp2eT706ZNK40ff/zxp5xv3rx5Q58ffvjhUXYHAAAAAAAAAPWhqmH+8LB+/fr1m/1+Y2Pj0PjpAvrhDwcsX758DDoEAAAAAAAAgNqrapi/6667Dn3e3Kr7SqVS2n7/tttue8r5/vCHPwx9HrlFPwAAAAAAAABMVFUN8/fcc88kSVEUpSB+uH322Wfo89VXX/2kc91888257777hsbDt9wHAAAAAAAAgImsqmH+brvtlrlz5yZJ1q1bl//+7//e5JjDDjts6PM999yTT33qU5scs2TJknzwgx9MpVJJMrii/xWveMU4dQ0AAAAAAAAA1VX1velf/epX58orr0ySXH/99Xnuc59b+v5rX/va7Lrrrnn44YdTFEUuuOCC/OQnP8kBBxyQadOm5Y9//GMWL16cnp6eFEWRSqWS1772tZkzZ061fxQAAAAAAAAAGBdVXZmfJIcffniSwa32L7/88k2+39zcnI9+9KNJBlfcF0WR+++/P5deemm++tWv5sc//nE2bNgwdHx7e3vOO++86jQPAAAAAAAAAFVQ9ZX5BxxwQN71rndlYGAgSbJ06dJN3nd/0EEH5R//8R/zf/7P/0lvb+/QdvobbQz5Z86cmS9+8Yt55jOfWbX+AQAAAAAAAGC8VT3Mb2pqyl/91V897XHHHXdc9ttvv3z1q1/NDTfckOXLlw99b7fddsthhx2WM888M7NmzRrPdgEAAAAAAACg6qoe5m+N3XffPf/0T/+UJOnq6sratWszffr0tLS01LgzAAAAAAAAABg/dR3mD9fa2prW1tZatwEAAAAAAAAA466qYf4f//jH3HjjjUPjI444IjvuuGM1WwAAAAAAAACAulfVMP/GG2/MJz7xiSTJzJkz8/a3v72alwcAAAAAAACACaGhmhfr7u5OURRJkn322SdNTRNml38AAAAAAAAAqJqqhvmzZs0a+rzDDjtU89IAAAAAAAAAMGFUNcyfN2/e0OfVq1dX89IAAAAAAAAAMGFUNcx/+ctfntbW1hRFkd/97ndDW+4DAAAAAAAAAE+oapjf1taW173udUmSVatW5cc//nE1Lw8AAAAAAAAAE0JVw/wkOffcczNz5swkyT/90z/l4YcfrnYLAAAAAAAAAFDXqh7mz5s3L5/5zGcybdq0LFu2LCeddFKuvfbaarcBAAAAAAAAAHWrqdoXvOmmmzJlypR86EMfyic+8YksW7Ys733ve7PbbrvloIMOyvOf//zMmjUrbW1tWzXvfvvtN04dAwAAAAAAAEB1VT3MP/XUU1OpVIbGlUolRVFkyZIlueSSS7ZpzkqlkjvvvHOsWgQAAAAAAACAmqp6mL9RURRDof7wcL8oilq1BAAAAAAAAAB1oSZh/sbAXnAPAAAAAAAAAJuqepj/iU98otqXBAAAAAAAAIAJpeph/rHHHlvtSwIAAAAAAADAhNJQ6wYAAAAAAAAAgDJhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUmaZqX/DKK68cl3mPOeaYcZkXAAAAAAAAAKqt6mH+hz/84VQqlTGfV5gPAAAAAAAAwGRR9TB/o6IoRj1HpVJJURTj8nAAAAAAAAAAANRKQy0uOpogv1KpDIX3Y/FAAAAAAAAAAADUm6qvzL/44ou36viBgYGsXbs299xzT372s5/l5ptvTpLMmDEjH/7wh7PrrruOR5sAAAAAAAAAUDNVD/P333//bTrvDW94Q84555zcfPPN+dCHPpQHH3wwn/zkJ/Mf//Efed7znjfGXQIAAAAAAABA7dRkm/3RePnLX55LL700O++8c1asWJG//Mu/zIoVK2rdFgAAAAAAAACMmQkX5ifJvHnzct555yVJHnvssXz+85+vcUcAAAAAAAAAMHYmZJifDG67P2vWrBRFkauvvjpdXV21bgkAAAAAAAAAxsSEDfMrlUpe+MIXJknWr1+fX//61zXuCAAAAAAAAADGxoQN85Nk+vTpQ58feeSRGnYCAAAAAAAAAGNnQof5q1evHvq8Zs2aGnYCAAAAAAAAAGNnwob5GzZsyK233jo0njlzZu2aAQAAAAAAAIAxNGHD/H/7t39LZ2fn0HjPPfesYTcAAAAAAAAAMHaaat3A1lqyZEn+7//9v1m0aFEqlUqKosgOO+yQl73sZbVuDQAAAAAAAADGRNXD/PPOO2+rz+nv78+aNWty//33Z8mSJUmSoiiSJJVKJeecc04aGibsJgMAAAAAAAAAUFL1MP+73/1uKpXKNp07PMDfuCr/8MMPz6mnnjqWLQIAAAAAAABATU2obfY3BvhFUaSlpSXnnHNOzjrrrFq3BQAAAAAAAABjqiZh/sYV9luqsbEx7e3t2WGHHfK85z0vr3zlK3PkkUdm+vTp49QhAAAAAAAAANRO1cP83//+99W+JAAAAAAAAABMKA21bgAAAAAAAAAAKBPmAwAAAAAAAECdEeYDAAAAAAAAQJ0R5gMAAAAAAABAnWmq9gX7+vpyzz33DI133333tLa2btUc69evz5IlS4bGz33uc9PQ4LkEAAAAAAAAACaHqof53/ve93LeeeclSWbOnJnrr79+q+eoVCqZP39+Vq9enST5zGc+k8MPP3xM+wQAAAAAAACAWqn6cvbvfOc7KYoiSXLCCSekpaVlq+dobW3NiSeemKIoUhRFLr/88rFuEwAAAAAAAABqpqph/rp163LLLbcMjd/0pjdt81zDz73pppvS3d09qt4AAAAAAAAAoF5UNcy/66670tfXlySZNWtWnvOc52zzXM95znMya9asJElvb2/uvPPOMekRAAAAAAAAAGqtqmH+/fffn2Twnfd77733qOcbPsfGuQEAAAAAAABgoqtqmL9q1aqhzzvssMOo59u4Mj9JVq9ePer5AAAAAAAAAKAeVDXMH27jdvuj0d/fP/S5t7d31PMBAAAAAAAAQD2oapg/fDX+Y489Nur5hs8xc+bMUc8HAAAAAAAAAPWgqmH+nDlzkiRFUeSOO+7Ihg0btnmu7u7u/Pa3vx0az549e9T9AQAAAAAAAEA9qGqYv++++6axsTGVSiU9PT1ZtGjRNs911VVXpaenJ0lSqVSy7777jlWbAAAAAAAAAFBTVQ3zOzo68qIXvShFUaQoinz+85/P0qVLt3qepUuX5vOf/3wqlUoqlUr22WefzJo1axw6BgAAAAAAAIDqq2qYnyRnnnlmksHV9MuXL8+ZZ56Z+++/f4vPf+CBB/KOd7wjy5cvT1EUSZIzzjhjXHoFAAAAAAAAgFqoeph/6KGH5qUvfWmKokilUsm9996bt7zlLTn//PNz7733Pul59913X84///wcc8wxuffee4dW5b/whS/MkUceWcWfAAAAAAAAAADGV1MtLvq5z30uxx13XJYvX55KpZKurq4sWLAgCxYsyMyZM7PHHnuko6MjlUola9euzX333ZeVK1cmydBDAEVRZN68efniF79Yix8BAAAAAAAAAMZNTcL8efPmZcGCBXn3u9+dP/7xj6lUKkkGg/qVK1fmlltuKR2/cTv9javxi6LIs5/97Hzxi1/MvHnzqt4/AAAAAAAAAIynqm+zv9Gee+6ZK664Im9/+9vT3NxcCuxHGh72Nzc355RTTskVV1yRPffcs6o9AwAAAAAAAEA11GRl/kbTpk3Lxz72sbz73e/OokWL8qtf/Sq33XZbVq1aVTpuxowZednLXpZXvvKVefOb35xZs2bVpmEAAAAAAAAAqIKahvkbzZ49O2eeeWbOPPPMJElfX19Wr16dZDDIb2qqizYBAAAAAAAAoCrqMiVvamrK7Nmza90GAAAAAAAAANREQ60bAAAAAAAAAADKhPkAAAAAAAAAUGeqvs1+X19f7rnnnqHx7rvvntbW1q2aY/369VmyZMnQ+LnPfW4aGjyXAAAAAAAAAMDkUPUw/3vf+17OO++8JMnMmTNz/fXXb/UclUol8+fPz+rVq5Mkn/nMZ3L44YePaZ8AAAAAAAAAUCtVX87+ne98J0VRJElOOOGEtLS0bPUcra2tOfHEE1MURYqiyOWXXz7WbQIAAAAAAABAzVQ1zF+3bl1uueWWofGb3vSmbZ5r+Lk33XRTuru7R9UbAAAAAAAAANSLqob5d911V/r6+pIks2bNynOe85xtnus5z3lOZs2alSTp7e3NnXfeOSY9AgAAAAAAAECtVTXMv//++5MMvvN+7733HvV8w+fYODcAAAAAAAAATHRVDfNXrVo19HmHHXYY9XwbV+YnyerVq0c9HwAAAAAAAADUg6qG+cNt3G5/NPr7+4c+9/b2jno+AAAAAAAAAKgHVQ3zh6/Gf+yxx0Y93/A5Zs6cOer5AAAAAAAAAKAeVDXMnzNnTpKkKIrccccd2bBhwzbP1d3dnd/+9rdD49mzZ4+6PwAAAAAAAACoB1UN8/fdd980NjamUqmkp6cnixYt2ua5rrrqqvT09CRJKpVK9t1337FqEwAAAAAAAABqqqphfkdHR170ohelKIoURZHPf/7zWbp06VbPs3Tp0nz+859PpVJJpVLJPvvsk1mzZo1DxwAAAAAAAABQfVUN85PkzDPPTDK4mn758uU588wzc//992/x+Q888EDe8Y53ZPny5SmKIklyxhlnjEuvAAAAAAAAAFALVQ/zDz300Lz0pS9NURSpVCq5995785a3vCXnn39+7r333ic977777sv555+fY445Jvfee+/QqvwXvvCFOfLII6v4EwAAAAAAAADA+GqqxUU/97nP5bjjjsvy5ctTqVTS1dWVBQsWZMGCBZk5c2b22GOPdHR0pFKpZO3atbnvvvuycuXKJBl6CKAoisybNy9f/OIXa/EjAAAAAAAAAMC4qUmYP2/evCxYsCDvfve788c//jGVSiXJYFC/cuXK3HLLLaXjN26nv3E1flEUefazn50vfvGLmTdvXtX7BwAAAAAAAIDxVPVt9jfac889c8UVV+Ttb397mpubS4H9SMPD/ubm5pxyyim54oorsueee1a1ZwAAAAAAAACohpqszN9o2rRp+djHPpZ3v/vdWbRoUX71q1/ltttuy6pVq0rHzZgxIy972cvyyle+Mm9+85sza9as2jQMAAAAAAAAAFVQ0zB/o9mzZ+fMM8/MmWeemSTp6+vL6tWrkwwG+U1NddEmAAAAAAAAAFRFzbbZfypNTU2ZPXt2Zs+e/ZRB/tKlS/PVr341RxxxRBW7AwAAAAAAAIDxNeGWvHd3d+fHP/5xFi1alF/+8pcZGBiodUsAAAAAAAAAMKYmTJh/00035bvf/W6uueaarF+/PklSFEWSpFKp1LI1AAAAAAAAABhTdR3mL1myJFdeeWWuuuqqPPTQQ0nKAX6lUhkaAwAAAAAAAMBkUXdhfmdnZ374wx/mu9/9bm699dYkmw/wi6LInDlzcthhh+WII46oZcsAAAAAAAAAMKbqIswviiI//elPc+WVV+a6667Lhg0bhupJSgH+jjvumEMPPTSHH354XvGKV9hiHwAAAAAAAIBJp6Zh/h/+8Id897vfzdVXX53ly5cnefJt9I899ti8+c1vzv7775+Ghoaa9QwAAAAAAAAA463qYf6KFSvyve99L1deeWXuuuuuJE++jf7wVffvfe97s8suu1S7XQAAAAAAAACouqqE+X19fbn++uvz3e9+NzfeeGP6+/ufNMDffffdc9RRR+Xoo4/OoYceWo32AAAAAAAAAKCujGuYf/vtt+fKK6/M97///axZsyZJeRX+xgB/hx12yBFHHJGjjz46L3nJS8azJQAAAAAAAACoe2Me5i9dujSLFi3KlVdemfvvvz9JOcDfqLm5OYccckiOPvroHHjggWlqqvqO/wAAAAAAAABQl8Y8QT/44IOHVtxvtHEVfpLsv//+efOb35zDDjss7e3tY315AAAAAAAAAJjwxjzMHxgYSKVSGVqFXxRF9tprrxx99NE56qijstNOO431JQEAAAAAAABgUhm3ve2LokilUslrX/vanHvuudlrr73G61IAAAAAAAAAMKk0jNfEG1fm33jjjTnqqKNy7LHHZsGCBXnsscfG65IAAAAAAAAAMCmMeZj/Z3/2Z6lUKimKYqhWFEXuuuuunH/++TnooINy5pln5sorr8z69evH+vIAAAAAAAAAMOGNeZi/YMGCXHfddXnf+96X3XfffSjU37hSv7+/P7/4xS9y3nnn5YADDsgHPvCBLF68OP39/WPdCgAAAAAAAABMSOOyzf5OO+2Us88+Oz/60Y/y7W9/OyeeeGKmT5++yWr9rq6u/PCHP8w555yTAw88MB//+Mdz2223jUdLAAAAAAAAADBhNI33BV7ykpfkJS95ST7ykY/kJz/5SRYtWpSf/exn6evrG1qtXxRFVqxYkUsvvTSXXnppnvnMZ+aoo44a79YAAAAAAAAAoC6Ne5i/UXNzcw4//PAcfvjhefzxx3PVVVflyiuvzN13350kpWD/gQceyJe+9KVUKpWh1fy24QcAAAAAAABgezEu2+w/ndmzZ+eMM87IokWLcuWVV+a0007LrFmzhoL7jcH+xs9FUeTNb35zPvCBD+Taa69NT09PLdoGAAAAAAAAgKqoSZg/3POe97z87d/+bW688cb83//7f3PooYemqakpRVGUwv3169fnhz/8Yd773vfmVa96Vf7mb/4m1113XXp7e2v8EwAAAAAAAADA2KraNvtPp7GxMYccckgOOeSQrF69Ot/73vdy5ZVX5re//W2S8jb869aty/e///18//vfT3t7e173utflX/7lX2rZPgAAAAAAAACMmZqvzN+cGTNm5OSTT87ChQvz/e9/P2eddVbmzp27yTb8RVFk7dq1WbRoUS3bBQAAAAAAAIAxVZdh/nB77rln/uZv/iaLFy/OBRdckCOPPDJTp05NURRDoT4AAAAAAAAATCZ1s83+06lUKjnggANywAEHpLOzMz/84Q+zaNGi3HzzzbVuDQAAAAAAAADG1IQJ84drb2/P8ccfn+OPPz5/+tOfbLMPAAAAAAAAwKRS99vsP53ddtst73nPe2rdBgAAAAAAAACMmQkf5gMAAAAAAADAZCPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOiPMBwAAAAAAAIA6I8wHAAAAAAAAgDojzAcAAAAAAACAOtNU6wYmuoGBgdxyyy1ZsmRJli9fnunTp2fnnXfOfvvtl7a2tlq3BwAAAAAAAMAEJMzfRv39/bngggtyySWXZNmyZZt8v62tLUceeWTOPffczJgxo+r9ffazn81XvvKVUu0Tn/hE3vKWt1S9FwAAAAAAAAC2jm32t8GaNWtyyimn5NOf/vRmg/wkWb9+fRYuXJijjz46d955Z1X7+8Mf/pALLrigqtcEAAAAAAAAYOxYmb+V+vr68td//de55ZZbhmq77LJLjj766Oy6665ZsWJFrr322vz2t79Nkjz66KM5++yzs3DhwsybN2/c+yuKIh/96EfT29s77tcCAAAAAAAAYHxYmb+VLrzwwvz85z8fGr/pTW/KNddck/e///054YQTcvbZZ+fyyy/PRz7ykVQqlSTJ0qVL89GPfrQq/X3rW9/KrbfemiTZY489qnJNAAAAAAAAAMaWMH8rdHZ25mtf+9rQeJ999sn555+f5ubmTY497bTTcvLJJw+Nb7jhhtx8883j2t+yZcvy6U9/Okkyc+bMvO997xvX6wEAAAAAAAAwPoT5W2HRokVZtWrV0Pjcc89NU9OTv6ngfe97X1pbW4fGF1988Xi2l49//ONZu3btUG8zZ84c1+sBAAAAAAAAMD6E+VvhJz/5ydDnXXfdNa961aue8viOjo4cdthhQ+Of/vSn6enpGZferr/++lxzzTVJkn333Tdvfetbx+U6AAAAAAAAAIw/Yf4W6u7uzq9//euh8atf/epUKpWnPe/Vr3710Od169aNy1b769evzz/8wz8kSZqamvK///f/3qLeAAAAAAAAAKhPwvwtdN9996W3t3do/JKXvGSLznvZy15WGt99991j2leSfO5zn8vDDz+cJDnttNOy9957j/k1AAAAAAAAAKgeYf4Wuvfee0vj3XfffYvO23XXXdPY2Dg0vu+++8a0r9/97ne55JJLkiQ777xz3vve947p/AAAAAAAAABUnzB/Cz344IOl8c4777xF5zU2NmbOnDlD4z/96U9j1lN/f38+9rGPpb+/P0nyd3/3d2lraxuz+QEAAAAAAACoDWH+Furs7CyNZ8yYscXnTp8+fejzunXrxqyniy++OHfccUeS5OCDD87rX//6MZsbAAAAAAAAgNppqnUDE8X69etL46lTp27xuS0tLU86z7Z66KGH8vnPf35o/r/7u78bk3mr5Z577klDg2dJRqO3t3fof2+//fYadwMwubjHAowf91iA8eU+CzB+3GMBxs9kuMcODAyM+ZzC/C20YcOG0njKlClbfG5zc/PQ5+7u7jHp5x/+4R+GHgx417velWc84xljMm+19Pf3D70egNHbeIMDYOy5xwKMH/dYgPHlPgswftxjAcaPe+wThPlbaORK/N7e3i1end/T0zP0efgq/W31gx/8IIsXL06S7LXXXjnzzDNHPWe1NTY2Wpk/SsNvZFvzcAkAT889FmD8uMcCjC/3WYDx4x4LMH4mwz12YGBgzBczC/O3UFtbW2m8YcOGLQ7zh6/GHznP1lqzZk3++Z//eWj893//9xPyF3qvvfZKe3t7rduY0G6//fb09vZmypQpefGLX1zrdgAmFfdYgPHjHgswvtxnAcaPeyzA+JkM99jOzs7cfffdYzqnpdFbaGTwvHr16i0+d+3atUOfp02bNqo+PvWpT+Wxxx5LkhxzzDHZf//9RzUfAAAAAAAAAPVHmL+FRr6T/pFHHtmi8/r7+7Ns2bKh8W677bbNPdx111257LLLkiQzZszIBz/4wW2eCwAAAAAAAID6ZZv9LbTHHnuUxkuWLNmiVfEPPfRQ6d0II+fZGg899FCKokgy+N6Ik0466SmPH769fzK4qv/LX/7y0PjrX/965s2bt839AAAAAAAAADA+hPlbaI899siUKVPS29ubJPnNb36T44477mnPu/XWW0vj5z73uWPSz/r167NkyZKtOufxxx/P448/PjTe+LMAAAAAAAAAUF9ss7+FWltbs99++w2Nf/GLXwytkn8qP//5z4c+t7W15RWveMW49AcAAAAAAADA5GFl/lZ4/etfPxTOP/jgg/nFL36RV7/61U96/Nq1a3PNNdcMjQ888MA0NzeP6vp33333Fh//q1/9KqeddtrQ+BOf+ETe8pa3bPP1AQAAAAAAAKgOK/O3wtFHH50ZM2YMjT/1qU+lr6/vSY//t3/7t3R1dQ2NhwfrIx1yyCHZe++9s/fee+eQQw4Zm4YBAAAAAAAAmJCE+Vuho6MjZ5111tD4jjvuyIc//OHNvnv+kksuyaWXXjo0PvDAA22xDwAAAAAAAMAWsc3+VjrjjDPys5/9LL/61a+SJFdffXVuueWWHHXUUXnGM56RFStW5Nprr83tt98+dM6cOXPy8Y9/vFYtAwAAAAAAADDBCPO30pQpU/KFL3wh73znO3PrrbcmSR566KF85Stf2ezxc+fOzZe//OXstNNO1WwTAAAAAAAAgAnMNvvbYMaMGbn00kvz/ve/P3PmzNnsMW1tbTnuuONy9dVX54UvfGGVOwQAAAAAAABgIrMyfxs1Njbm7LPPzl/8xV/klltuyQMPPJDHH38806dPz84775z9998/bW1tWzzfddddN+Y9vvKVr8zdd9895vMCAAAAAAAAML6E+aPU2NiY/fbbL/vtt1+tWwEAAAAAAABgkrDNPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAADUGWE+AAAAAAAAANQZYT4AAAAAAAAA1BlhPgAAAAAAAPz/7N13mFXVuT/wdxhmgKHXAUYEsYC9AcauaDRqwILExFgSKyZYcg2Wm2g0xRa9MUquxh6I0Vx7IxYsqFFBBcQKYqN3gaFMn98f/DiZA1POwBnmAJ/P8/C41zlrr/2efYYF8t17bYAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADCPMBwAAAAAAAIAMI8wHAAAAAAAAgAwjzAcAAAAAAACADNO0sQvY3FVUVMTEiRNjxowZsWjRomjTpk1069Yt+vfvH3l5eQ1+/KKiopg2bVp88cUXsWTJkigtLY02bdpEQUFB7L333tGmTZsGrwEAAAAAAACA9BLmb6Dy8vK49957Y/To0bFgwYL13s/Ly4vjjjsuRowYEW3btk3rsefOnRtjxoyJcePGxcSJE6O0tLTafllZWXHwwQfHeeedF/37909rDQAAAAAAAAA0HGH+Bli+fHmcf/75MXHixBr7rFq1Kh555JF444034o477ohddtklLcd+880345xzzonKyso6+1ZWVsbrr78eb7zxRpxxxhlxxRVXRJMmnqwAAAAAAAAAkOmE+fVUVlYWF198cVKQ37179xg8eHAUFBTEkiVLYuzYsfHhhx9GRMS8efNi2LBh8cgjj0R+fv5GH7+oqCgpyM/JyYnddtst9t133+jatWu0aNEi5s+fH//+97/j/fffj4g1of7f/va3KCoqit/+9rcbXQMAAAAAAAAADUuYX0/3339/vPXWW4n297///bj++usjNzc38dqwYcNi1KhRcd1110VlZWXMnz8/rrrqqrjrrrvSVkevXr3i1FNPjeOPPz7atWu33vs///nP4/XXX49f/vKXsWzZsoiI+Oc//xlHHnlkHHLIIWmrAwAAAAAAAID0s+Z6PaxYsSLuueeeRHuXXXaJG2+8MSnIX+uMM86IH//4x4n2uHHjEnfKb4wOHTrE73//+xgzZkyceeaZ1Qb5ax1yyCFx++23R1ZWVuK1dF5QAAAAAAAAAEDDEObXw1NPPRVLly5NtEeMGBFNm9a8uMEll1wSLVq0SLRHjRq10TXss88+MXTo0MjOzk6p/3777RcHH3xwoj1x4sQoLCzc6DoAAAAAAAAAaDjC/Hp4+eWXE9sFBQWx//7719q/devWcfTRRyfab7zxRpSUlDRYfTXZb7/9Etvl5eUxZ86cTV4DAAAAAAAAAKkT5qeoqKgoJkyYkGgfcMABScvX1+SAAw5IbK9cuTItS+3XV8uWLZPaq1ev3uQ1AAAAAAAAAJA6YX6KvvzyyygtLU2099xzz5T223vvvZPaU6dOTWtdqZg1a1ZSu2PHjpu8BgAAAAAAAABSJ8xP0RdffJHU7tmzZ0r7FRQUJD3f/ssvv0xrXakYO3ZsYrtz586xzTbbbPIaAAAAAAAAAEidMD9F697d3q1bt5T2y87Ojs6dOyfaM2fOTGtddXn11Vfj66+/TrSPPvrolB4PAAAAAAAAAEDjEeanaMWKFUnttm3bprxvmzZtEtsrV65MW011WbFiRfzud79LtJs1axbnnXfeJjs+AAAAAAAAABumaWMXsLlYtWpVUrtZs2Yp79u8efMax2kolZWV8d///d8xe/bsxGvDhw+P/Pz8TXL8ukyfPj2aNHEtycYoLS1N/HfKlCmNXA3AlsUcC9BwzLEADcs8C9BwzLEADWdLmGMrKirSPqYwP0XFxcVJ7ZycnJT3zc3NTWwXFRWlrabajBw5Ml544YVEe8CAAXHOOedskmOnory8PMrLyxu7jC3G2gkOgPQzxwI0HHMsQMMyzwI0HHMsQMMxx/6HMD9F696JX1pamvLd+SUlJYntqnfpN5R//vOfMXLkyER72223jT/96U8ZdSd8dnZ2RtWzOao6kdXn4hIA6maOBWg45liAhmWeBWg45liAhrMlzLEVFRVpv5lZmJ+ivLy8pHZxcXHKYX7Vu/HXHSfdxowZE9dcc02i3blz57jvvvuiU6dODXrc+tphhx2iVatWjV3GZm3KlClRWloaOTk5scceezR2OQBbFHMsQMMxxwI0LPMsQMMxxwI0nC1hjl2xYkVMnTo1rWO6NTpF6wbPy5YtS3nfwsLCxHbLli3TVtO6xo0bF5dddlnieQzt2rWL+++/P3r06NFgxwQAAAAAAAAg/YT5Kdpmm22S2nPnzk1pv/Ly8liwYEGi3VDB+jvvvBMXXnhhYgmKVq1axT333BM77rhjgxwPAAAAAAAAgIYjzE9R7969k9ozZsxIab/Zs2cnPRth3XHSYdKkSXHBBRdEcXFxRES0aNEi/vrXv8buu++e9mMBAAAAAAAA0PCE+Snq3bt35OTkJNqTJ09Oab9JkyYltXfaaad0lhWffPJJnHfeebFq1aqIiMjJyYmRI0dGv3790nocAAAAAAAAADYdYX6KWrRoEf3790+033777aisrKxzv7feeiuxnZeXl9aQ/Ysvvoizzz47li9fHhERTZs2jVtvvTUOOuigtB0DAAAAAAAAgE1PmF8PRx55ZGJ71qxZ8fbbb9fav7CwMF544YVE++CDD47c3Ny01DJz5sz46U9/GkuWLImIiCZNmsT111+fVCMAAAAAAAAAmydhfj0MHjw42rZtm2jffPPNUVZWVmP/W2+9NVavXp1on3HGGTX2HThwYPTp0yf69OkTAwcOrLWO+fPnx09/+tOYP39+4rVrr702Bg8enMrHAAAAAAAAACDDCfProXXr1nHOOeck2h9//HFcccUVUVpaul7f0aNHx4MPPphoH3zwwWlZYn/p0qVx9tlnx8yZMxOvXXnllfGDH/xgo8cGAAAAAAAAIDM0bewCNjc//elP480334zx48dHRMQzzzwTEydOjEGDBsU222wTS5YsibFjx8aUKVMS+3Tu3Dl+//vfp+X4Dz74YHz++eeJdnZ2djz44INJFw7U5fTTT691lQAAAAAAAAAAGpcwv55ycnLi9ttvj/PPPz8mTZoUERGzZ8+OO++8s9r+Xbp0iTvuuCO6du2aluNXVFQktcvLy2PGjBn1GmPZsmVpqQUAAAAAAACAhmGZ/Q3Qtm3bePDBB+MXv/hFdO7cudo+eXl5cfLJJ8czzzwTu+222yauEAAAAAAAAIDNmTvzN1B2dnYMGzYszj333Jg4cWJ88803sXjx4mjTpk1069YtBgwYEHl5eSmP98orr6TU78ILL4wLL7xwQ8sGAAAAAAAAYDMgzN9I2dnZ0b9//+jfv39jlwIAAAAAAADAFsIy+wAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGE+QAAAAAAAACQYYT5AAAAAAAAAJBhhPkAAAAAAAAAkGGaNnYBsLHKysqisLAwCgsLo6ysLMrLyxu7pE2irKws8d/PP/+8kasB2LKYY+uWnZ0dTZs2jdatW0fr1q2jaVN/rQQAAAAASCf/6spmq6KiIubOnRvLly9v7FIaRXZ2dmJ7begEQHqYY+tWVlYWxcXFsXLlypg3b160adMmunXrFk2aWPgJAAAAACAdhPlslioqKmLWrFmxcuXKpNezsrKSApgtWVZWVmJ7a/nMAJuKObZu5eXlUVlZmWgvX748ysvLY5ttthHoAwAAAACkgTCfzdLcuXMTQX6TJk2iffv20aZNm2jWrFlSALMlW7VqVVRWVkZWVlbk5eU1djkAWxRzbN0qKyujuLg4li9fHt9++21UVFTEypUrY+7cuVFQUNDY5QEAAAAAbPbcNsVmp6ysLLG0fpMmTaJHjx7RpUuXaN68+VYT5ANAY8vKyormzZtHly5dokePHom78ZcvX+7RBAAAAAAAaSDMZ7NTWFiY2G7fvr07JgGgkeXl5UX79u0T7ap/VgMAAAAAsGGE+Wx2qgYEbdq0acRKAIC1qv6ZLMwHAAAAANh4wnw2O2uX7s3KyopmzZo1cjUAQEREs2bNEo+7scw+AAAAAMDGE+az2SkvL4+IiOzs7ERoAAA0rqysrMjOzo6I//xZDQAAAADAhhPmAwAAAAAAAECGEeYDAAAAAAAAQIYR5gMAAAAAAABAhhHmAwAAAAAAAECGEeYDAAAAAAAAQIYR5gMAAAAAAABAhhHmAzSw22+/Pfr06RN9+vSJ008/vbHLAQAAAAAAYDMgzAcAAAAAAACADNO0sQsAWNf48eNjwoQJERFRUFAQJ510UiNXBAAAAAAAAJuWMB/IOBMmTIiRI0dGRMSAAQOE+QAAAAAAAGx1hPkADezCCy+MCy+8sLHLAAAAAAAAYDPSpLELAAAAAAAAAACSCfMBAAAAAAAAIMNYZh/YKlRUVMSkSZNixowZsXDhwmjevHkcfPDBsd1221Xbf9GiRTFt2rT45ptvorCwMLKysqJdu3bRu3fv2GOPPSInJ2eT1l9UVBTjx4+PWbNmxcqVK6N9+/ax1157xY477tjgxy4rK4vPP/88vvjii1i0aFGsXr06WrduHR07dox99tkn8vPzN/oYS5YsiYkTJ8bChQtj2bJlkZubG126dIk+ffrEDjvsEFlZWfUab8WKFfH+++/H/Pnz49tvv43s7Ozo1KlT7LjjjtG3b9/Izs7e6JrTrbCwMCZMmBALFiyI5cuXR4cOHeKEE06o9metsrIyvvjii5g+fXrMmzcvVq9eHXl5edGxY8fYY489Ytttt93oejbHcwgAAAAAAFsSYT6QMfr06bPeaxMmTKj29YiI4cOHJz2Lfvz48XHGGWck2lOnTo3Kysr429/+Fvfff3/Mmzcvaf8rr7wyKcyfNm1aPPXUU/Hqq6/GF198UWOdeXl58YMf/CDOP//86NChQ52f6/bbb4+RI0dGRMSAAQNi9OjRKfcrKSmJ22+/PR5++OFYvnz5evvstttucc0118Tuu+9eZx31UVRUFC+++GKMGTMmJkyYECtXrqyx72677RbDhw+Pww8/vN7HGTduXNxxxx0xefLkqKysrLZPp06d4phjjolzzjknunbtWut4kyZNipEjR8Y777wTZWVl1fZp06ZNHHnkkXHOOefE9ttvn/TerFmz4ogjjki0X3755dhmm23q/BxXXHFFPPHEExERceKJJ8YNN9yQcr9FixbF9ddfHy+++GKUlJQk9T/66KMTYX5ZWVm89tpr8dxzz8Vbb70VS5curbGe7bbbLoYNGxbHH398vS+E2NBzWFRUFAcddFAUFhZGxPq/P+vy5JNPxuWXXx4REVlZWTF27NiUzj0AAAAAAGypLLMPbLFKS0vj/PPPj+uvv369IL86V1xxRdxzzz21BvkREatWrYoHHngghgwZEtOmTUtXuetZtmxZnHbaaXHXXXdVG+RHRHz00Udx+umnx7vvvpvWY7/99tsxYsSIePXVV2sN8tfWMGzYsLjhhhtqDOTXtXr16vj5z38e5513XkyaNKnW/RYtWhSjR4+Ot956q8Y+5eXlcc0118QPf/jDePPNN2sMoSMili9fHo8//niMGTMmpVob0scffxzHH398PPvss+sF+ev68ssv4+c//3mMGTOm1iA/IuKrr76Kyy+/PC699NI6x11rY89h8+bN47jjjku0n3jiiZR/HiIiHn/88cT2d77zHUE+AAAAAABbPXfmAxlj7dLgy5Yti2XLlkVERLNmzWpcxr1t27a1jnfjjTfGuHHjImLN3eOHHXZYdO3aNVauXBmffPJJNG/evNr9srKyYpdddom99tortt1222jdunUUFRXFV199Fa+88krMnj07IiLmzJkTw4YNi6effjpatWq1QZ+5JhUVFfFf//Vf8cEHH0R2dnYccsgh0a9fv2jXrl0sWbIkXn755Zg8eXJErAnGR4wYEc8991y0bNkyrXVERLRr1y723Xff2GWXXaJjx46Rk5MTixcvjkmTJsXrr78e5eXlERFx//33R/fu3ZNWR6hOcXFxnHnmmfHBBx8kXsvJyYn9998/+vXrFx07dozi4uKYM2dOTJw4MSZPnhwVFRU1jldZWRkXXXRRjB07NvFakyZNol+/frHffvtFfn5+lJWVxfz58+ODDz6Id999N0pLSzfyrGy8ZcuWxYUXXhiLFi2KZs2axeGHHx577713tGzZMhYtWhSvvvpqjXfV5+Xlxb777hu77bZbdO7cOZo3bx5Lly6NKVOmxKuvvhrFxcUREfHcc89F586d48orr6y1lnSdw6FDh8bDDz8cERGzZ8+Od955J/bff/86z8WsWbNiwoQJifaQIUPq3AcAAAAAALZ0wnwgY7z00ksRkbzc/J577lnjsvR1GT16dOTm5sb1118f3//+9+vs37Jlyxg2bFgMHTq0xruCr7zyyrjvvvvilltuicrKypg9e3bccccdMWLEiA2qsSYTJ06MioqK6NGjR4wcOTL69u2b9P55550Xd9xxR9x6660RETF37tx47LHH6gzS62PvvfeOc889Nw455JBqn9seseYO8IsvvjimTp0aERG33HJLDBo0KNq3b1/juNddd11SkD9gwID4wx/+UONz3ufNmxd/+9vfokWLFtW+f/fddyeF0DvttFPceOONscsuu1Tbf8mSJfF///d/DXLhQ3288sorERGx8847x+233x49evRIev+CCy5Yb58dd9wxzjvvvPjud79b4/lYsGBBXHrppYlw/G9/+1ucfPLJseOOO9ZYS7rO4W677RY777xzfPrppxGx5m77VML8xx9/PHEXf5s2beKoo46qcx8AAAAAANjSWWYf2KL97ne/SynIj4i455574he/+EWty3tnZ2fHueeemxS0PvrooykvZZ6qioqKaN26dfztb39bL8hf64ILLoh+/fol2s8991zajn/AAQfEww8/HEcccUSNQX7Emmez33fffdGhQ4eIWPPc9LXPhK/OJ598krhzO2JNkH/PPffUGORHRHTt2jUuv/zyOOaYY9Z7b+HChXH77bcn2ttvv338/e9/rzGEjojo0KFDDBs2LE4//fQa+2wqHTt2jPvuu2+9IL86vXr1iqeffjoGDx5cY5AfEdGlS5f461//Gr17946INXfdVz3n60r3ORw6dGhi+6WXXooVK1bU+rkqKyvjySefTLSPO+64aNasWa37AAAAAADA1kCYz1alvLIyFpZsIb9K4z+/0jx2eT2ec53Jdt999zjhhBNS7l+fAPG8886LvLy8iIhYunRpfPTRR/UtL6VjFBQU1NqnanD6ySef1Pqc8/qoz7no1KlT/PjHP06033zzzRr73n///UnHuP766zcquH3wwQeTLqS47rrr6nz8Qib5+c9/nrgQoi65ubnRpElqf2zn5eXF+eefn2jX9p2k+xwOGjQo8QiL1atXx5gxY2rt/8477yQeXRFhiX0AAAAAAFjLMvtsNR5ZUBkXTotY0PiPyk6Tmu/M3VhdciJu36kyhnap/nndm4vjjz++wcZu0aJF7LXXXvHWW29FRMTHH38c++yzT1qPceKJJ9bZZ6+99kpsl5SUxOzZs6Nnz55prSMV+++/f+Lu7o8//rjaPuXl5UlLuX/ve9+rdRWEVLzwwguJ7X79+iWdj0yXnZ2d8qoRG6Lq8vbffPNNrFixIlq1arVev3Sfw7XL5D/99NMRsWYJ/R/84Ac19n/00UcT23369Indd999o44PAAAAAABbCnfms9U4b+qWFOQ3rAWla87X5q6hg92OHTsmtufPn5/WsQsKCqJz58519uvSpUtSe/ny5WmtI1WdOnVKbC9dujSKi4vX6/Ppp5/GqlWrEu0jjzxyo465ZMmS+Oqrr9I23qbWu3fvBl1FoOrPZ2VlZbU/ow11DquuGDFp0qT48ssvq+1XWFiYdIHHSSedlJbjAwAAAADAlsCd+cAWq7bnsNdm0aJF8dxzz8V7770X06ZNi2+//TZWrlxZ6xL2hYWFG1pmtaqG47VZu9T/WqtXr05rHRUVFTF+/PgYO3ZsfPLJJzFz5sxYsWJFnccpLCxcb/n8L774Iqm96667blRtX375ZVRWeSTExo63qfXo0WOD950yZUr861//io8//ji+/vrrKCwsjNWrVyedj3VV9+z6hjqHAwYMiF69esXXX38dEWvuzv/lL3+5Xr/nnnsuioqKIiIiJycnBg8enJbjAwAAAADAlkCYz1bjrj6xhS2z33DWLLPf2FVsvJYtW9arf0lJSYwcOTLuu+++KC2t3w9K1WeOp8OGPke+tjC3vqZMmRJXXXVVfPbZZ/Xet7o785cuXZrUTmXlgdqsO16qF0Bkivr+fEZEfPXVV3H11VfHhAkT6r1vKt9JOs/hkCFD4pZbbomIiKeeeip+8YtfRHZ2dlKfxx57LLE9cODA6NChQ9qODwAAAAAAmzthPluNoV2y4qTOlbFkCwnzV/3/u3CzsrIir0WLtI7dISciOysrrWM2hqZNU5/iysvL46KLLopXX311vfeys7OjXbt20axZs6QxFy9eHCtXroyI9IbomWD8+PFx3nnnJe6arqply5bRsmXLaNasWWT9/5+T8vLymD17dqJPdedj7bmKWPPd5ObmblSNVcdbW9fmpD4/nxER06dPj9NOOy2+/fbb9d5r0aJFtGrVKpo1axZNmvznCTozZsxIbNf1nUSk9xyedNJJ8ec//znKyspiwYIF8eabb8ahhx6aeH/69OkxZcqURHvIkCFpOzYAAAAAAGwJhPlsVbKzsqLzxuWHGWNVWURlZURWVkRe7uYfvDe2hx9+OCnI79u3b5x22mmx3377RUFBwXp3FEdEXH755fHkk09uwio3jaKiorjiiiuSlj//4Q9/GN/97ndj1113jVatWq23z8yZM+t83nrVoLisrCxKSko2KtBfN3heN5jeklRWVsaVV16ZCPKzsrLi+OOP031hbAAAW5BJREFUj+9///ux2267Rfv27avdp2/fvrWO25DnsFOnTnHYYYfF2LFjI2LNXfhVw/yqd+Xn5+fHQQcdlLZjAwAAAADAlkCYDxARo0aNSmwfcMAB8de//rXOoHn58uUNXVajGDt2bMyZMyciIpo0aRJ333137L///rXuU1hYWOe47dq1S2ovXLgwCgoKNrjOdcdbtGhR9O7de4PHi4jESgP1Vd0KBuk0efLkpLvY//CHP9R5J3sqP58NcQ6rGjp0aCLMf+WVV+Lbb7+N9u3bR1lZWTz99NOJfieccEK1F8wAAAAAAMDWrEndXQC2bPPnz4+vv/460b7kkktSumN81qxZDVhV43nnnXcS2wceeGCdQX5Eaudihx12SGp//PHH9S+uiu233z4pfN/Y8SLWLFdfVaoh/eLFizf62LWp+p307t07pSXpU/lOGuIcVnXwwQdH165dIyKitLQ0nn322YiIGDduXCxatCjR76STTkrrcQEAAAAAYEsgzAcyTtVniVdUVDT48ebPn5/Urmtp8oiIJUuWxPTp0xuqpEa1YMGCxHYq5yIiYvz48XX26du3b9Ky7mvv2N5Q7du3j+233z5t40XEeo8QqHoualJWVhYfffTRRh+7Ng31nTTEOawqOzs7TjzxxET78ccfT/pvRES/fv2iV69eaT0uAAAAAABsCYT5QMbJy8tLbK9YsWKTH7+4uLjOPv/4xz82yYUGjaGysjKxncq5KCwsjKeeeqrOftnZ2XHUUUcl2s8//3zMnj17w4r8/773ve8ltt9777344IMPNmq83NzcpKX/UxnvxRdfjFWrVm3UcetS3++krKws/vnPf6Y0drrP4bqGDBmSuPv/k08+iX//+98xbty4pPcBAAAAAID1CfOBjFM1TP3mm2+ipKSkQY+3dhnwtV577bVa+0+dOjXuuuuuBqyocXXr1i2x/cYbb9R50cK1114bhYWFKY39k5/8JLFdXFwcV1xxxUZ9v6eeemo0a9Ys0b7yyitj2bJlGzxeRMSee+6Z2H7qqaeirKysxr6FhYVx8803b9TxUlH1O3nvvfdi5cqVtfa//fbbkx4dUZuGOIdV9ejRI77zne8k2pdddlmUlpZGRETLli2TLiYAAAAAAAD+Q5gPZJzdd989cSfv6tWr489//nNKdyNvqC5dusSOO+6YaN94443x+eefV9v37bffjp/85CdRXFwcTZpsmVPoAQcckNj+6quv4vrrr4/y8vL1+q1YsSKuvPLKeOaZZ1I+F3379o3TTjst0Z4wYUKcffbZMXPmzBr3WbBgQdx8883xr3/9a733OnbsGJdcckmi/cUXX8Rpp50Wn376aY3jLVu2LO66664YPXp0te8fd9xxie2vvvoqbrjhhmovaJg1a1aceeaZMXv27KTnzjeEqt/JsmXL4sorr6z290RJSUn8z//8T9x5550pfycNcQ7XNXTo0MT2okWLEtvHHHNM0kocAAAAAADAfzStuwvAppWfnx8HHnhgvPnmmxERcc8998To0aOjoKAgcnNzE/1++MMfxo9+9KO0HPOcc86Jyy+/PCLWhI0nnXRSHHXUUbH33ntHixYtYsGCBfHvf/873n333YiI2GmnnaJ3797x/PPPp+X4meTII4+MXr16Je7sHjVqVLz11ltx9NFHR0FBQRQVFcXUqVPjxRdfjG+//TYiIoYPHx633XZbSuNfdtll8dFHH8XkyZMjYk2gf8wxx8SBBx4Y++67b3To0CFKSkpi7ty5MXny5HjvvfeioqIirr/++mrH++lPfxqTJk2KF198MSIipk2bFieddFL0798/9ttvv+jSpUuUl5fH/Pnz48MPP4x33nknSktLY/jw4dWOd/jhh8cuu+wSn3zySUREjB49OsaPHx/HHHNM5OfnR2FhYXzwwQcxduzYKCkpiZ122im22267eOGFF1I9xfW2++67x3e+85145513IiLihRdeiA8//DCOPfbY6NWrV5SVlcWXX34ZL730UsydOzci6vedpPscruu73/1utGvXLpYuXZr0uiX2AQAAAACgZsJ8ICNdc801ccYZZ8ScOXMiYs2S7F9++WVSn6p3+G6sE044ISZMmBCPPfZYRKy5w/nZZ5+NZ599dr2+PXr0iJEjR8Ydd9yRtuNnkqZNm8af//znOP3002P58uURETF9+vSYPn36en2zsrLiggsuiOOPPz7l4LhZs2bxwAMPxC9+8Yt49dVXIyKitLQ0XnvttTofcVCdrKysuPXWW+Oaa66J//u//4uIiIqKihg/fnyMHz++3uNlZ2fHjTfeGGeccUbiYoVp06bFtGnT1uvbs2fP+N///d/4y1/+Uu/j1NdNN90Up5xySiKsnzNnTtxzzz3V9j3xxBPjZz/7WcrfSbrP4bpyc3Nj8ODBMWrUqMRrvXv3jn322WejxwYAAAAAgC3VlrlGNLDZ69GjRzz11FNx+eWXx/777x+dO3dOeq53Q/jDH/4QV155ZbRr167a9/Py8uKUU06JJ598Mnr27NmgtTS2vn37xqOPPhoHHnhgrX3++te/xsUXX1zv8Vu0aBF33nlnjBw5Mnbdddda++bn58dZZ50VBx10UI19srOz43e/+12MHj06+vfvX+sS8+3atYtTTjklBg0aVGOfnXbaKR566KEaP3+zZs1i6NCh8fjjj0ePHj1qrT9d8vPz47HHHotjjjmmxs/Xs2fPuOGGG+KGG26o99L/6T6H6zrhhBOS2ieddFK96gMAAAAAgK1NVmVlZWVjF8GWb8WKFTF16tREu0+fPtGqVasNGuvzzz+PsrKyaNq0adJzzrc2q1atisrKysjKyvLM6TQrLi6O999/P6ZPnx6rVq2K9u3bR9euXWPAgAHRokWLxi5vk5s5c2a8//77sWDBgsjJyYnOnTtH3759Y4cddkjbMebNmxeTJk2KRYsWRWFhYeTl5UWXLl2iT58+sf3229d7vCVLliRqXrZsWTRv3jw6deoUO+64Y/Tp0yfl58lHrPn87733XixcuDCaNWsW3bt3jwEDBkTbtm3rXVe6zJ8/P959992YN29eRER07tw5tt9++9htt93Sdox0nsOIiCeffDLxKIumTZvGa6+9Fp07d05bvelmjt0w/owGUjFlypQoLS2NnJyc2GOPPRq7HIAtjnkWoOGYYwEazpYwx6YzD13LMvsA62jWrFkccMABccABBzR2KRmhR48eDX73edeuXeOYY45J23gdOnSI7373u2kZa1N8/vrKz8+P73//+w16jHSew4hIPMIiIuKQQw7J6CAfAAAAAAAygWX2AYAG9dVXX8W7776baP/gBz9oxGoAAAAAAGDzIMwHABrUX//611j7VJ/u3bvHIYcc0sgVAQAAAABA5rPMPgDQICoqKuIf//hHPPnkk4nXzjnnnMjOzm68ogAAAAAAYDMhzAcA0ubll1+O2267LSoqKmLOnDmxYsWKxHvbb799DB06tBGrAwAAAACAzYcwHwBIm2XLlsVnn3223utt2rSJ//mf/4nc3NxGqAoAAAAAADY/wnwAoEE0bdo08vPz46CDDophw4ZF9+7dG7skAAAAAADYbAjzAYC0Oemkk+Kkk05q7DIAAAAAAGCz16SxCwAAAAAAAAAAkgnzAQAAAAAAACDDCPMBAAAAAAAAIMMI8wEAAAAAAAAgwwjzAQAAAAAAACDDCPMBAAAAAAAAIMMI8wEAAAAAAAAgwwjzAQAAAAAAACDDCPMBAAAAAAAAIMMI8wEAAAAAAAAgwwjzAQAAAAAAACDDCPMBAAAAAAAAIMMI8wEAAAAAAAAgwwjzAQAAAAAAACDDCPMBAAAAAAAAIMMI8wEAAAAAAAAgwwjzAQAAAAAAACDDCPMBAAAAAAAAIMMI8wE20uOPPx59+vSJPn36xMCBA2vsN378+ES/Pn36pL2OqmOPHz8+7eM3pM25dgAAAAAAgIYgzAcAAAAAAACADNO0sQsAYMvw6aefxtixYyMionXr1vGTn/ykcQsCAAAAAADYjAnzAUiLTz/9NEaOHBkREQUFBcJ8AAAAAACAjSDMB9hE9ttvv5g6dWpjl5GRnBcAAAAAAIBkTRq7AAAAAAAAAAAgmTAfAAAAAAAAADKMZfaBrdKyZcti6tSp8fXXX8fSpUsjIqJdu3bRo0eP2HvvvaN58+aNW+A6Pvvss/j4449j8eLF0a5du9hmm22if//+kZOTs1Hjbm7nYV0VFRUxefLk+Oqrr2Lx4sXRrFmz6NSpU+y9997RvXv3tByjsLAwxo8fH3Pnzo2ioqLo1KlT9OvXL3r06JGW8WtTUlISn332WXz55ZexZMmSKC4ujjZt2kR+fn7ss88+0aFDh40+xrx582Ly5MmxePHiWL58ebRo0SK6desWffv2jZ49e9Z7vCVLlsTEiRNj4cKFsWzZssjNzY0uXbpEnz59YocddoisrKyNrjndFi1aFBMnTowFCxbEypUro3v37nHEEUdU27esrCw+//zz+OKLL2LRokWxevXqaN26dXTs2DH22WefyM/P3+h6NsdzCAAAAABA+gnzgYxx1llnxb///e+IiOjfv3/8/e9/T3nfhQsXxqGHHhrl5eUREfHb3/42TjnllKQ+M2fOjKeffjrGjh0bn332WVRUVFQ7Vk5OTgwaNCiGDx8eBQUFG/hp1jd+/Pg444wzEu1UnhM/adKkuPbaa+PTTz9d772OHTvGT37ykzj33HPrFe6l+zwMHDgwZs+enfTa7Nmzo0+fPtX2P/HEE+OGG25Ieq1q31GjRsV+++1X62coKiqKe+65J/7+97/Ht99+W22f3XbbLS699NI44IADah0rIuKKK66IJ554Iqm+FStWxE033RRPPfVUFBUVrbfPgQceGFdffXX06tWrzvHrY/ny5TFmzJh4/vnnY+LEiVFcXFxtv6ysrNhvv/3ioosuin333bdex6ioqIhnn3027r777pg2bVqN/QoKCmLQoEFx1llnRdu2bWsdc9y4cXHHHXfE5MmTo7Kysto+nTp1imOOOSbOOeec6Nq1a9J7G/L7IyLi9NNPjwkTJkRExPDhw+PCCy9Mud8333wTf/jDH+LNN99MzB0REa1bt04K84uKiuLFF1+MMWPGxIQJE2LlypU11rPbbrvF8OHD4/DDD0+p/qo29BzOnTs3Bg4cmPi9fP3118dJJ52U8nH/8pe/xG233RYRES1btow333wz8vLy6l0/AAAAAADpZZl9IGMMGjQosf3ee+/FnDlzUt73ueeeS4RxOTk58b3vfW+9Pn/84x/jtttui08++aTGADsiorS0NB5//PE48cQTE+FfY3jkkUfi1FNPrTbIj4hYvHhx3HLLLXHBBRdEWVlZyuNubudhXXPmzInjjz8+br/99hqD/IiIjz76KH7605/G73//+xqD0ZrMmjUrhgwZEv/85z+rDfIjIv7973/Hj370o/jiiy/qNXZdnn766fjNb34Tb7/9do1BfkREZWVlvPPOO3HaaafFAw88kPL4S5YsiVNPPTVGjBhRa5AfseaijDvvvDM+++yzGvusXr06fv7zn8d5550XkyZNqvVcL1q0KEaPHh1vvfVWyvU2lNdffz1OPPHEGDduXFKQX5233347RowYEa+++mqtQX7Emp+7YcOGxQ033JDyz93GnsNu3brFgQcemGg//vjjKR03Ys3P0doLWSIijjnmGEE+AAAAAECGcGc+kDG++93vxjXXXBNFRUVRWVkZzz77bJx33nkp7fvMM88ktg899NA67yLeYYcdYq+99ortt98+2rRpE6WlpTFz5swYN25cTJ8+PSLWLEH/s5/9LJ5++um0LdmeqnHjxsXVV1+dFLYPGDAgDj744Gjfvn3Mnz8/XnjhhZg2bVq8+uqrcfvtt2/QcdJxHgoKCiI7OztWrlwZixcvjoiIpk2b1njOOnbsuEG1RqwJok877bSklQC6desWxxxzTGy33XaxevXqmDx5cowdOzZKSkoiImL06NGRlZUVv/rVr1I6xurVq+NnP/tZfP3119GsWbMYOHBg7LXXXtGqVauYP39+PP/884kQfMmSJXHZZZfFI488Ek2apP/6uC5dusS+++4bffv2jfbt20eTJk1i/vz5MWHChBg/fnxErLnL/vrrr48ePXrUuDT8WkuWLIlTTjklZsyYkXgtLy8vDj744Nh9992jffv2sXr16pgxY0a8//778fHHH9c6XnFxcZx55pnxwQcfJF7LycmJ/fffP/r16xcdO3aM4uLimDNnTkycODEmT55c6wUkm8rMmTNj1KhRsXLlymjVqlUcddRR0bdv38jLy4t58+YlVgipTrt27WLfffeNXXbZJTp27Bg5OTmxePHimDRpUrz++uuJCwPuv//+6N69e9JqA9VJ1zkcOnRovPHGGxGx5mKoGTNmxLbbblvnuXj33Xdj5syZifaQIUPq3AcAAAAAgE1DmA9kjFatWsXAgQNjzJgxEbEmoE8lzP/qq6/io48+SrQHDx5cbb+cnJw49dRT49RTT40dd9yx2j6XXXZZPPHEE3H11VdHSUlJFBYWxk033RS33npr/T/QBlq5cmVSkJ+bmxt//OMf11tt4Oc//3ncfffdccstt8Rdd92V8vjpPg+jR4+OiDV3A1955ZUREZGfnx8vvfRSyjWl6ne/+11SkH/KKafEr371q2jWrFnitTPPPDOmTZsWP/vZzxIh5ahRo+Kwww5Lunu5Ji+++GJUVFTEbrvtFn/+859jm222SXp/2LBhce2118Y///nPiFhzJ/arr75aZ5CeqqysrDjkkEPi7LPPjgEDBtR4kcAHH3wQl1xySWIFi2uvvTYOPfTQaNq0+j/aKysr4/LLL08K8o8++ui46qqronPnztXu89VXX8W9995b45jXXXddUgg9YMCA+MMf/lBjiDxv3rz429/+Fi1atKj2/U3lqaeeiog1j0r44x//uN4FJhdeeGGsWrUq6bW99947zj333DjkkEMiJyen2nG/+uqruPjiixOPCLjlllti0KBB0b59+xprSdc5HDhwYHTs2DEWL14clZWV8fjjj8cll1xS43HXeuyxxxLbvXv3jn322afOfQAAAAAA2DQssw9klKpB/LRp01J6bnbVu/Jbt25d47Oqr7vuuvjNb35TY4C91oknnhi/+c1vEu2xY8fGwoUL66wjXR588MGYN29eon311VdX+9iArKysOO+88+LMM8+s193Om8t5WNfHH3+cuNAjYs1KDtdee21SkL/WTjvtFPfcc0/ScuE33XRTSsepqKiIgoKCeOCBB9YL8iMisrOz49e//nVS2Prcc8/V56PU6uSTT4677747vvOd79R6t/+ee+4Z99xzTyJYnj9/frz88ss19h87dmy8/vrrifb3v//9uPXWW2sM8iMitttuu/j9738f++6773rvffLJJ/Hwww8n2gMGDIh77rmn1rvBu3btGpdffnkcc8wxNfbZVHbccce44447Ulop4oADDoiHH344jjjiiBqD/Ig15+u+++6LDh06REREUVFR0hL260rnOczJyYnjjz8+0X7yySfrnBdWrFgRL7zwQqJ90kkn1dofAAAAAIBNS5jP1qWyPKJ84Zbxq6LKr3SPXVn786Mb0tpl5NeqGtTX5Nlnn01sH3300ZGbm1ttv+pC35oMGTIkEaiVlpbGO++8k/K+G6vqnbK77rprnHzyybX2v+iii2q983ddm8t5WFfV0DM3Nzd+9atfRVZWVo39e/XqFeecc06i/dlnn8WkSZNSOtYvf/nLaN26dY3v5+bmxgknnJBoT5kyJaVxU1Gf72f77bePQYMGJdpvvvlmjX3vv//+xHanTp3immuu2ahHA1Qdr1mzZnH99dfXq/bGNmLEiJTrrc/n6tSpU/z4xz9OtFP9TtJxDocOHZrYnjt3brz99tu19v/Xv/4Vq1evjog1j8ao+jMNAAAAAEDjs8w+W48Vj0QsHh5RvqCxK0mLvLq7bLjsLhEdR0a0Glp33zRr2rRpHHPMMfGPf/wjItbc8XzppZfWGNpOmTIlvvnmm0S7arC5MbKysmK//fZLLEn+8ccfp23s2nz11Vfx9ddfJ9onn3xyrYF1xJrHExx77LHx4IMPpr2exjoP1XnttdcS24ccckh069atzn1OOeWU+Mtf/pJ4jvm4ceNi7733rnWfli1bxlFHHVXn2HvttVdie9asWVFaWlrrXdsNZf/994/HH388IqLGZ9wvWrQo3n///UT7Bz/4Qa0XK9SlvLw8xo4dm2h/73vfq3YVg0zVoUOHOOiggxps/P333z9uv/32iKj5O2mIc9i7d+/Yd999E9/1448/XuujJapeOHTwwQfXukoDAAAAAACbnjvz2XosOneLCfIbXPmCNeerkVRdan/OnDnx3nvv1dj36aefTmx37do1BgwYkLY6qi6/PX/+/LSNW5sPP/wwqZ3KM97r029DNMZ5WNf8+fNjwYL//P49+OCDU9qvU6dOscsuuyTa657f6uy66641PiO+qi5duiS2Kysro7CwMKWa0q1Tp06J7Zq+n6pBfkTEkUceuVHH/PTTT5OeKb+x421qe+yxR2RnZzfY+FW/k6VLl0ZxcfF6fRrqHFa9O/+ll16K5cuXV9vvq6++Slqpoq4VQAAAAAAA2PTcmQ9knL333jt69OgRM2fOjIg1S+33799/vX7l5eXxr3/9K9E+7rjjUlo2fPny5fHCCy/E22+/HdOmTYuFCxfGypUro7S0tMZ9NlVQW/Wu/GbNmkWPHj1S2m+nnXaq97Ey+Tysq+p5iajf5+3Tp08ixF93nOpUDWJr06JFi6T22uXK06W0tDTeeOONeOWVV+Kzzz6LOXPmxIoVK6oNhteq6fv54osvEts5OTkb9PNS03gRay6A2Jyk+vtqXRUVFTF+/PgYO3ZsfPLJJzFz5sxYsWJFnd99YWHhesvnN9Q5/N73vhd/+MMforCwMIqLi+O5556LH/3oR+v1W7uaQ8SaC3YOO+ywtBwfAAAAAID0Eeaz9eh09xa1zH6DWrvMfiMaNGhQ/O///m9ERDz//PPx61//OnJzc5P6vPXWW7Fo0aJEu+od/dWprKyMBx54IG677bakO2JTUVuAmk5V76Jt165dys80b9++fcrH2BzOw7rWvbu4Q4cOKe9btW9NdylXtaHPLK+srNyg/arz+uuvx7XXXhuzZs2q1341fT9Lly5NbLdr126jHwdQdbyI2OyWZ2/ZsmW995kyZUpcddVV8dlnn9V73+q+l4Y6hy1atIjjjjsuHn744YhYE9qvG+aXl5fHk08+mWgff/zxKa1GAQAAAADApuVfbtl6tBoa0fKkiIoljV1JWqxavSoqKysjKysr8lrkpXfwJh0ishpuCepUDB48OBHmL1u2LF5//fX1lqF+9tlnE9s77bRT9O3bt9Yxr7322njooYfWez0rKyvatWsXzZs3Two5ly1bFsuWLduYj1FvVe/wbd68ecr7rXuXeG02h/OwrnUvOqjP563at74XLzSGZ599NkaMGBEVFRXrvde6devIy8tLuuCgqKgo6REE1Vm5cmViOy9v4+eLquM1bdp0vQttMl19g+vx48fHeeedF0VFReu917Jly2jZsmU0a9YssrKyImJNWD579uxEn+ou9GjIczh06NBEmD9lypSYPn167LDDDon333zzzaSfmSFDhqTt2AAAAAAApI8wn61LVnZE9uZ1B2mNmqyKqKyMyMqKyE5zmJ8Btttuu9htt93io48+iog1S+1XDfOLioripZdeSrQHDRpU63ivvfZaUoDdo0ePOOOMM+KAAw6Inj17Vnun8m233RZ/+ctfNvaj1EvV4Lm64LAmqS7xvrmch3Wteyd1fZa0r9o3HUF2Q1q4cGFcffXViSC/VatWcdppp8Xhhx8effr0qfYihnfeeSfOPPPMWsetev7ScUFD1fHKysqipKRkswv0U1VUVBRXXHFF4vdjTk5O/PCHP4zvfve7seuuu0arVq3W22fmzJnrXXy0roY8h7vttlvsvPPO8emnn0ZExGOPPRaXX3554v3HHnsssb3nnnsmBf0AAAAAAGQOYT6QsQYPHpwI81999dVYsWJFIjh75ZVXEne2ZmVlxfe///1axxo9enRie6eddoqHHnqo2hCuqlSWZE+3Nm3aJLaXLVsWFRUVKS21/+2336Y0/uZyHtZV9bxERCxZsiR69eqV0r5LlvxnNY51x8k0jz/+eOLnukWLFvHQQw/V+Xz7wsLCOsdt165dYnvp0qVRWlq6UUvtVx0vYs1FCAUFBRs8XkQk7mqvr/pc9LIhXn311ZgzZ05ERDRp0iTuvvvu2H///Wvdp77fSUR6zmFVQ4cOjd/+9rcREfH000/HpZdeGk2bNo1vv/02XnnllUQ/d+UDAAAAAGSu1B7GDNAIjjvuuMjOXrPcf3Fxcbz44ouJ955++unEdr9+/aJ79+41jlNRURHjx49PtC+44II6A+yIqPfzytOhakBdVFQUM2fOTGm/adOm1dlnczoP6+rZs2dSe+rUqSnvW7VvqhcANJZ33nknsX388cfXGeRHpPb9VL3zurS0NKWfl1THi4j4+OOPN2q8iPUfK5Hq6guLFy/e6GPX5t13301sH3jggXUG+RH1/04i0nMOqxo0aFDinC5atChef/31iFizyklpaWlErLlg5LjjjkvrcQEAAAAASB9hPpCxOnXqlBScPfPMMxGx5s7iN998M/F6XUvsr70Tea0+ffrUeeySkpKYNGlSfUveaLvvvntS+9///ndK+6XSr6HPQ9XnkFf3vPeNkZ+fH/n5+Yl21e+/NosWLYpPPvkk0d5jjz3SWle6VX2Oed++fVPap+oFGjXZd999k9pjx46tX2Hr6Nu3b9Iy8Rs7XsT6qyZUPRc1WbhwYdKz6RvCwoULE9vp/E4a4hxW1aZNmzjqqKMS7ccffzzpvxERRx11VEoX9AAAAAAA0DiE+UBGGzx4cGL7nXfeiQULFsTzzz+fCKVzcnLie9/7Xq1jVFZWJrVLSkrqPO5zzz0XS5curX/BG2m77bZLunu8avBWk5UrV8a//vWvOvs19Hmo+jz6FStWpLRPfRx22GGJ7ddffz3mzp1b5z6PPPJIlJeXVztGJqr6HRUXF9fZf+bMmYk7rmvTsWPHGDBgQKL9yCOPbNR3lJ2dnRQUP//88xsdqhcUFCQt/f/BBx/Uuc8TTzyxUcdMRX2/k8LCwnjqqafq7NcQ53BdJ598cmL7tddei3//+9/x6aefJl6zxD4AAAAAQGYT5gMZ7cgjj4wWLVpExJq7vceMGZO4Qz8i4tBDD422bdvWOka7du0SY0SsCbVqM3/+/Ljppps2vOiNVDVg+/DDD+sM9EeOHJn0XPiaNPR5qPq878LCwpg3b17K+6bilFNOSWyXlJTEH/7wh/UuUKhqxowZcddddyXaO++8c+y5555prSndunXrltgeN25crX1LS0vjv//7v5MuVqjNT37yk8T2woUL4ze/+U2t568+4xUXF8cVV1yR0gUiNcnJyYlddtkl0X7sscdq7T979uyk77ehdO3aNbH9xhtv1LnqxLXXXhuFhYUpjZ3uc7iu/fbbL/GIitLS0rjssssS72277bZJF3gAAAAAAJB5hPlARmvZsmUcccQRifbo0aPj/fffT7Sr3rlfk+zs7Nhvv/0S7bvuuismTJhQbd9PP/00TjvttFiyZEk0adI4U+SPf/zjpADxN7/5Tbz44ovr9ausrIx77rkn7rvvvpRqbejzsP322yfdnX/zzTen9Q79XXfdNY499thE+6WXXoprrrmm2vBz+vTpcc4558SqVasSr1UNMjPVAQcckNh+66234r777qu236JFi+JnP/tZTJgwIeXv54gjjojDDz880X722Wfj4osvjkWLFtW4z4wZM+Lqq6+OiRMnrvde375947TTTku0J0yYEGeffXbMnDmzxvEWLFgQN998c40rSVT9ft9555249957q+332WefxRlnnBGFhYWRlZVV4/HSoervma+++iquv/76ai+gWLFiRVx55ZXxzDPPpPydNMQ5XFfVu/Orftcnnnhig587AAAAAAA2TtO6uwA0rsGDB8ezzz4bERGzZs1KvN66deukcLI255xzTuJO9FWrVsWZZ54Zhx9+eAwYMCDatGkTS5YsifHjx8ebb74ZFRUV0aVLlxg4cGA8/PDDaf88dWnZsmVce+21ccEFF0RFRUWUlJTEhRdeGAMGDIhDDjkk2rdvH/Pnz48XX3wxPvvss4iIOP/88+OOO+6oc+yGPA+5ubkxaNCg+Oc//xkREc8880w8//zzUVBQEM2bN0/0GzhwYFx88cUbcGYirrrqqvjggw8Sy5E//PDD8frrr8cxxxwTvXr1iqKiopg8eXK89NJLSSH/GWeckRSUZ6qhQ4fGXXfdlXi0wY033hj/+te/YuDAgZGfnx8rVqyIjz/+OF566aVYuXJlZGdnxwUXXBAjR45MafzrrrsufvSjH8XXX38dEREvvPBCvPHGG3HIIYfEHnvsEe3atYuioqKYOXNmvP/++zFlypSIiDjuuOOqHe+yyy6Ljz76KCZPnhwRa8LoY445Jg488MDYd999o0OHDlFSUhJz586NyZMnx3vvvRcVFRVx/fXXVzveySefHPfdd1/Mnz8/IiJuuummeOmll+KII46IDh06xNKlS+Pdd9+N119/PcrLy+PAAw+MoqKipAt80u3www+PXr16Jc7ZqFGj4q233oqjjz46CgoKoqioKKZOnRovvvhifPvttxERMXz48LjttttSGj/d53BdJ554Yvz5z3+OsrKyxGtNmjSJk046KfWTAAAAAABAoxDmAxnvwAMPjI4dO8bixYuTXj/66KMjNzc3pTH69+8fF154Ydx+++0RsWbJ/pdffjlefvnl9fp26NAhRo4cmdKzyBvKYYcdFr/97W/j6quvTizrPWHChGrvpB84cGAMHz48pTC/oc/Df/3Xf8WkSZNi2rRpEbFmae+1IehaO++8c8rjVVfT3//+9/jpT3+aGHfOnDk13sEdEXH66afHf//3f2/wMTelNm3axP/8z//EsGHDEhcjTJkyJRGqV5WTkxNXXXVV9OrVK+XxO3ToEA899FAMGzYs8Uz6VatWxfPPPx/PP/98vett1qxZPPDAA/GLX/wiXn311YhY852/9tprdT7GoTqtWrWKm266Kc4///woKiqKiIhJkybFpEmT1uu7++67x5/+9KcYPnx4vY9TH02bNo0///nPcfrpp8fy5csjYs3KD9OnT1+vb1ZWVlxwwQVx/PHHpxzmp/scrqtz585x6KGHJv0eP+CAA5JW/wAAAAAAIDNZZh/IeE2bNk1afnutQYMG1Wuc4cOHxx//+Mek55JXlZubG8cee2w89dRTGfFs9aFDh8aDDz5YY/jdoUOHuPTSS+N///d/o2nT1K/Nasjz0K5du3j00Ufj2muvjUMOOSS6du2adFd+OnTv3j2eeuqpuPDCC6N9+/Y19tt1113j3nvvjV//+teb1XLiBx54YPzjH/+IPfbYo8Y+++yzTzz44INxyimn1Hv8Dh06xMMPPxx/+MMf6rwQoGfPnnHhhRcmPct+XS1atIg777wzRo4cGbvuumut4+Xn58dZZ50VBx10UI19vvOd78To0aNj9913r/b9Vq1axTnnnBP/+Mc/om3btrUeL1369u0bjz76aBx44IG19vnrX/+6QatOpPscruuEE05Iag8ZMqTeNQIAAAAAsOllVVZWVjZ2EWz5VqxYEVOnTk20+/TpE61atdqgsT7//PMoKyuLpk2bxo477piuEjc7q1atisrKysjKykp6Tjl1Kysri8mTJ8fUqVOjsLAw2rRpE/n5+dG/f/9o06ZNY5dXrc8++yw+/PDDWLJkSbRr1y622WabGDBgQOTk5GzwmJvjeVhXeXl5TJ48Ob788sv49ttvIzc3Nzp16hR77713FBQUNHZ5G+3zzz+PyZMnx5IlS6J58+bRuXPn2GOPPWKbbbZJ2zG++eab+PDDD2PRokWxatWqaNmyZXTv3j369u0bPXr0qPd48+bNi0mTJsWiRYuisLAw8vLyokuXLtGnT5/Yfvvt6zVW1c/fqlWr6N69e3znO9+JFi1a1Luu+qppjl37CIIFCxZETk5OdO7cOfr27Rs77LBD2o6dznMYETFy5MjEahzt2rWLN954I+VVTerLn9FAKqZMmRKlpaWRk5NT68VrAGwY8yxAwzHHAjScLWGOTWceupZl9oGtTtOmTaNfv37Rr1+/xi4lZX379o2+ffumdczN8TysKzs7O/bdd9/Yd999G7uUBrHjjjs2eCDas2fP6NmzZ9rG69q1axxzzDFpGWtTfP766tGjxwZd5FAf6TyHlZWV8eSTTybagwYNarAgHwAAAACA9LLMPgDAFuqtt96KmTNnJto/+MEPGrEaAAAAAADqQ5gPALCFuvPOOxPb++yzT+y0006NWA0AAAAAAPVhmX0AgC1MSUlJjBw5MiZMmJB47fzzz2/EigAAAAAAqC9hPgDAFuChhx6Khx9+OMrKymL27NmxevXqxHv7779/HHbYYY1XHAAAAAAA9SbMBwDYAixatCg+++yz9V7v3r173HDDDY1QEQAAAAAAG0OYDwCwhcnJyYmCgoIYOHBgnHfeedG+ffvGLgkAAAAAgHoS5gMAbAEuvPDCuPDCCxu7DAAAAAAA0qRJYxcAAAAAAAAAACQT5gMAAAAAAABAhhHmAwAAAAAAAECGEeYDAAAAAAAAQIYR5gMAAAAAAABAhhHmAwAAAAAAAECGEeYDAAAAAAAAQIYR5rPZyc7OjoiI8vLyqKysbORqAICIiMrKyigvL4+I//xZDQAAAADAhhPms9lp2rRpRKwJDYqLixu5GgAgIqK4uDhxkd3aP6sBAAAAANhwwnw2O61bt05sL1++vBErAQDWqvpnctU/qwEAAAAA2DDCfDY7VQOCb7/9NlatWtWI1QAAq1atim+//TbRFuYDAAAAAGw8YT6bnaZNm0abNm0iIqKioiJmzpwZCxYsiKKiosTyvgBAw6qsrIyioqJYsGBBzJw5MyoqKiIiok2bNpbZBwAAAABIA//SymapW7duUV5eHitXroyKiopYvHhxLF68OLKysiI7O7uxy9skysvLE9tby2cG2FTMsXUrLy9f7yK6li1bRrdu3RqpIgAAAACALYswn81SkyZNYptttom5c+cmPaO3srIyysrKGrGyTaekpCSxnZub24iVAGx5zLH116ZNm+jWrVs0aWLhJwAAAACAdBDms9lq0qRJFBQURH5+fhQWFkZhYWGUlZUl3U25JVu9enVUVlZGVlaW5YwB0swcW7fs7Oxo2rRptG7dOlq3bu08AQAAAACkmX91ZbPXtGnTaN++fbRv376xS9mkpkyZEqWlpdG0adPYcccdG7scgC2KORYAAAAAgMZmHVQAAAAAAAAAyDDCfAAAAAAAAADIMJbZ30gVFRUxceLEmDFjRixatCjatGkT3bp1i/79+0deXt4mq6OkpCTee++9mD17dixZsiQ6dOgQBQUF0a9fv8jNzd1kdQAAAAAAAACw8YT5G6i8vDzuvffeGD16dCxYsGC99/Py8uK4446LESNGRNu2bRusjqKiorjtttvisccei6VLl673frt27WLIkCFx0UUXRfPmzRusDgAAAAAAAADSxzL7G2D58uVx2mmnxS233FJtkB8RsWrVqnjkkUdi8ODB8cknnzRIHbNnz44hQ4bEvffeW22QHxGxdOnSuPfee2PIkCExe/bsBqkDAAAAAAAAgPRyZ349lZWVxcUXXxwTJ05MvNa9e/cYPHhwFBQUxJIlS2Ls2LHx4YcfRkTEvHnzYtiwYfHII49Efn5+2upYsWJFDBs2LKZPn554bfvtt49jjz028vPzY968eTFmzJj48ssvIyJi+vTpMWzYsHjooYeiVatWaasDAAAAAAAAgPQT5tfT/fffH2+99Vai/f3vfz+uv/76pOfSDxs2LEaNGhXXXXddVFZWxvz58+Oqq66Ku+66K2113HzzzTFt2rRE++yzz44RI0ZEVlZW4rXhw4fHTTfdFPfdd19EREybNi1uueWW+M1vfpO2OgAAAAAAAABIP8vs18OKFSvinnvuSbR32WWXuPHGG5OC/LXOOOOM+PGPf5xojxs3Lt5///201DFz5sx49NFHE+3DDz88LrvssqQgPyIiKysrLr/88jj88MMTrz3yyCMxc+bMtNQBAAAAAAAAQMMQ5tfDU089lfRs+hEjRkTTpjUvbnDJJZdEixYtEu1Ro0alpY6HHnooSktLI2JNYH/FFVfU2r/q+6WlpfHQQw+lpQ4AAAAAAAAAGoYwvx5efvnlxHZBQUHsv//+tfZv3bp1HH300Yn2G2+8ESUlJWmto3///tGrV69a+/fq1Sv69+9f7f4AAAAAAAAAZB5hfoqKiopiwoQJifYBBxyw3rL21TnggAMS2ytXrtzopfa/+eab+Prrr6sdP9U6vv7665gxY8ZG1QEAAAAAAABAwxHmp+jLL79MLG0fEbHnnnumtN/ee++d1J46depG1TFt2rSk9l577bVBdaw7DgAAAAAAAACZQ5ifoi+++CKp3bNnz5T2KygoiOzs7ET7yy+/TGsd2267bUr79ejRo9ZxAAAAAAAAAMgcwvwUzZo1K6ndrVu3lPbLzs6Ozp07J9ozZ85MWx1NmjSJ/Pz8lPbLz8+PJk3+83VvbB0AAAAAAAAANJymjV3A5mLFihVJ7bZt26a8b5s2bWLevHkREbFy5cq01dGyZcto2jS1rzAnJydatGiROP7G1lFf5eXlSe1Vq1Zt0uNviSoqKhL/XffnE4CNY44FaDjmWICGZZ4FaDjmWICGsyXMsevmn+vmoxtCmJ+idU9+s2bNUt63efPmNY6zMXXUp4a1dawN8Td1mF5cXJzUtjJA+pSXl8fUqVMbuwyALZI5FqDhmGMBGpZ5FqDhmGMBGs6WNMeum49uCMvsp2jdk52Tk5Pyvrm5uYntoqKitNVRnxrSXQcAAAAAAAAADUeYn6J174IvLS1Ned+SkpLEdtW79De2jvrUkO46AAAAAAAAAGg4ltlPUV5eXlK7uLg45WXuq94Fv+44G1NHfZdmSGcd9dWuXbukdrNmzSI7O3uT1gAAAAAAAADQEMrLy5Py23Xz0Q0hzE9Rq1atktrLli2LNm3apLRvYWFhYrtly5Zpq2PVqlVRVlYWTZvW/TWWlZXF6tWr01ZHfeXm5kaXLl026TEBAAAAAAAANleW2U/RNttsk9SeO3duSvuVl5fHggULEu0ePXqkrY7y8vKYP39+SvvNmzcvKioq0lYHAAAAAAAAAA1HmJ+i3r17J7VnzJiR0n6zZ8+O8vLyGsfZVHXMnDmz1nEAAAAAAAAAyBzC/BT17t07cnJyEu3JkyentN+kSZOS2jvttNNG1dGnT5+kdmPVAQAAAAAAAEDDEeanqEWLFtG/f/9E++23347Kyso693vrrbcS23l5edGvX7+NqqNnz57Rs2fPasdPtY5evXoljQEAAAAAAABAZhHm18ORRx6Z2J41a1a8/fbbtfYvLCyMF154IdE++OCDIzc3d6PrOOKIIxLb7777bnz99de19v/666/j3XffTbQHDhy40TUAAAAAAAAA0HCE+fUwePDgaNu2baJ98803R1lZWY39b7311li9enWifcYZZ9TYd+DAgdGnT5/o06dPnWH7j370o8SS/5WVlXHjjTfW2v+GG25IbOfk5MSpp55aa38AAAAAAAAAGpcwvx5at24d55xzTqL98ccfxxVXXBGlpaXr9R09enQ8+OCDifbBBx+80Uvsr7XtttvGSSedlGi/8sor8cc//nG9Zf8rKyvjpptuildffTXx2pAhQ6JHjx5pqQMAAAAAAACAhpFVmcqD30koLS2Ns88+O8aPH594raCgIAYNGhTbbLNNLFmyJMaOHRtTpkxJvN+5c+d49NFHo2vXrjWOO3DgwJg9e3ZivFdeeaXWOlasWBGnnHJKTJ8+PfHaDjvsEMccc0zk5+fH/Pnz47nnnosvv/wy8f6OO+4YDz/8cLRq1arenxsAAAAAAACATUeYvwGWLVsW559/fkyaNKnOvl26dIk77rgjdtttt1r71TfMj4iYNWtWnHvuuUmBfU169+4dd999d2yzzTZ19gUAAAAAAACgcVlmfwO0bds2HnzwwfjFL34RnTt3rrZPXl5enHzyyfHMM8/UGeRvqG222SaeeOKJOOuss6Jt27Y11nrWWWfFE088IcgHAAAAAAAA2Ey4M38jlZeXx8SJE+Obb76JxYsXR5s2baJbt24xYMCAyMvL22R1lJSUxLvvvhuzZ8+Ob7/9Ntq3bx8FBQXRv3//yM3N3WR1AAAAAAAAALDxhPkAAAAAAAAAkGEssw8AAAAAAAAAGUaYDwAAAAAAAAAZRpgPAAAAAAAAABlGmA8AAAAAAAAAGUaYDwAAAAAAAAAZRpgPAAAAAAAAABlGmA8AAAAAAAAAGUaYDwAAAAAAAAAZRpgPAAAAAAAAABlGmA8AAAAAAAAAGUaYDwAAAAAAAAAZpmljFwDUT0VFRUycODFmzJgRixYtijZt2kS3bt2if//+kZeX19jlAWxVpk2bFlOnTo358+dHbm5u5Ofnx9577x1dunRp7NIAGlRJSUl88cUX8fnnn8fixYujuLg4WrduHfn5+bHXXntFp06dNvoY5lhga7Vs2bL4/PPPY86cObFkyZJYtWpV5ObmRtu2bWP77bePnXfeOVq0aLFRxzDHAjQccyxAw5k5c2Z8+OGHMX/+/IiIyM/Pj9133z169OjRyJU1HGE+bCbKy8vj3nvvjdGjR8eCBQvWez8vLy+OO+64GDFiRLRt27YRKgTIDCUlJTF16tT46KOP4sMPP4wPP/wwvvjiiygvL0/0mTp16kYdY+zYsXH77bfHZ599tt572dnZsf/++8cVV1wRO+6440YdByCTLFmyJJ5//vl49dVX47333otVq1bV2HefffaJs88+O4488sh6H8ccC2yNPvzww/jb3/4WEydOjNmzZ9fat3nz5nHUUUfFsGHDYvvtt6/XccyxANX7v//7v7jqqquSXhs+fHhceOGFKY9hjgW2Vn369Nmg/caMGZPy32ffe++9uPnmm2PSpEnVvr/33nvHL3/5y+jXr98G1ZLJsiorKysbuwigdsuXL4/zzz8/Jk6cWGffrl27xh133BG77LLLJqgMILOcfPLJ8dlnn0VpaWmt/TYmzP/tb38bDz74YJ39mjVrFr/97W/jhBNO2OBjAWSKL774IgYPHhxlZWX12u+4446L6667Lpo3b55Sf3MssLV64IEH4vrrr6/XPjk5OTFixIg488wzU+pvjgWo3qJFi+LYY4+NZcuWJb1enzDfHAtszRo6zL/rrrviT3/6U1RUVNTaLzs7Oy655JI477zzNqieTOXOfMhwZWVlcfHFFycF+d27d4/BgwdHQUFBLFmyJMaOHRsffvhhRETMmzcvhg0bFo888kjk5+c3VtkAjWLtXNhQbr/99qT/Oc/Ly4vBgwdHnz59ori4ON5777145ZVXoqKiIoqLi+NXv/pV5Ofnx/7779+gdQE0tJKSkqQgv0mTJrHzzjtHv379onv37tG6detYvHhxTJgwId58881Ye834c889FytWrIg77rgjsrOzaz2GORZgjYKCgthjjz1iu+22i06dOkVeXl6sXLkyvvrqq3jttddi1qxZERFRWloa1113XeTk5MSpp55a65jmWICaXXfddesF+fVhjgX4jy5duqR8QX9ubm6dfR5//PG45ZZbEu2cnJw47rjjYvfdd4+Kior48MMP41//+leUlpZGeXl53HLLLdG5c+c48cQTN/gzZBp35kOGu/vuu+Pmm29OtL///e/H9ddfv94kN2rUqLjuuusS/3B66KGHxl133bVJawVobFWvAm3VqlXssssusfvuu8fEiROTlmDakDvzP/jgg/jBD36QdKy77757vQun3nvvvbjgggti+fLlERHRsWPHeOmll6Jly5b1PiZApvj000/jhBNOiPz8/PjhD38YQ4YMqfHC0SlTpsTFF18cc+bMSbz2m9/8ptagyRwLbO1ef/31+Oabb2LgwIFRUFBQY7/Kysp48MEH47rrrks8RiovLy9eeOGFGp/FbI4FqNnrr78e5557bkRE9O7dO7788svEe6ncmW+OBUj+N9lRo0bFfvvtl5Zx58yZE0cffXSUlJRERES3bt3i3nvvXe9u/unTp8c555wTc+fOjYg1Fwm8+OKL0a1bt7TU0diaNHYBQM1WrFgR99xzT6K9yy67xI033ljt1UpnnHFG/PjHP060x40bF++///4mqRMgU5x++ulx4403xpgxY+K9996L0aNHx2WXXRa9evXa6LH/9Kc/Jbbz8vLizjvvrDbI6tevX/z+979PtBcvXhyjRo3a6OMDNKa8vLy4/PLL46WXXoqf/exnta4Atccee8S9994bzZo1S7x299131zq+ORbY2h1yyCFx+umn1xrkR0RkZWXFaaedFhdddFHitVWrVsWYMWNq3MccC1C91atXxzXXXBMRa+70/O///u96j2GOBWg4f/nLXxJBfnZ2dtx2223VLsu/ww47xG233ZZYEbCkpCT+8pe/bNJaG5IwHzLYU089FUuXLk20R4wYEU2b1vx0jEsuuSRatGiRaPsLIbC1+fWvfx0nnHBCbL/99pGVlZW2cadPnx5vv/12on3GGWdE9+7da+x/9NFHxz777JNo//3vf6/zmU4Amaxnz55x1llnJQX0tendu3ecdNJJifacOXPi888/r7avORag/k499dSkx5fU9LgpcyxAzW677baYPXt2RESce+65sd1229Vrf3MsQMNZvnx5PPXUU4n2scceG3vssUeN/ffYY4849thjE+0nn3wyCgsLG7TGTUWYDxns5ZdfTmwXFBTU+Ryl1q1bx9FHH51ov/HGG4mrlgDYcGPHjk1qDx06tM59Tj755MT2okWL4oMPPkh7XQCZbN1l9WbOnFltP3MsQP21adMmOnTokGh/++231fYzxwJU79NPP03cCLXtttvGsGHD6j2GORag4YwbNy5KS0sT7frOsaWlpTFu3LgGqW1TE+ZDhioqKooJEyYk2gcccEBKd5kecMABie2VK1daah8gDar+xa9nz56xzTbb1LnPgQceWOMYAFuDdZ//uXr16mr7mWMB6q+ysjJWrVqVaLdr167afuZYgPVVVFTEVVddFWVlZRERcdVVV6W8AlVV5liAhlN1fmzevHnsu+++de6z7777RvPmzasdY3MmzIcM9eWXXyZddbTnnnumtN/ee++d1J46dWpa6wLYGk2bNi2xnep83LVr1+jatWu1YwBsDWbNmpXU7tixY7X9zLEA9ff+++/HypUrE+2qyzZXZY4FWN/f//73xONJjj766DjkkEM2aBxzLEDDqTo/7rrrrrU+gnqtnJyc2HXXXasdY3MmzIcM9cUXXyS1e/bsmdJ+BQUFSc/N+/LLL9NaF8DWZv78+bFixYpEO9X5OGLNUn1rrTuvA2zpqj4yat3/oV7LHAtQf0uWLIlrr7020e7QoUMcf/zx6/UzxwKsb968eXHrrbdGxJqVpH71q19t0DjmWIDq/e1vf4shQ4bEfvvtF7vttlt85zvfiUGDBsVVV10VL730UlRUVNQ5RkVFRXz99deJ9obOsV999VVKx8t0dV/GADSKde9k6tatW0r7ZWdnR+fOnWPevHkRUfOzSQFIzYbOxxGRdLX97Nmz01YTQKb77LPP4q233kq0DzrooGjduvV6/cyxAKlZuXJlzJw5M95444144IEHYtGiRRERkZubGzfffLM5FiBF1157bWJlk4suuijy8/M3aBxzLED1ql7YHxHx7bffxrfffhvTpk2L//u//4tevXrFVVddFQcddFCNYyxcuDCKi4sT7Q2dY4uLi2PhwoUbPNdnCmE+ZKiqV3ZGRLRt2zblfdu0aZMI86suuwdA/W3MfFy1b2lpaRQXF2/Qc/gANidlZWXx61//Ounq95///OfV9jXHAlTviiuuiCeeeKLWPrvuumtcc801sccee1T7vjkWINmLL74Yr7zySkRE7LzzznH66adv8FjmWICatWzZMtq2bRvFxcWxdOnSKC8vT7z39ddfx7nnnhsjRoyIs846q9r9151j27Rpk/Kx152PV6xYIcwHGsaqVauS2vX5C13z5s1rHAeA+ll3Hs3NzU1533Xn7pUrV/ofdGCLd/PNNyeeQRoRccopp8Tuu+9ebV9zLED9ZWVlxZAhQ+KXv/xltG/fvsZ+5liA/1ixYkX87ne/i4g18+g111yT9KjS+jLHAvxHbm5uHHXUUXHEEUfEvvvumxSer1q1Kt5999144IEHEiv4VVRUxI033hj5+flx3HHHrTfeujep1meOXLfvlpCRCfMhQ1VdQiRizXNGU1X1L49FRUVpqwlga5Su+bi6sQC2NI899ljcf//9ifZ2220XV155ZY39zbEA1evYsWPieZ8VFRWxYsWKWLp0aUREVFZWxqOPPhpjxoyJ8847L84///xo0qTJemOYYwH+45ZbbokFCxZERMQPfvCD2GuvvTZqPHMswH+MGzcuOnToUO17eXl5ceihh8ahhx4aDzzwQFx//fWJ937729/GoYceGq1atUrap6SkJKm9tc+x6/9NH8gI6149VFpamvK+VSe6qnfpA1B/6ZqPqxsLYEsybty4uPrqqxPtdu3axV/+8pdo0aJFjfuYYwGqN2LEiHjppZfipZdeipdffjnGjx8fb7/9dtxwww2x/fbbR8Sau4xuvfXWGDFiRFRWVq43hjkWYI3JkyfHww8/HBERHTp0iEsvvXSjxzTHAvxHTUH+un7yk5/EGWeckWgvXbo0HnroofX6rRvIb+1zrDAfMlReXl5Suz5XD1W9G3/dcQCon3Xn0XX/Qlibdefuli1bpqUmgEzz3nvvxUUXXRRlZWURsWa+u/vuuxOBU03MsQCp69ChQ5x44onx5JNPxtFHH514/dlnn02EVFWZYwEiysrK4qqrroqKioqIiLj88svr9Xz7mphjATbM8OHDk+bQ1157bb0+686L9cnH1u27JWRkwnzIUOsuK7Js2bKU9y0sLExs+8sgwMbZmPl4+fLlie2cnJwt4kpQgHV99NFHcf755ycuKG3WrFnccccdsccee9S5rzkWoP5yc3PjpptuioKCgsRrd955ZyKoWsscCxBx3333xbRp0yIiYsCAAXHCCSekZVxzLMCGadu2bfTv3z/R/uCDD9brs+4cW3XerMu6fdcda3MkzIcMtc022yS1586dm9J+5eXliec/RUT06NEjrXUBbG02dD5et2/Vf2wF2FJMmzYtzj777FixYkVErPnHyNtuuy3222+/lPY3xwJsmObNm8dJJ52UaM+bNy+mTp2a1MccC2ztFi5cGH/5y18iYs3fU3/zm9+kbWxzLMCG69mzZ2K7tLR0vQC+c+fOSRc6begc26xZs+jcufNGVJoZmjZ2AUD1evfundSeMWNGDBgwoM79Zs+eHeXl5TWOA0D95OfnR6tWrRJB1YwZM1Let2pf8zGwpfn666/jrLPOiqVLl0ZERHZ2dtx0001x2GGHpTyGORZgw/Xt2zepPWPGjNh5550TbXMssLVbtGhRYvWorKysuOCCC2rtX/XfVCMiRo8eHU8//XSiffPNN8eee+4ZEeZYgI3RokWLpHZRUVG0adMm0W7SpEn07NkzsbLKhs6xvXr1iiZNNv/72jf/TwBbqN69e0dOTk6iPXny5JT2mzRpUlJ7p512SmdZAFulqnNpqvPxvHnzYt68edWOAbC5mzNnTvz0pz+NhQsXRsSafxz93e9+F8cee2y9xzLHAmyY3NzcpPa6IVSEORZgrZKSkpgxY0atv2bPnp20z7Jly5LeX3thwFrmWIANs2jRoqR2u3bt1uvTp0+fxPbHH38cZWVldY5bWloaH3/8caK9pcyxwnzIUC1atEh6bsjbb78dlZWVde731ltvJbbz8vKiX79+DVIfwNbkkEMOSWx/8803MWvWrDr3+fe//53UPvTQQ9NeF0BjWLhwYfzkJz+JOXPmJF771a9+FUOGDNmg8cyxABtm3fmyU6dO6/UxxwI0HHMswIaZOHFiYrtLly7rXaQakTzHrl69Ot5///06x33//feTLrzaUuZYYT5ksCOPPDKxPWvWrHj77bdr7V9YWBgvvPBCon3wwQdXOwkCUD9V5+OIiEceeaTOfR599NHEdseOHWOvvfZKd1kAm9zSpUvjrLPOim+++Sbx2qWXXhqnn376Bo9pjgXYMC+99FJiu2nTpkl3L61ljgW2ZjvvvHNMnTo15V8vv/xy0v7Dhw9Pen+//fZLet8cC1B/b7/9dnz11VeJ9gEHHFBtv8MOOyyaNv3P0+LrO8fm5OQI84GGN3jw4Gjbtm2iffPNN9e6lMitt94aq1evTrTPOOOMBq0PYGux4447Jv1P+6hRo5LuSF3XCy+8kHSF6Y9//OMt4vlMwNZtxYoVcc455ySeWRcRMWzYsDjvvPM2alxzLLC1KyoqioqKinrtM2bMmKSV+fbbb7+kfz9YyxwL0HDMscDWrrS0NKXl79dasmRJ/PrXv0567fjjj6+2b5s2bWLw4MGJ9pgxY2LKlCk1jj1lypQYM2ZMoj148OBo06ZNyrVlMn9SQAZr3bp1nHPOOYn2xx9/HFdccUWUlpau13f06NHx4IMPJtoHH3ywJfYB0ui//uu/EturVq2KCy64IBYsWLBev/feey/pL6UdOnSIn/zkJ5uiRIAGU1xcHBdccEF8+OGHidfOOOOM+MUvfpGW8c2xwNbsgw8+iMGDB8eTTz4ZK1eurLVvcXFx/PWvf43LLrss8VqTJk1qnY/NsQANxxwLbM3mz58fxxxzTDzyyCNRWFhYa9/3338/TjnllKRHkhx44IE13pkfsWaFlJycnIiIKC8vj4svvji++OKL9fpNnz49LrrooigvL4+INXflDx8+fEM+UkbKqkzlIdxAoyktLY2zzz47xo8fn3itoKAgBg0aFNtss00sWbIkxo4dm3RFUufOnePRRx+Nrl27NkbJAI1m1KhRMXr06PVeX7x4cdI/jG677bbr9enatWu1+1b1pz/9Ke68885Eu2XLlnH88cfHTjvtFMXFxfHee+/Fyy+/nLizKjs7O/7617/GwQcfvKEfCSAjPPnkk3H55ZcnvdajR4/IyspKeYyjjjoqRowYUeP75lhgazV+/PjEynrNmzePvfbaK3bZZZfIz8+P1q1bR3l5eSxZsiQ+++yzePPNN9f7h9Irr7yyzkDIHAtQt1mzZsURRxyRaA8fPjwuvPDCOvczxwJbq6rzZm5ubuyzzz6x8847R7du3aJVq1ZRUlISc+fOjbfffnu9u+q33Xbb+Oc//xkdOnSo9RiPPPJI0sVQubm5cdxxx8Vuu+0WEREffvhhPPfcc0k3wf7+97+PoUOHputjNrqmdXcBGlNOTk7cfvvtcf7558ekSZMiImL27NlJf0GsqkuXLnHHHXcI8oGt0rJly2LGjBl19quuz9orN2tzySWXxNKlS+Phhx+OiIiVK1fGP/7xj2r75ubmxrXXXut/zoEtQnXLP8+cObNeYyxevLjW982xAGuW3H/nnXfinXfeqbNv69at48orr4whQ4bU2dccC9BwzLEAESUlJSn/PXa//faLP/7xj3UG+RERQ4cOjUWLFsVtt90WFRUVUVJSEk888UQ88cQT6/Vt0qRJXHzxxVtUkB9hmX3YLLRt2zYefPDB+MUvfhGdO3eutk9eXl6cfPLJ8cwzzySuSAIgvbKysuLaa6+NkSNHxk477VRtnyZNmsSBBx4Yjz32WJx00kmbuEKAzZc5Ftha9enTJy699NLo379/NGvWrM7+3bp1i2HDhsW//vWvlIL8CHMsQEMyxwJbq3bt2sWpp54a22+/fZ0r92VlZcU+++wTf/rTn+KBBx6I/Pz8lI9zwQUXxKhRo2Kvvfaqsc/ee+8do0aNimHDhqU87ubCMvuwmSkvL4+JEyfGN998E4sXL442bdpEt27dYsCAAZGXl9fY5QFsVaZOnRpTp06NBQsWRE5OTuTn58fee+9dr7+MAlA9cyywNSotLY3p06fH119/HQsWLIhVq1ZFdnZ2tG7dOjp37hw777xzFBQUbPRxzLEADcccC2yNVqxYEdOmTYtZs2bF4sWLY/Xq1ZGTkxNt2rSJ7t27x5577hlt2rTZ6OPMmDEjPvzww5g/f35EROTn58fuu+9e7WNVtxTCfAAA4P+1d6exUZbdH4BPS6GAimMXmoAKioi40igucX1TjXvUREVDiIKKG4gRFaNEEz+gjZpo3OKK1Yhxw13jEtQIRkSDiAjaaKOpCEJbWhTpgO37wT/zOnSbCoXHv9eVNOl55jznuTvf4Df3PQAAAABAwjhmHwAAAAAAAAASRpgPAAAAAAAAAAkjzAcAAAAAAACAhBHmAwAAAAAAAEDCCPMBAAAAAAAAIGGE+QAAAAAAAACQMMJ8AAAAAAAAAEgYYT4AAAAAAAAAJIwwHwAAAAAAAAASRpgPAAAAAAAAAAkjzAcAAAAAAACAhBHmAwAAAAAAAEDCCPMBAAAAAAAAIGGE+QAAAAAAAACQMMJ8AAAAAAAAAEgYYT4AAAAAAAAAJIwwHwAAAAAAAAASRpgPAAAAsI3V1tbGiBEjMj/33nvv9l4SAAAACVOwvRcAAAAAbHu1tbVRUVGxVWbdf//9cfzxx2+VWQAAAMCf7MwHAAAAAAAAgIQR5gMAAAAAAABAwjhmHwAAAIiysrKYNWvW37q3uLh4K68GAAAAEOYDAAAAUVBQELvuuuv2XgYAAADwfxyzDwAAAAAAAAAJI8wHAAAAAAAAgIRxzD4AAACwzaXT6fjss8/ip59+ioaGhkilUjF06NA4+OCDo1evXls0u6WlJRYvXhw1NTVRV1cXra2tUVxcHEOHDo2DDjoo8vO3zt6GmpqaWLp0aTQ0NERTU1P069cvSktLY/jw4bHXXntt0XNaWlpi4cKF8eOPP8aqVauif//+MXjw4Bg9enTsuOOOW2X9AAAAJJswHwAAANjqamtro6KiIlNPmjQpJk+eHL/++mvcf//9MXv27FizZk2b+4qLi2P8+PExYcKEbof6TU1N8eCDD8ZLL70UDQ0N7fakUqk444wz4oorrohUKtWt+Zue8fjjj8fLL78cP//8c4d9u+yyS/znP/+J888/Pw488MCc57e2tkZVVVVUVVXF8uXL27zeu3fvOOecc2LKlCl/a/0AAAD8cwjzAQAAgG3i559/jvHjx0dNTU2HPXV1dXHnnXfGe++9F48++mjstNNOOc1esGBBTJo0qd0PCPzVmjVroqqqKl5++eW455574ogjjsh5/e+++27ceOON0dTU1GVvQ0NDzJ49O77++ut45ZVXcpq/du3auPrqq2Pu3Lkd9mzYsCFmzZoV8+fPj5kzZ0ZZWVnO6wcAAOCfRZgPAAAA9Ljm5uaYOHFiJsjv06dPjBo1KkpLS6OxsTEWL14cjY2Nmf4vvvgiLr744njyySejsLCw09nz5s2Lyy+/PJqbm7OuDxs2LPbcc8/Iy8uLmpqaqK6uzrzW2NgYl1xySdx3331x3HHHdbn+J554Im6//fZobW3Nul5aWhojRoyIVCoV69evjxUrVsS3334b6XS6y5l/9ccff2QF+X379o0DDzwwSktLY/369fHVV1/FypUrM/3fffdd3HDDDTFz5sxuPQcAAIB/DmE+AAAA0OOeffbZaGpqiry8vBg3blxcddVVWbvu0+l0PPfcc3HnnXfG77//HhF/Bvr33XdfTJ06tcO5dXV1cd1112UF+fvtt1/ceuutsf/++2f1Llu2LKZPnx6LFy+OiD93uU+bNi1effXVTne4f/TRR1FZWZkV5I8ePTquueaaKC8vj7y8vKz+dDodc+fOjZdeeil++umnHN6diGeeeSbWrFkThYWFMWXKlBg7dmz07ds383pra2vMnj07brnlltiwYUNERHz88cfx4YcfxrHHHpvTMwAAAPhnyWvd/CPlAAAAwP97m3+nfVlZWcyaNavbc/r16xfFxcVdzt/k+uuvj4suuqjDeXPnzo3LLrssE1gXFBTEW2+9Fbvvvnu7/TfddFO88MILmbq8vDxmzpwZ/fr1a7d//fr1MWHChPj8888z10477bS466672u3//fffo6KiIurq6jLXxo4dG9OnT4/8/PwO/45NVq9eHSUlJW2ut/f+9OnTJ2bOnBmHHHJIh/OeffbZuPnmmzP1SSedFPfcc0+X6wAAAOCfR5gPAAAA/0Idhe3dVVFREQ888EBO8w899NB46qmnupxZWVkZjz/+eKa+6KKL4vrrr2/T19DQEMcee2xmV37fvn3jjTfeiF133bXT+cuXL49TTjklcwJA7969Y86cOTFw4MA2vVVVVTFjxoxMfdhhh0VVVVWb3fjd1d77c80118Sll17a6X0tLS1x3HHHZY7cLykpiXnz5m3RWgAAAEimrj9CDgAAALAVXHHFFTn1TZw4MXr37p2pX3vttXb73nnnnazj9c8666wug/yIiEGDBsW5556bqTds2BBvvvlmu73PP/98Vn3jjTducZDfnv79+8fYsWO77MvPz4+jjz46U69evTpWrVq11dcDAADA9ifMBwAAAHpcUVFRHHbYYTn17rLLLnH44Ydn6l9++SWWL1/epm/hwoVZ9WmnnZbzejbv3XxWRER9fX1UV1dn6gMOOCD22WefnJ/RHeXl5bHjjjvm1Lvnnntm1fX19T2xJAAAALazgu29AAAAAGD7Gzx4cMyZM6fH5u+77745fcf8JgcccEB89NFHmXrJkiUxaNCgrJ4lS5Zkfu/Vq1fsv//+3VpPnz59Ip1Ot5m1yaJFi7Lqzr7LfkttHtB3Zqeddsqqf/311629HAAAABLAznwAAACgx+2+++7d6h8yZEhWXVdX16bnrzvSy8rKom/fvjnPLygoiN12263dWZusXr06qx42bFjO87tr84C+MwUF2XszNm7cuLWXAwAAQAII8wEAAIAel+sR8h31NzU1ten567Xuzo/IDtB/++23NqF4Q0NDh/1bW3dOLQAAAODfwb8UAQAAAHKQl5e3vZcAAADAv4gwHwAAAOhx3f1e9837BwwY0Kbnr9f+zvfGr127NvP7Djvs0Ob4+lQqlVW3dzoAAAAA9BRhPgAAANDjfvzxx271//DDD1l1cXFxm56ioqLM7ytXroz169fnPH/jxo1RW1vb7qxNSkpKsurvv/8+5/kAAACwpYT5AAAAQI9bsmRJtLS05Ny/ePHirHq//fZr0/PXa3/88Ud89dVXOc9funRpNDc3dzp/1KhRWfVnn32W83wAAADYUsJ8AAAAoMc1NDTE/Pnzc+795JNPMvXAgQNj0KBBbfrKy8uz6rfeeivn9bz++uudzor4c7f+3nvvnam//PLL+Oabb3J+BgAAAGwJYT4AAACwTTzwwAM59T388MOxYcOGTH366ae323fCCSdEYWFhpp49e3asWLGiy/krV66M5557LlMXFBTEySef3G7vueeem1Xffvvt0dra2uUzAAAAYEsJ8wEAAIBt4tNPP43HHnus05558+bFU089lakLCgpizJgx7fYWFRXFqaeemqnXrVsX1157bdbx+Ztrbm6Oa6+9NtatW5e5duKJJ0ZZWVm7/WeffXaUlJRk6o8//jhmzJiRc6C/evXqnPoAAABgc8J8AAAAIDZu3Bi1tbV/66eurq7L+QMGDIiIiDvuuCNmzJgRa9euzXo9nU7H008/HVdeeWXWrvwJEybEkCFDOpw7derUKCoqytQLFiyIcePGxdKlS9v0Llu2LMaNGxeffvpp5trOO+8c06ZN63B+v379orKyMvLz//dfKE8++WRccMEFsXDhwnbvSafT8f7778fkyZNj4sSJHc4GAACAzhRs7wUAAAAA29/KlSujoqLib91bUVHR5RH6Y8aMiQ8++CCqq6ujqqoqnnnmmSgvL4/S0tJobGyML7/8MhobG7PuGTVqVEyaNKnTuSUlJVFZWRlXXnllpNPpiIhYtGhRnHnmmTF8+PDYY489Ii8vL2pqauLbb7/Nurd3795x2223dbgrf5Ojjjoqpk2blnXE/vz58+O8886L0tLSGDFiRKRSqWhubo4VK1bEN998k1nLPvvs0+lsAAAA6IgwHwAAAOhxhYWF8dBDD8X48ePjhx9+iHQ6HfPnz++wf9SoUfHII49EYWFhl7OPOeaYeOSRR2LKlCmxZs2azPXq6uqorq5u954BAwbE3XffHUceeWRO67/wwgtj4MCBMX369Pjtt98y11etWhWrVq3KaQYAAAB0h2P2AQAAgG1i8ODB8eKLL8YFF1wQO++8c7s9xcXFMXXq1Hj66aczR/Pn4vDDD4+33347xo8fH6lUqsO+VCoV48aNi7fffjvnIH+TU045Jd57772YMGFClJSUdNpbUlISY8aMicrKym49AwAAADbJa910PhwAAADAVlJbW5t1bP+kSZNi8uTJmTqdTseCBQti+fLlUV9fH6lUKoYMGRKjR4+OXr16bdGzW1paYtGiRVFTUxP19fUREVFUVBRDhw6Ngw46aIvnR0S0trbGsmXLorq6Ourr62PdunXRv3//KCsri+HDh8ewYcMiLy9vi58DAADAv5dj9gEAAIBtrk+fPt3eGZ+r/Pz8KC8vj/Ly8h6ZHxGRl5cXI0eOjJEjR/bYMwAAAPh3c8w+AAAAAAAAACSMMB8AAAAAAAAAEkaYDwAAAAAAAAAJI8wHAAAAAAAAgIQR5gMAAAAAAABAwgjzAQAAAAAAACBhhPkAAAAAAAAAkDB5ra2trdt7EQAAAAAAAADA/9iZDwAAAAAAAAAJI8wHAAAAAAAAgIQR5gMAAAAAAABAwgjzAQAAAAAAACBhhPkAAAAAAAAAkDDCfAAAAAAAAABIGGE+AAAAAAAAACSMMB8AAAAAAAAAEkaYDwAAAAAAAAAJI8wHAAAAAAAAgIQR5gMAAAAAAABAwgjzAQAAAAAAACBhhPkAAAAAAAAAkDDCfAAAAAAAAABIGGE+AAAAAAAAACSMMB8AAAAAAAAAEkaYDwAAAAAAAAAJI8wHAAAAAAAAgIQR5gMAAAAAAABAwvwXsANZQasVI/sAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "image/png": {
              "width": 1017,
              "height": 717
            }
          }
        }
      ],
      "source": [
        "plt.plot(A2, label='train accuracy')\n",
        "plt.plot(B2, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd9c9eGmecP"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jFRntT6mgoW",
        "outputId": "b7bc2d48-2dec-4d06-e42f-57b41fac6f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5151515151515151"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CBG2MIlgmkHv"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXTHRhD5mqnn",
        "outputId": "963bb54d-1d53-4c69-9214-103db480cec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTdojX6Jmqqm",
        "outputId": "ec78423d-438f-4091-d2ae-a7bc5e52e130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Faixa 1       0.60      0.67      0.63        18\n",
            "     Faixa 2       0.00      0.00      0.00         9\n",
            "     Faixa 3       0.62      0.83      0.71         6\n",
            "\n",
            "    accuracy                           0.52        33\n",
            "   macro avg       0.41      0.50      0.45        33\n",
            "weighted avg       0.44      0.52      0.47        33\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "HQ54xluu8RL3",
        "outputId": "a598884b-246e-40e4-fcf0-d9a1df8774ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 981,
              "height": 730
            }
          }
        }
      ],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('Faixa TRL verdadeira')\n",
        "  plt.xlabel('Faixa TRL predita');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mZTajeJuiP_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49586e3ef35f418c9834f579f8e3127b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55b635e83840445f806dbc06f6ba2952",
              "IPY_MODEL_dcf708ea405943379c931248713dc20b",
              "IPY_MODEL_cfe47406b784481a93cbfa4a76bde335"
            ],
            "layout": "IPY_MODEL_333503810fdd44348c32655649bef518"
          }
        },
        "55b635e83840445f806dbc06f6ba2952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aced19eccbd2457fb8b6956e560ef5e9",
            "placeholder": "​",
            "style": "IPY_MODEL_abfc20fa4b104311850120fd0e00a04b",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "dcf708ea405943379c931248713dc20b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22354895248c4dbcb11b03cb87c16c95",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37dc3be83829465a85edf9d3a6894e25",
            "value": 43
          }
        },
        "cfe47406b784481a93cbfa4a76bde335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d14541d6c62e42259af364754d5c69b2",
            "placeholder": "​",
            "style": "IPY_MODEL_291ba8f4255146149ae454f3f16e3c44",
            "value": " 43.0/43.0 [00:00&lt;00:00, 682B/s]"
          }
        },
        "333503810fdd44348c32655649bef518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aced19eccbd2457fb8b6956e560ef5e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abfc20fa4b104311850120fd0e00a04b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22354895248c4dbcb11b03cb87c16c95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37dc3be83829465a85edf9d3a6894e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d14541d6c62e42259af364754d5c69b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "291ba8f4255146149ae454f3f16e3c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a329b7532b7840b980be3fd3f6a6babd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8061142a0a62440495a806eaa8919533",
              "IPY_MODEL_c52f80d3a865412884b6e2a820a86e58",
              "IPY_MODEL_45080521893b4bcbbba478e301e8361a"
            ],
            "layout": "IPY_MODEL_32e941bb41984c8cb9ea6eb77b9aac2d"
          }
        },
        "8061142a0a62440495a806eaa8919533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f71d495c2334358a6e2396c60739469",
            "placeholder": "​",
            "style": "IPY_MODEL_c3fe3c5dd7054244a01ccb5ecca3d429",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "c52f80d3a865412884b6e2a820a86e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_037c44d50beb42bb97cffcdbead0b5a5",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0de1fce48224a7bab4e3e5174a1f145",
            "value": 209528
          }
        },
        "45080521893b4bcbbba478e301e8361a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1557d76213d04d55b737aed86fd59cc7",
            "placeholder": "​",
            "style": "IPY_MODEL_6ac4a72d69c144e09f1e37c615183b21",
            "value": " 210k/210k [00:00&lt;00:00, 6.46MB/s]"
          }
        },
        "32e941bb41984c8cb9ea6eb77b9aac2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f71d495c2334358a6e2396c60739469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3fe3c5dd7054244a01ccb5ecca3d429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "037c44d50beb42bb97cffcdbead0b5a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0de1fce48224a7bab4e3e5174a1f145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1557d76213d04d55b737aed86fd59cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ac4a72d69c144e09f1e37c615183b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f696c823a1640559a8b4d21b404e1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b8d0442fd19474e9bf696d49578d47d",
              "IPY_MODEL_824e85318c4a4e37ae08262746e7e934",
              "IPY_MODEL_3a7f336694ee49bea7b6f8cf6aab3d0f"
            ],
            "layout": "IPY_MODEL_24c9047ddce14032bcffaba84ec993b1"
          }
        },
        "0b8d0442fd19474e9bf696d49578d47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48b838e61b942a180c61a6390c6d420",
            "placeholder": "​",
            "style": "IPY_MODEL_5d9282289e7c4eddaeb7bcfea5dc3bd6",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "824e85318c4a4e37ae08262746e7e934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a539bd7e3c04c6f8411e3fc4f353a22",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c43b7838b104c3e8e62009e9302afc9",
            "value": 2
          }
        },
        "3a7f336694ee49bea7b6f8cf6aab3d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad10bea2fe4949b3ba9fb241143b718c",
            "placeholder": "​",
            "style": "IPY_MODEL_1a714cfc844b4d41a5a180ff15d6e636",
            "value": " 2.00/2.00 [00:00&lt;00:00, 111B/s]"
          }
        },
        "24c9047ddce14032bcffaba84ec993b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48b838e61b942a180c61a6390c6d420": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9282289e7c4eddaeb7bcfea5dc3bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a539bd7e3c04c6f8411e3fc4f353a22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c43b7838b104c3e8e62009e9302afc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad10bea2fe4949b3ba9fb241143b718c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a714cfc844b4d41a5a180ff15d6e636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "056a811909654e2fa259ecabec441e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d1f33000ac64f2e9bef53e89203b9fa",
              "IPY_MODEL_33e88798afab4e49a095278024d74f70",
              "IPY_MODEL_813d21acb87143798c79efb3668f02ed"
            ],
            "layout": "IPY_MODEL_c690bdf457f04aa2920f986cc718f560"
          }
        },
        "4d1f33000ac64f2e9bef53e89203b9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56c6644b19da45708ac45a3285eff1d4",
            "placeholder": "​",
            "style": "IPY_MODEL_d278207ee0ef41c981ccddf791f0eae1",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "33e88798afab4e49a095278024d74f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bca35ede9de04d2dbd73249dc762ae70",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acaf4777d74e418bae970790160d4229",
            "value": 112
          }
        },
        "813d21acb87143798c79efb3668f02ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c704368f120447a69d95378a2aff4cc1",
            "placeholder": "​",
            "style": "IPY_MODEL_94a288ff2ceb4c98a0cf5ea52dfb840c",
            "value": " 112/112 [00:00&lt;00:00, 5.87kB/s]"
          }
        },
        "c690bdf457f04aa2920f986cc718f560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c6644b19da45708ac45a3285eff1d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d278207ee0ef41c981ccddf791f0eae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bca35ede9de04d2dbd73249dc762ae70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acaf4777d74e418bae970790160d4229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c704368f120447a69d95378a2aff4cc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94a288ff2ceb4c98a0cf5ea52dfb840c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c89d54b36f044beb8bc436e8b1fe624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_016c44500d754211bc1db1b5de46afb6",
              "IPY_MODEL_e9466442ac9d46c6ab5e9c2bedf57d59",
              "IPY_MODEL_816f609877fc4ee0a66752ec6f6bcaf5"
            ],
            "layout": "IPY_MODEL_b9549bf614a54a68b2e3e42317a62cd8"
          }
        },
        "016c44500d754211bc1db1b5de46afb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99c987c9d85d43109f78d186d94e7cc4",
            "placeholder": "​",
            "style": "IPY_MODEL_1a7c02cc4f3a4132878f56359cacda3f",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "e9466442ac9d46c6ab5e9c2bedf57d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3b02aa1e1c9403088e0da0163745956",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ed01970fb5748b189239729fd170746",
            "value": 647
          }
        },
        "816f609877fc4ee0a66752ec6f6bcaf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88594bcc156141b3862bcb0634e711a2",
            "placeholder": "​",
            "style": "IPY_MODEL_ee3163d5f056419fbf242f42158eeb9c",
            "value": " 647/647 [00:00&lt;00:00, 20.4kB/s]"
          }
        },
        "b9549bf614a54a68b2e3e42317a62cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c987c9d85d43109f78d186d94e7cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a7c02cc4f3a4132878f56359cacda3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3b02aa1e1c9403088e0da0163745956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ed01970fb5748b189239729fd170746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88594bcc156141b3862bcb0634e711a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee3163d5f056419fbf242f42158eeb9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "babdb282921444c1bd05404e1a92f16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92eafe18f3e8490cb642945f737d90cb",
              "IPY_MODEL_c8bf859e63fd43ab93399973b29f1a1d",
              "IPY_MODEL_b82826cfb2fd4126a518e9b1a2c890a0"
            ],
            "layout": "IPY_MODEL_dafd3265dad74de5b515a75c1ee3b8e9"
          }
        },
        "92eafe18f3e8490cb642945f737d90cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cea3a0295864117bb9f103060929514",
            "placeholder": "​",
            "style": "IPY_MODEL_3cc2fed01b6b4e2ead4a17588b6b9030",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "c8bf859e63fd43ab93399973b29f1a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd2f08445f87416ca9c7d6d0a8c0636d",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dec26ba1b904bd4a9281d29a1ad5a46",
            "value": 438235074
          }
        },
        "b82826cfb2fd4126a518e9b1a2c890a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34dfa3a129d445f4a7e563ecdfbe9c1c",
            "placeholder": "​",
            "style": "IPY_MODEL_2d59e1cecb7248f98f5d149ee68ccabc",
            "value": " 438M/438M [00:01&lt;00:00, 319MB/s]"
          }
        },
        "dafd3265dad74de5b515a75c1ee3b8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cea3a0295864117bb9f103060929514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc2fed01b6b4e2ead4a17588b6b9030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd2f08445f87416ca9c7d6d0a8c0636d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dec26ba1b904bd4a9281d29a1ad5a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34dfa3a129d445f4a7e563ecdfbe9c1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d59e1cecb7248f98f5d149ee68ccabc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}