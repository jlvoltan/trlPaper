{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6jKuooZfNFU"
      },
      "source": [
        "# Classificação faixa TRL - Bertimbau + Rede Neural + 512 tokens [kfold][P3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9-gf3IMD5C"
      },
      "source": [
        "- BERTimbau Base (aka \"bert-base-portuguese-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i41YvsyfSUR"
      },
      "source": [
        "## Código Versão 1.0 - 21 OUT 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfR22FfgbJ"
      },
      "source": [
        "- Uso dos classificadores: *Rede Neural*\n",
        "- **k-Fold cross-validation: Esse código executou a rodada 3**.\n",
        "- Matriz de confusão;\n",
        "- Semente 42\n",
        "- 50 épocas\n",
        "- BATCH_SIZE = 8\n",
        "- Limite 512 tokens\n",
        "- optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.csv   ou  dataset_pre_processado_1.csv  ou  dataset_pre_processado_stem_2.csv\n",
        "#     CSV1                  CSV2                                   CSV3\n",
        "dataset = \"dataset.csv\""
      ],
      "metadata": {
        "id": "vXMRttjhRQ2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lembre-se estamos usando o dataset: \" + dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJkf84RR64K",
        "outputId": "6d65a7a9-f980-4d3b-d134-c398a17db713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lembre-se estamos usando o dataset: dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "particao=3  # O KFold apresenta 5 partições (rodadas)\n",
        "particao-=1  #Funcionará com o índice de kfold_train e kfold_test"
      ],
      "metadata": {
        "id": "k2TIOYIqQNps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melhor_modelo = 'best_model_TRL_bertimbau_base_neural_' + str(particao+1) + '.bin'\n",
        "melhor_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lFCPPlujqL5L",
        "outputId": "f2cc0d1c-0d1f-4bb3-fdcc-c240702ae283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_model_TRL_bertimbau_base_neural_3.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512"
      ],
      "metadata": {
        "id": "_6clpQcISMQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jlIjUzOgRY"
      },
      "source": [
        "- 63,7% treinamento - 16% validação...............20% teste\n",
        "- EPOCHS = 50\n",
        "- lr=2e-5\n",
        "- BATCH_SIZE = 8\n",
        "- Quantidade de tokens máxima = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Msvyn0MfsOS"
      },
      "source": [
        "## Preparação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjC81VL_gQ-5"
      },
      "source": [
        "### Bibliotecas e ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cZxPMZOfICS",
        "outputId": "2863ff0b-6212-454c-a008-24c7dedc8204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5RDBcpVf0TS",
        "outputId": "521cd54e-856e-4ae5-b897-e17b7029aff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmcqgWcf0WZ",
        "outputId": "db11f274-cebd-49d2-9c87-35551d9c1024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 25 23:58:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoneZ3CngEDZ",
        "outputId": "f2751985-44ef-467f-e1aa-17c30152b2ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YWHFe7ef0Zp",
        "outputId": "8fa677f5-681b-499d-e958-d83f4993f226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy       : 1.23.5\n",
            "pandas      : 1.5.3\n",
            "torch       : 2.1.0+cu118\n",
            "transformers: 4.34.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particoes KFOLD"
      ],
      "metadata": {
        "id": "oENoyHsUPjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para mais detalhes, consulte o código *1-kfold.ipynb* em que as partições foram sorteadas. As células a seguir recriam o resultado obtido."
      ],
      "metadata": {
        "id": "qL3FKCGlPtBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = np.array([  0,   1,   2,   3,   5,   7,   9,  10,  11,  12,  13,  14,  15,\n",
        "         16,  17,  19,  20,  21,  22,  23,  25,  26,  27,  28,  30,  32,\n",
        "         33,  34,  35,  36,  37,  39,  40,  44,  45,  46,  47,  48,  49,\n",
        "         50,  52,  53,  54,  55,  56,  57,  59,  60,  61,  63,  64,  66,\n",
        "         68,  69,  70,  71,  72,  75,  76,  77,  78,  79,  80,  81,  82,\n",
        "         83,  84,  85,  87,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
        "        102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
        "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130,\n",
        "        131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145,\n",
        "        146, 148, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
        "        164, 165, 166, 167])\n",
        "\n",
        "array2 = np.array([  0,   1,   3,   4,   5,   6,   7,   8,  10,  13,  14,  15,  16,\n",
        "         17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
        "         34,  36,  37,  38,  40,  41,  42,  43,  44,  45,  47,  49,  51,\n",
        "         52,  53,  54,  55,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  70,  71,  73,  74,  75,  76,  77,  80,  83,\n",
        "         84,  86,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99,\n",
        "        100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 113, 114, 115,\n",
        "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
        "        129, 130, 132, 133, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
        "        147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162,\n",
        "        163, 165, 166, 167])\n",
        "array3 = np.array([  1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
        "         17,  18,  20,  21,  22,  23,  24,  25,  28,  29,  30,  31,  32,\n",
        "         33,  35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,\n",
        "         48,  50,  51,  52,  54,  55,  56,  57,  58,  59,  62,  63,  65,\n",
        "         67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
        "         82,  84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,\n",
        "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111,\n",
        "        112, 113, 114, 118, 120, 121, 122, 123, 124, 126, 128, 129, 131,\n",
        "        132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 146, 147,\n",
        "        148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162,\n",
        "        163, 164, 165, 166])\n",
        "array4 = np.array([  0,   2,   3,   4,   6,   8,   9,  10,  11,  12,  15,  16,  18,\n",
        "         19,  20,  21,  22,  24,  25,  26,  27,  28,  29,  31,  32,  33,\n",
        "         34,  35,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
        "         49,  50,  51,  53,  56,  57,  58,  60,  61,  62,  63,  64,  65,\n",
        "         66,  67,  68,  69,  71,  72,  73,  74,  75,  76,  78,  79,  80,\n",
        "         81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
        "         94,  95,  98, 100, 101, 103, 104, 106, 107, 108, 109, 110, 111,\n",
        "        112, 114, 115, 116, 117, 119, 121, 123, 124, 125, 127, 128, 129,\n",
        "        130, 131, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145,\n",
        "        146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159,\n",
        "        162, 163, 164, 166, 167])\n",
        "array5 = np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
        "         14,  15,  16,  17,  18,  19,  21,  23,  24,  26,  27,  28,  29,\n",
        "         30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,  43,\n",
        "         44,  46,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,\n",
        "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  72,  73,  74,\n",
        "         77,  78,  79,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
        "         92,  94,  95,  96,  97,  99, 101, 102, 103, 105, 106, 107, 108,\n",
        "        110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 127,\n",
        "        128, 130, 131, 132, 133, 134, 136, 137, 138, 140, 141, 143, 144,\n",
        "        145, 146, 147, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160,\n",
        "        161, 162, 164, 165, 167])\n",
        "# Criar a lista composta\n",
        "kfold_train = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "Cn4vhJ9LPncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as cinco arrays numpy\n",
        "array1 = np.array([  4,   6,   8,  18,  24,  29,  31,  38,  41,  42,  43,  51,  58, 62,  65,  67,  73,  74,  86,  88,  89,  90,  95, 101, 107, 111, 128, 133, 144, 147, 149, 152, 156, 162])\n",
        "array2 = np.array([  2,   9,  11,  12,  21,  28,  32,  33,  35,  39,  46,  48,  50, 56,  72,  78,  79,  81,  82,  85,  87,  91, 103, 106, 108, 131, 134, 140, 141, 146, 148, 158, 159, 164])\n",
        "array3 = np.array([  0,   3,  15,  16,  19,  26,  27,  34,  40,  49,  53,  60,  61, 64,  66,  68,  69,  83,  92,  94, 110, 115, 116, 117, 119, 125, 127, 130, 137, 143, 145, 154, 157, 167])\n",
        "array4 = np.array([  1,   5,   7,  13,  14,  17,  23,  30,  36,  44,  52,  54,  55, 59,  70,  77,  96,  97,  99, 102, 105, 113, 118, 120, 122, 126, 132, 136, 138, 151, 160, 161, 165])\n",
        "array5 = np.array([ 10,  20,  22,  25,  37,  45,  47,  57,  63,  71,  75,  76,  80,  84,  93,  98, 100, 104, 109, 112, 114, 121, 123, 124, 129, 135, 139, 142, 150, 153, 155, 163, 166])\n",
        "\n",
        "\n",
        "# Criar a lista composta\n",
        "kfold_test = [array1, array2, array3, array4, array5]"
      ],
      "metadata": {
        "id": "c3Ia2J4DPpO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4aPPergNnL"
      },
      "source": [
        "### Bertimbau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl6AUm0K3yE"
      },
      "source": [
        "- https://huggingface.co/neuralmind/bert-base-portuguese-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RQrbYpK5vC"
      },
      "source": [
        "```\n",
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "156b17a7400b4e899a5557d0929323d1",
            "961f225bb6594f64831073c5b438da2b",
            "ab4989cbe5e1474e8bcbf961db732043",
            "cd20c7e6707d43008f27d2281276c5b5",
            "80a1eb253bec44979744ade2b6085e12",
            "c8d8bbac632440e4a98e80ae01af3303",
            "41d54752fd04436381a2935339ff9e9e",
            "25acdb47fece43e5a6c0a9865dda250d",
            "13244d33e3c744419445514af4f6bc4d",
            "1abb89a469aa45ebb36fc154fd642f15",
            "8088f876fbc44cdc987967fd84fff10a",
            "5b2610a620dd4e69a812d7bf935ddb09",
            "77cbf4d11836472284329d1f9add49a3",
            "1876090a226e45d286daf6834709d1e4",
            "226ced9f2f094945bac8a12fcfd9cec2",
            "6946c66114de4dd1a3b96bb884a1ca7a",
            "26d4576b24ee4d559258a84c3dd2609c",
            "9870baa421654b47ac8923e772bdb646",
            "02c549250cd046caa5c88a22776fc43b",
            "9decf754bab8432f8af1bb0e05fd0f03",
            "6ca1d29428f44411a74e57c7619d7743",
            "7b23aae6b23c48b28a54c8f4e86e5d71",
            "8319284d4c9144d281e99c8ec54ae733",
            "5fcc7bd6ff5647bc985720006d108d08",
            "06b889814dbc4bdfa9dbe4c2442d2d0a",
            "4da9b643d70245248ce6cb0a11fb20fe",
            "689a635e42724d61a65d87b34c1211c8",
            "b3a87120ec7e4d49a8493f9bd10e03f8",
            "af4e7d16e8fa49a5945ec0149e16b5cb",
            "4ef7f5f688a74094a09e5b04e834d35d",
            "4b295d4bd7b74ccd99a36e41589d3cec",
            "7f733e443b3a4a59980c6e6a5b81071a",
            "0ecc5f00325844098a1c3db3e6876d76",
            "3a6e2ec061ab40cba915b66e2fdba1cb",
            "e803b93ae741483ab7e9765fabdbf97c",
            "bfa7ad4742ec42b29c91ae61c6e5639f",
            "a7d26ea72c98459499b754a1a62ab62c",
            "921a0e22cb3c467795e08217b4ca3993",
            "27e596f227a54a13a412c15ca78cb842",
            "7e1eececa4a34b44b0cd7b9187c3071f",
            "ad48d2ef15da451f8c624556218dcdf4",
            "2a1e2e741c144c38bc3ae6b276f84ef7",
            "13cf458229444f26bcc3c0e59c7cb159",
            "01c7c26bb1f3407bacdfe5a1a705d50f",
            "3a653672ef54480ab09b265f49fcc62f",
            "1543d4c344f146d0b18da6fbb38126f0",
            "d51b785b140344e981e2c39153ea69cd",
            "f04eefab8be04f59a566e877ec16e004",
            "6cddede123814c90a55c6789da590d30",
            "55362a372a6943bba302d00de9c8cb59",
            "2f1382693e9643ff94b4150700f8f8b9",
            "64055a594fb04625bdc67e493bd38b2e",
            "4a2279c7d7124d649cd5a0d15cf7b112",
            "9b1a76a3d8ea47b8829ebee01de36a13",
            "eecf52596cb942e39f3c6bc132b3deda"
          ]
        },
        "id": "WPj7c-IBgWRx",
        "outputId": "34d1b3e6-cdf6-4434-9e8e-6bfca4864802"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "156b17a7400b4e899a5557d0929323d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b2610a620dd4e69a812d7bf935ddb09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8319284d4c9144d281e99c8ec54ae733"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a6e2ec061ab40cba915b66e2fdba1cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a653672ef54480ab09b265f49fcc62f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fa4f28ce1a2548e8bc580ab728eaed4c",
            "3eacbb725a644735a1b65db6c273af88",
            "4986d3b902cc428087a83b253c73fde1",
            "f788d42ec3f24a66a8f0027a61f3a1e7",
            "47f201b5dbab4333bc7714e7158607ab",
            "5e2c7ceeb60d4b11a337664068def975",
            "584bb741727643929dc36f6a66f076a5",
            "30d0a5ad56774554b6bb979a3e52d00b",
            "cfa9895405024df9909b89bddb287230",
            "b1301498d461436cacf344dc6f066103",
            "41c618099e6f4c11aa56d6f112c0e889"
          ]
        },
        "id": "qSErznNMh4P5",
        "outputId": "4f0b421f-5ee7-4fa6-fc3f-050b672d259e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa4f28ce1a2548e8bc580ab728eaed4c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6s_rPsFgWUr"
      },
      "outputs": [],
      "source": [
        "frase = 'A avaliação de prontidão tecnológica em tecnologias de interesse militar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJbGMogWaZ",
        "outputId": "460112a6-3970-4cba-e917-e49b5d1b50b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  frase,\n",
        "  max_length=20,   #Perceba que irei cortar um token da minha frase anterior\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3fSHD8KgqDb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYjQrfrglJS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvfsVlnFglPp"
      },
      "outputs": [],
      "source": [
        "novo_df = df[[\"resumo\", \"rotulo\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nth2Ui-qg6Ix"
      },
      "outputs": [],
      "source": [
        "def adapto_faixa(faixa):\n",
        "    faixa -=1\n",
        "    return faixa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKj3lo8Qg9Vj",
        "outputId": "f56f97d7-1ec4-47b6-dba1-fc1b06825747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3dc243303e67>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)\n"
          ]
        }
      ],
      "source": [
        "novo_df ['rotulo'] = novo_df.rotulo.apply(adapto_faixa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_cANKWhOhCix",
        "outputId": "21616ef6-a372-4f29-834c-b1446dc5c0bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                resumo  rotulo\n",
              "163  Rio de Janeiro (RJ) – O Centro de Avaliações d...       2\n",
              "164  Este trabalho apresenta um sistema para contro...       2\n",
              "165  No contexto das comunicações táticas baseadas ...       2\n",
              "166  O valor da velocidade de alvos móveis em image...       0\n",
              "167  Neste artigo é apresentada a análise da seção ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aab63618-64a6-4931-b23f-eab06fe2a02c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resumo</th>\n",
              "      <th>rotulo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Rio de Janeiro (RJ) – O Centro de Avaliações d...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Este trabalho apresenta um sistema para contro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>No contexto das comunicações táticas baseadas ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>O valor da velocidade de alvos móveis em image...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Neste artigo é apresentada a análise da seção ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aab63618-64a6-4931-b23f-eab06fe2a02c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aab63618-64a6-4931-b23f-eab06fe2a02c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aab63618-64a6-4931-b23f-eab06fe2a02c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0a3afeee-9bc1-45cc-8830-38ad2f52e521\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a3afeee-9bc1-45cc-8830-38ad2f52e521')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0a3afeee-9bc1-45cc-8830-38ad2f52e521 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "novo_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZK6EQ4xhFHI"
      },
      "outputs": [],
      "source": [
        "class_names = ['Faixa 1', 'Faixa 2', 'Faixa 3']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lmol6gshNCR"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFoXmzH5hFOo"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      #padding='longest',\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K-OuLWzhYhB"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.resumo.to_numpy(),\n",
        "    targets=df.rotulo.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBrznGAWiJWg"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'],\n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-yCbjniOLI",
        "outputId": "176208d7-64e3-41b9-a1ef-b2162a1859b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "bert_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxhW_89kRiv"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiZnSuuTiTOZ"
      },
      "outputs": [],
      "source": [
        "class TRLClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(TRLClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    #The last_hidden_state is a sequence of hidden states of the last layer of the model\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kX7ClUGiiCQ"
      },
      "outputs": [],
      "source": [
        "model = TRLClassifier(len(class_names)) #Pois tenho 3 classes\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXw5SIa0hkah"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTSemYwhnlI"
      },
      "source": [
        "### Separação e preparação do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lembrar que estamos utilizando o Kfold"
      ],
      "metadata": {
        "id": "z09fs6pHk5FG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTLzT3ttq-0t"
      },
      "source": [
        "    - 63,7% treinamento - 16% validação                20% teste\n",
        "\n",
        "      ------------ 80%----------------------------------20%--"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegando o índice dos conjuntos\n",
        "index_df_train = kfold_train[particao]\n",
        "index_df_test = kfold_test[particao]"
      ],
      "metadata": {
        "id": "FeqFToMFnaMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = novo_df.iloc[index_df_train], novo_df.iloc[index_df_test]"
      ],
      "metadata": {
        "id": "EpR4mtHLoDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Bhg2qAhSlg"
      },
      "outputs": [],
      "source": [
        "#Separando o treinamento em treino e validação\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, stratify= df_train['rotulo'], random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3FAkW0qhW2J",
        "outputId": "dcedbb41-7337-4dd8-df72-f9f9bff447dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((107, 2), (27, 2), (34, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxinAAqhW5A",
        "outputId": "dc7c445c-b07d-4800-e550-95be2715a3d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fsBv27hW8L",
        "outputId": "9aaeffde-02a0-43e9-a913-9eb30d41c195"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(train_data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbQEYoXhvVA",
        "outputId": "b5ea777c-2ed5-4439-d141-442b0604bdee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7qyCztOhvX4",
        "outputId": "85292c2f-d263-4929-f273-cff7a913664c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512])\n",
            "torch.Size([8, 512])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP6kiy4whvaa",
        "outputId": "35d200ea-08c4-49f1-fefa-f74a4dbeae74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512])\n",
            "torch.Size([8, 512])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzMSQeLlRHn"
      },
      "source": [
        "### Treinamento mesmo..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oRhXpG_iBWB",
        "outputId": "d4808829-5886-439c-8dbb-626fdb11db51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp8dJvCviFTI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model,\n",
        "  data_loader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  device,\n",
        "  scheduler,\n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tKAqpPliFWJ"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZisESWlkqH",
        "outputId": "2b20ad43-545f-42b5-827c-9211848f1dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.848407761326858 accuracy 0.6074766355140186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.674145046621561 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 2/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.756963353071894 accuracy 0.7009345794392523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7391863763332367 accuracy 0.6296296296296295\n",
            "\n",
            "Epoch 3/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7387127301522664 accuracy 0.6822429906542056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6741011440753937 accuracy 0.7037037037037037\n",
            "\n",
            "Epoch 4/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6824798211455345 accuracy 0.7289719626168224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.780799463391304 accuracy 0.5925925925925926\n",
            "\n",
            "Epoch 5/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.48797181780849186 accuracy 0.7850467289719626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8553727567195892 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 6/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.44132653943129946 accuracy 0.8130841121495327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7777508795261383 accuracy 0.6666666666666666\n",
            "\n",
            "Epoch 7/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.17166363314858504 accuracy 0.9439252336448597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.5541871190071106 accuracy 0.48148148148148145\n",
            "\n",
            "Epoch 8/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.16170373033466084 accuracy 0.9439252336448597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.6015244126319885 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 9/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.029330756897772 accuracy 0.9906542056074765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.8089784071198665 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 10/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.007525974219398839 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.266790333087556 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 11/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0030666787892446984 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.4254261926398613 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 12/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0021051569991478963 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5157111731241457 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 13/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0017303530974978848 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.5889630297606345 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 14/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.001452188182156533 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.6452955056447536 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 15/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0011736053773867233 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.688335127342725 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 16/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0010462748801468738 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.723557767676539 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 17/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0010370406505119587 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.75596237492573 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 18/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0009850012720562518 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.786841164561338 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 19/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0009219240912768457 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.8152724237152142 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 20/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0008404680452908256 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.8410224625986302 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 21/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007393332821915724 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.863172986122663 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 22/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007578205591666379 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.8808879990974674 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 23/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0007187380605111164 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.897884556747158 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 24/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000668005708056236 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.9130731657496653 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 25/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0006498714958849762 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.926702297758311 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 26/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000753471051991385 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.9460148596263025 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 27/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005827130163587364 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.9619005808854126 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 28/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005909815850567871 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.972363224660512 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 29/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005481197461319555 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.9814009698820882 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 30/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005340617748775653 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.9903371684486046 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 31/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005371774688163507 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 2.9984194602511707 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 32/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005595392244036443 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0061080930536264 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 33/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000536724239022338 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0137096038452 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 34/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004951319237339444 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0209354959151824 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 35/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0005029986184256684 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0278391949032084 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 36/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004714433162007481 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0340757289377507 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 37/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000469073183402153 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0394278789754026 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 38/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.000503451717252444 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.042577329229971 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 39/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004811548444974635 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0455185704340693 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 40/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00042895873255994435 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0490592556307092 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 41/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004277388174419424 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0525916358819813 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 42/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004627839932384502 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0559700871017412 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 43/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00043564248231372664 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0589085467508994 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 44/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00045952638902235776 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0613706757649197 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 45/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004440491008738588 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0633097044265014 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 46/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00043547227687668055 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0649132980543072 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 47/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00044475979978285195 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0662355576932896 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 48/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004567525133357516 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.067199335062469 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 49/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.00041986559309797097 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0677482640967355 accuracy 0.5185185185185185\n",
            "\n",
            "Epoch 50/50\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.0004262219777696633 accuracy 0.9999999999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 3.0679603573007626 accuracy 0.5185185185185185\n",
            "\n",
            "CPU times: user 8min 25s, sys: 19.9 s, total: 8min 45s\n",
            "Wall time: 9min 30s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "#   if val_acc > best_accuracy:\n",
        "#     torch.save(model.state_dict(), melhor_modelo)\n",
        "#     best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjGnJKDmZVJ"
      },
      "source": [
        "- Gráfico de desempenho no treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQOy_NBBlks7"
      },
      "outputs": [],
      "source": [
        "A= history['train_acc']\n",
        "A2 = [tensor.item() for tensor in A]\n",
        "B= history['val_acc']\n",
        "B2 = [tensor.item() for tensor in B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "hHRH2aNQlkvy",
        "outputId": "9e75671b-9f91-4b55-e02f-e843d2ae3540"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 1017,
              "height": 717
            }
          }
        }
      ],
      "source": [
        "plt.plot(A2, label='train accuracy')\n",
        "plt.plot(B2, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd9c9eGmecP"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jFRntT6mgoW",
        "outputId": "eab3ecef-4d3a-4001-d005-6cf2d599c608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6764705882352942"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBG2MIlgmkHv"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXTHRhD5mqnn",
        "outputId": "a3d92b37-736e-45e5-e960-1cd2dc2a44f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTdojX6Jmqqm",
        "outputId": "b5245b3b-d635-436e-fdb5-8c744b3fbc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Faixa 1       0.70      0.84      0.76        19\n",
            "     Faixa 2       0.25      0.12      0.17         8\n",
            "     Faixa 3       0.86      0.86      0.86         7\n",
            "\n",
            "    accuracy                           0.68        34\n",
            "   macro avg       0.60      0.61      0.60        34\n",
            "weighted avg       0.62      0.68      0.64        34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "HQ54xluu8RL3",
        "outputId": "b30e567a-53e6-4f5f-9c6c-4ce93d4e35ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 981,
              "height": 730
            }
          }
        }
      ],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('Faixa TRL verdadeira')\n",
        "  plt.xlabel('Faixa TRL predita');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZTajeJuiP_z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "156b17a7400b4e899a5557d0929323d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_961f225bb6594f64831073c5b438da2b",
              "IPY_MODEL_ab4989cbe5e1474e8bcbf961db732043",
              "IPY_MODEL_cd20c7e6707d43008f27d2281276c5b5"
            ],
            "layout": "IPY_MODEL_80a1eb253bec44979744ade2b6085e12"
          }
        },
        "961f225bb6594f64831073c5b438da2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d8bbac632440e4a98e80ae01af3303",
            "placeholder": "​",
            "style": "IPY_MODEL_41d54752fd04436381a2935339ff9e9e",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "ab4989cbe5e1474e8bcbf961db732043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25acdb47fece43e5a6c0a9865dda250d",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13244d33e3c744419445514af4f6bc4d",
            "value": 43
          }
        },
        "cd20c7e6707d43008f27d2281276c5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1abb89a469aa45ebb36fc154fd642f15",
            "placeholder": "​",
            "style": "IPY_MODEL_8088f876fbc44cdc987967fd84fff10a",
            "value": " 43.0/43.0 [00:00&lt;00:00, 2.70kB/s]"
          }
        },
        "80a1eb253bec44979744ade2b6085e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d8bbac632440e4a98e80ae01af3303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d54752fd04436381a2935339ff9e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25acdb47fece43e5a6c0a9865dda250d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13244d33e3c744419445514af4f6bc4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1abb89a469aa45ebb36fc154fd642f15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8088f876fbc44cdc987967fd84fff10a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b2610a620dd4e69a812d7bf935ddb09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77cbf4d11836472284329d1f9add49a3",
              "IPY_MODEL_1876090a226e45d286daf6834709d1e4",
              "IPY_MODEL_226ced9f2f094945bac8a12fcfd9cec2"
            ],
            "layout": "IPY_MODEL_6946c66114de4dd1a3b96bb884a1ca7a"
          }
        },
        "77cbf4d11836472284329d1f9add49a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26d4576b24ee4d559258a84c3dd2609c",
            "placeholder": "​",
            "style": "IPY_MODEL_9870baa421654b47ac8923e772bdb646",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "1876090a226e45d286daf6834709d1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02c549250cd046caa5c88a22776fc43b",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9decf754bab8432f8af1bb0e05fd0f03",
            "value": 209528
          }
        },
        "226ced9f2f094945bac8a12fcfd9cec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ca1d29428f44411a74e57c7619d7743",
            "placeholder": "​",
            "style": "IPY_MODEL_7b23aae6b23c48b28a54c8f4e86e5d71",
            "value": " 210k/210k [00:00&lt;00:00, 8.87MB/s]"
          }
        },
        "6946c66114de4dd1a3b96bb884a1ca7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d4576b24ee4d559258a84c3dd2609c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9870baa421654b47ac8923e772bdb646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02c549250cd046caa5c88a22776fc43b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9decf754bab8432f8af1bb0e05fd0f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ca1d29428f44411a74e57c7619d7743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b23aae6b23c48b28a54c8f4e86e5d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8319284d4c9144d281e99c8ec54ae733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fcc7bd6ff5647bc985720006d108d08",
              "IPY_MODEL_06b889814dbc4bdfa9dbe4c2442d2d0a",
              "IPY_MODEL_4da9b643d70245248ce6cb0a11fb20fe"
            ],
            "layout": "IPY_MODEL_689a635e42724d61a65d87b34c1211c8"
          }
        },
        "5fcc7bd6ff5647bc985720006d108d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3a87120ec7e4d49a8493f9bd10e03f8",
            "placeholder": "​",
            "style": "IPY_MODEL_af4e7d16e8fa49a5945ec0149e16b5cb",
            "value": "Downloading (…)in/added_tokens.json: 100%"
          }
        },
        "06b889814dbc4bdfa9dbe4c2442d2d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ef7f5f688a74094a09e5b04e834d35d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b295d4bd7b74ccd99a36e41589d3cec",
            "value": 2
          }
        },
        "4da9b643d70245248ce6cb0a11fb20fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f733e443b3a4a59980c6e6a5b81071a",
            "placeholder": "​",
            "style": "IPY_MODEL_0ecc5f00325844098a1c3db3e6876d76",
            "value": " 2.00/2.00 [00:00&lt;00:00, 100B/s]"
          }
        },
        "689a635e42724d61a65d87b34c1211c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3a87120ec7e4d49a8493f9bd10e03f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af4e7d16e8fa49a5945ec0149e16b5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ef7f5f688a74094a09e5b04e834d35d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b295d4bd7b74ccd99a36e41589d3cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f733e443b3a4a59980c6e6a5b81071a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ecc5f00325844098a1c3db3e6876d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a6e2ec061ab40cba915b66e2fdba1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e803b93ae741483ab7e9765fabdbf97c",
              "IPY_MODEL_bfa7ad4742ec42b29c91ae61c6e5639f",
              "IPY_MODEL_a7d26ea72c98459499b754a1a62ab62c"
            ],
            "layout": "IPY_MODEL_921a0e22cb3c467795e08217b4ca3993"
          }
        },
        "e803b93ae741483ab7e9765fabdbf97c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e596f227a54a13a412c15ca78cb842",
            "placeholder": "​",
            "style": "IPY_MODEL_7e1eececa4a34b44b0cd7b9187c3071f",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "bfa7ad4742ec42b29c91ae61c6e5639f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad48d2ef15da451f8c624556218dcdf4",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a1e2e741c144c38bc3ae6b276f84ef7",
            "value": 112
          }
        },
        "a7d26ea72c98459499b754a1a62ab62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13cf458229444f26bcc3c0e59c7cb159",
            "placeholder": "​",
            "style": "IPY_MODEL_01c7c26bb1f3407bacdfe5a1a705d50f",
            "value": " 112/112 [00:00&lt;00:00, 4.72kB/s]"
          }
        },
        "921a0e22cb3c467795e08217b4ca3993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e596f227a54a13a412c15ca78cb842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e1eececa4a34b44b0cd7b9187c3071f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad48d2ef15da451f8c624556218dcdf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a1e2e741c144c38bc3ae6b276f84ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13cf458229444f26bcc3c0e59c7cb159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c7c26bb1f3407bacdfe5a1a705d50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a653672ef54480ab09b265f49fcc62f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1543d4c344f146d0b18da6fbb38126f0",
              "IPY_MODEL_d51b785b140344e981e2c39153ea69cd",
              "IPY_MODEL_f04eefab8be04f59a566e877ec16e004"
            ],
            "layout": "IPY_MODEL_6cddede123814c90a55c6789da590d30"
          }
        },
        "1543d4c344f146d0b18da6fbb38126f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55362a372a6943bba302d00de9c8cb59",
            "placeholder": "​",
            "style": "IPY_MODEL_2f1382693e9643ff94b4150700f8f8b9",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "d51b785b140344e981e2c39153ea69cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64055a594fb04625bdc67e493bd38b2e",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a2279c7d7124d649cd5a0d15cf7b112",
            "value": 647
          }
        },
        "f04eefab8be04f59a566e877ec16e004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b1a76a3d8ea47b8829ebee01de36a13",
            "placeholder": "​",
            "style": "IPY_MODEL_eecf52596cb942e39f3c6bc132b3deda",
            "value": " 647/647 [00:00&lt;00:00, 30.3kB/s]"
          }
        },
        "6cddede123814c90a55c6789da590d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55362a372a6943bba302d00de9c8cb59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f1382693e9643ff94b4150700f8f8b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64055a594fb04625bdc67e493bd38b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a2279c7d7124d649cd5a0d15cf7b112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b1a76a3d8ea47b8829ebee01de36a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eecf52596cb942e39f3c6bc132b3deda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa4f28ce1a2548e8bc580ab728eaed4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3eacbb725a644735a1b65db6c273af88",
              "IPY_MODEL_4986d3b902cc428087a83b253c73fde1",
              "IPY_MODEL_f788d42ec3f24a66a8f0027a61f3a1e7"
            ],
            "layout": "IPY_MODEL_47f201b5dbab4333bc7714e7158607ab"
          }
        },
        "3eacbb725a644735a1b65db6c273af88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e2c7ceeb60d4b11a337664068def975",
            "placeholder": "​",
            "style": "IPY_MODEL_584bb741727643929dc36f6a66f076a5",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "4986d3b902cc428087a83b253c73fde1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30d0a5ad56774554b6bb979a3e52d00b",
            "max": 438235074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfa9895405024df9909b89bddb287230",
            "value": 438235074
          }
        },
        "f788d42ec3f24a66a8f0027a61f3a1e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1301498d461436cacf344dc6f066103",
            "placeholder": "​",
            "style": "IPY_MODEL_41c618099e6f4c11aa56d6f112c0e889",
            "value": " 438M/438M [00:01&lt;00:00, 342MB/s]"
          }
        },
        "47f201b5dbab4333bc7714e7158607ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e2c7ceeb60d4b11a337664068def975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "584bb741727643929dc36f6a66f076a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30d0a5ad56774554b6bb979a3e52d00b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa9895405024df9909b89bddb287230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1301498d461436cacf344dc6f066103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c618099e6f4c11aa56d6f112c0e889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}